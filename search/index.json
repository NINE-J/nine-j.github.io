[{"content":"📌개요 메일함에서 Medium Daily Digest를 확인하다가 발목이 잡혔다. 공감할 수 있는 내용이고 자신의 태도에 대해 고민해 볼 수 있는 좋은 글이라고 생각한다.\n이 글에서 \u0026lsquo;우리\u0026rsquo;, \u0026lsquo;자신\u0026rsquo;이라는 키워드가 자주 등장할 텐데 만약 본인은 포함되지 않는다고 생각한다면 빠르게 뒤로 가기, 탭 닫기를 통해 시간을 아끼길 바란다.\n\u0026ldquo;나의 5년 자바 경력이 60분 면접 만에 무너졌다 - My 5 Years of Java Experience Collapsed in a 60-Minute Interview\u0026quot;는 자극적인 제목보다 더 충격적이었던 건, 그 안의 질문들에 선뜻 답하지 못하는 나의 모습이었다.\n우리는 모두가 자신의 문제를 해결하기 위해 바쁘게 생산적인 일을 한다. 매일 스프린트를 쳐내고, 라이브러리를 가져다 쓰고, 비즈니스 로직을 구현한다. 하지만 \u0026lsquo;어떻게(How)\u0026rsquo; 돌아가게 만드는 것에 익숙해진 나머지, \u0026lsquo;왜(Why)\u0026rsquo; 그렇게 설계되었는지에 대해서는 추상화라는 장막 뒤로 숨겨두곤 한다.\n이 포스팅은 원작자가 겪은 뼈아픈 면접 기록을 통해, 우리가 놓치고 있었던 \u0026lsquo;백엔드 개발자의 기술적 깊이\u0026rsquo;란 무엇인지, 그리고 \u0026lsquo;연차\u0026rsquo;를 \u0026lsquo;실력\u0026rsquo;으로 치환하기 위해 어떤 태도가 필요한지 정리해본다.\n📌내용 어느 5년 차 개발자의 뼈아픈 고백 원문: My 5 Years of Java Experience Collapsed in a 60-Minute Interview\n\u0026ldquo;간단한 것부터 시작하죠.\u0026rdquo; 면접관의 이 한마디가 비수가 되어 돌아오기까지는 그리 오랜 시간이 걸리지 않았습니다. 5년 동안 자바를 다뤄왔고 수많은 마이크로서비스를 배포했으며 운영 환경의 복잡한 버그들을 잡아왔던 한 개발자의 자신감은 60분 만에 산산조각이 났습니다.\n최근 해외 개발 관련 글에서 공감을 얻고 있는 My 5 Years of Java Experience Collapsed in a 60-Minute Interview라는 글의 이야기다. 이 글은 단순한 면접 탈락 수기가 아니다. 우리가 \u0026lsquo;경험\u0026rsquo;이라고 부르는 시간이 얼마나 허망하게 쌓여있을 수 있는지에 대한 자기반성이자 동시에 우리 모두에게 던지는 경고장이다.\n나 역시 이 글을 읽으며 묘한 서늘함을 느꼈다. \u0026ldquo;내가 저 자리에 있었다면 과연 면접관의 꼬리 질문을 견뎌낼 수 있었을까?\u0026ldquo;라는 질문에 선뜻 \u0026ldquo;예쓰\u0026quot;라고 답할 수 없었기 때문이다. 우리는 과연 경력이라는 이름 뒤에 숨어 성장을 멈춘 것은 아닐까?\n왜 우리는 경험을 실력이라 착각하는가? 원작자는 5년의 경력을 가졌음에도 Arrays.asList()로 생성한 리스트에 요소를 추가할 때 왜 에러가 나는지, 왜 공유 자원에 대한 동시성 이슈를 해결할 때 AtomaticInteger가 대안이 될 수 있는지 명확히 설명하지 못했다. 왜 이런 일이 벌어질까?\n1. 숙련도와 전문성의 괴리: 사용과 이해는 다르다. 우리는 도구에 익숙해지는 것을 실력이라 착각하곤 한다.\n숙련도(How): 특정 라이브러리의 API를 호출하고 늘 쓰던 패턴으로 서비스를 구축하는 능력. 전문성(Why): 그 도구가 내부적으로 어떻게 동작하며, 왜 다른 대안이 아닌 이 기술을 선택했는지 논리적으로 증명하는 능력. 시니어에게 요구되는 것은 전자가 아니라 후자다. 하지만 많은 이들이 프레임워크가 제공하는 편리함(추상화)에 안주하며 내부 원리를 궁금해하지 않는다. 2. 업무의 반복성: 1년 차 5번 반복의 함정 원작자는 뼈아픈 통찰을 남겼다. \u0026ldquo;나는 5년 동안 코딩해 왔지만, 5년 동안 배우지는 않았다\u0026quot;라는 것이다.\n매일 비슷한 비즈니스 로직을 짜고, 기존 템플릿을 복사해 새 서비스를 만들며 바쁘게 보낸 시간은 우리를 익숙한 개발자로 만들 뿐이다. 새로운 기술적 난제에 부딪히고 이를 해결하기 위해 깊게 파고드는 경험이 없다면, 그것은 5년의 경력이 아니라 1년의 경험을 5번 반복한 것에 불과하다. 우리는 수평적으로 경험을 확장하는 데 급급해 정작 한 분야의 뿌리를 깊게 내리는 수직적 성장을 소홀히 해왔을지도 모른다.\n백엔드 개발자에게 요구되는 진짜 깊이란? 원작자가 마주했던 질문들은 결코 지엽적인 말장난이 아니었다. 이는 백엔드 개발자가 매일 다루는 도구의 한계점과 동작 원리를 얼마나 깊게 인지하고 있는지를 묻는 본질적인 질문들이었다.\n1. 자바 기본기: Arrays.asList()와 추상화의 함정 단순히 \u0026ldquo;에러가 난다\u0026quot;를 아는 것과 그 이유가 ArrayList라는 이름 뒤에 숨겨진 구현의 차이 떄문임을 아는 것은 다르다.\nArrays.asList()는 우리가 흔히 쓰는 java.util.ArrayList가 아니라, 내부 클래스인 Arrays$ArrayList를 반환한다. 이는 \u0026lsquo;고정 크기\u0026rsquo; 배열을 래핑한 것에 불과하기 때문에 add() 호출 시 예외를 던지는 것이다.\n우리가 사용하는 인터페이스(List) 뒤에 어떤 구체적인 구현체가 숨어 있는지 확인하는 습관이 없다면, 런타임에 예상치 못한 사이드 이펙트를 마주하게 된다.\n2. Concurrency: 동시성 이슈를 해결하는 논리적 근거 공유 자원 문제에 단순히 synchronized를 붙이는 것은 가장 쉬운 해결책일 수 있지만, 성능 최적화 관점에서는 최악일 수 있다. 면접관이 던진 AtomaticInteger라는 대안은 Lock-based(비관적 락)와 CAS(Compare-And-Swap, 낙관적 락)의 차이를 이해하고 있는지 묻는 것이다.\n실무에서 발생하는 데드락이나 성능 저하 문제는 대개 이런 원리에 대한 무지에서 시작된다. \u0026ldquo;돌아가니까 됐다\u0026quot;는 시니어의 답변이 될 수 없다.\n3. 시스템 설계: Trade-off의 예술 레이트 리미터 설계 예시에서 중요한 건 \u0026lsquo;정답\u0026rsquo;이 아니다. 왜 고정 윈도우가 아닌 슬라이딩 윈도우를 택했는지, 토큰 버킷 방식이 트래픽 버스트를 어떻게 처리하는지 설명할 수 있어야 한다.\n모든 기술 선택에는 비용이 따른다. 시니어는 성능, 복잡도, 유지보수성 사이에서 최적의 균형점을 찾는 \u0026lsquo;의사결정권자\u0026rsquo;여야 한다.\n얕은 지식을 온전히 내 것으로 만드는 전략 사실 이건 엉덩이 붙이고 앉아서 사투를 벌이며 체화하는 것 말고 왕도가 있을까 싶다. 자신에게 맞는 효율적인 방법을 찾아 시간을 아낄 수 있다면 더할 나위 없이 좋은 가치가 된다. 하지만 효율을 핑계로 깊이 파고드는 고통을 회피하고 있는 것은 아닌지 스스로 경계해야 한다.\n파편화된 지식을 경험으로 연결하기 위해 내가 실천하고 있는, 그리고 원작자가 제안한 \u0026lsquo;리셋\u0026rsquo; 전략이다.\n1. 소스 코드 여행(Go to Declaration) IDE에서 라이브러리 메서드를 사용할 때 Ctrl+Click을 누르는 습관을 들여야 한다. 내가 호출한 메서드가 내부적으로 어떤 객체를 생성하고 어떤 예외를 던지는지 직접 눈으로 확인하는 과정이 쌓여 기술적 깊이가 된다.\n2. 문서화와 언어화 머릿속으로 안다고 생각하는 것과 그것을 타인에게 설명하는 것은 천지 차이다. 학습한 내용을 블로그에 정리하거나 동료에게 공유해보자. 논리의 빈틈은 설명하려 할 때 비로소 드러난다.\nPoC(Proof of Concept) 중심 학습 이론으로만 배운 개념은 금방 휘발된다. AtomaticInteger가 정말 synchronized보다 빠른지, 1000개의 스레드를 돌려 직접 벤치마크 코드를 작성해 보아라. 직접 현상을 재현하고 데이터를 눈으로 확인한 지식은 결코 무너지지 않는다.\n연차의 무게를 감당하는 개발자가 되기 위해 원작자는 마지막에 이렇게 말한다. \u0026ldquo;당신의 경험은 그것을 깊이 있게 설명할 수 있을 떄만 의미가 있다.\u0026rdquo;\n5년, 10년이라는 숫자가 주는 안락함에 속지 말자. 오늘 내가 작성한 코드 한 줄의 \u0026lsquo;이유\u0026rsquo;를 설명하지 못한다면 우리는 그저 거대한 프레임워크의 부품으로 머물 뿐이다. 연차에 어울리는 무게감은 내가 아는 기술의 뿌리에서 나온다. 내일의 우리는 오늘보다 조금 더 깊은 개발자이기를 바란다.\n🎯결론 경력은 단순히 시간이 흐른 결과가 아니라 그 시간 동안 쌓아 올린 논리적 근거의 총합이다. 효율을 핑계로 깊이 파고드는 고통을 회피하고 있는 것은 아닌지 스스로 경계해야 한다.\n","date":"2026-01-04T14:54:18+09:00","permalink":"https://blog.b9f1.com/p/2026-01-04-my-5-years-of-java-experience-collapsed-in-a-60-minute-interview/","title":"당신은 5년 차인가요, 1년 차 5번인가요?"},{"content":"📌개요 노트 정리를 꽤 만족스럽게 유지하고 있다고 생각했는데 저장소 관리와 배포 파이프라인을 점검하면서 Zettelkasten이라는 새로운 이론을 마주하게 됐다.\nZettelkasten이 뭐지? 메모 정리 방법? 상향식 사고? PARA도 정리법인데 어떤 차이가 있지?\nInfo Zettelkasten: 메모 상자 혹은 체텔카스텐/제텔카스텐은 연구와 공부를 하는 데 도움을 주는 지식 관리와 더불어 메모 작성 기법이다.\n키워드를 검색했을 때 많은 문서함이 쌓여있는 이미지를 보고 이 캐릭터가 떠오르지 않을 수 없었다. 어디에 뭐가 있는지 속속들이 알고 굉장한 속도로 원하는 것을 찾아낸다. 사실 그 보관함은 도구에 불과할 뿐 그 모든 걸 이해하고 다루는 본체가 중요하겠지만.\nPARA 체계를 통해 관심사에 따라 정리하는 게 현재 내가 필요로 하는 것에 빠르게 접근할 수 있어서 만족스러웠다. 하지만 점점 노트의 양이 많아지면서 Area, Archive 영역이 점점 비대해졌다. 메모를 작성할 때 내가 어떤 생각을 하고 남겼는지 다시 찾을 수 없다면 그냥 용량 차지하는 쓰레기를 모아둔 것과 같다. 그럴 바엔 인터넷 검색으로 남들이 잘 정리해둔 글을 찾는 게 빠를 것이다.\n넘치는 데이터는 통제할 수 없고 누구도 강요하지 않는다. 적어도 내 의지로 작성한 건 내 입맛대로 관리할 수 있어야 한다.\n📌내용 쌓여가는 메모, 왜 활용하지 못할까? 나의 소중한 Resource 폴더가 지식의 무덤이 되고 있는 안타까움에 주요한 사고방식부터 비교적 문제가 아닐 수도 있는 사소한 원인까지 다양하게 고민했다.\n당장 해결해야 하는 원인만 중요도 순서로 정리해봤다.\n지식 수집이 소화되지 않고 병목이 발생하는 Resource 포화 상태 계속해서 쏟아지는 지식을 그저 수집하기만 했던 기간의 Resource 영역은 너무나 정신 사나웠다. 부담이 됐고 빠르게 치워버리고 싶었다. 하향식 사고 언제부턴가 시작하기 위해서 목표부터 정하고 그걸 해결하기 위한 계획을 세우는 것에 익숙해졌다. 그게 잘못된 건 아니지만 속도 있게 만족스러운 결과를 얻기 위한 방식에는 적절하지 않다는 생각이 들었다. 내가 백링크를 제대로 활용하지 못한 것. 문서의 백링크는 중요하다. 옵시디언은 기본적으로 문서의 위치가 바뀌어도 링크를 유지해준다. 하지만 난 그 기능을 믿지 못해서 처음부터 백링크를 거의 사용하지 않았다. 이건 대안이 많다. 자연어 쿼리로 문서 검색, 연관 문서 Rating을 제공하는 Smart Connections 같은 플러그인도 있다. 관심 포화, 지적 갈망 지식 수집이 소화되지 않는 문제. 이거 아주 크다. 욕심은 많아서 이것저것 담는데 그걸 소화하는가? 소화하지도 못하는 걸 다 담아서 스치듯 접하기만 한다고 내 것이 되는 게 아니다. 그저 지적 갈망일 뿐이다.\n간단히 개발 관련 지식으로 예를 들어 보자.\n요즘 메일 구독 서비스에서 자주 등장하는 서버 부하 테스트 포스팅을 여러 건 읽었다고 치자.\n1단계. 이건 그저 읽은 거다. 접한 것이다. 포스팅이 삭제되거나 새로운 정보로 최신 기사가 덮이면 기억조차 못한다.\n2단계. 그걸 읽고 AI에게 질문하여 이해까지 했다. 이건 이해만 한 것이고 오히려 더 위험하다. 이해했다고 착각하는 순간 뇌는 리소스를 소모하면서 기억할 가치가 없다고 판단하며 다시 사용할 때까지 보류된다. 그 과정에서 잊혀진다.\n3단계. 서버 부하 테스트에 필요한 세팅을 구성하고 다양한 시나리오를 적용해 한정된 환경에서 부하 테스트를 완료한 뒤 기록하는 과정을 거치고 트레이드오프와 더 현실적인 테스트 방식까지 고민해본다. 이건 내 것이다. 어떻게 뺏어갈 수 있나? 이미 내가 다 해봤는데.\n뛰어난 환경에서의 경험과 재능까지 모두 놓고 1, 2단계로도 충분하다고 집요하게 따진다면 본인에게 효율적인 그 방식을 선택하기 바란다.\n하지만 지금 간략한 예시로 관심 포화, 지적 갈망의 위험성은 충분히 설명된 것 같다.\n하향식 사고에서 상향식 사고로 전환 절대 하향식 사고가 잘못된 것이 아니다. 이건 사람마다 성향 차이가 있을 뿐이다. 심지어 시간이 지남에 따라 본인의 성향이 달라질 수도 있을 것이다. 본인에게 잘 맞는 방식으로 지식 관리가 이루어진다면 더 큰 시너지를 기대할 수 있을 것이다.\n현재의 나로서는 목표부터 정하고 그 구조에 맞게 하향식으로 설계하는 건 반대의 경우보다 어렵다. 두 가지 모두 장단점이 확실하고 어느 한 쪽으로 치우치기 쉽다.\n예를 들어 내가 선호하는 방식이다.\n우선 관심이 생기면 시작한다. 나의 한계에 부딪히며 더 나은 방식을 탐색하고 탐구한다. 지금 내 상황보다 나아진다면 만사형통이다. 목표를 계속해서 바꿀 수 있다. 반대로 누군가의 요청에 의해 또는 스스로 하향식 사고로 설계한다면 다음과 같다.\n목표를 정해야 한다. 그 목표에 대한 정보를 수집해야 한다. 목표 실현 계획을 세우기 위해 정보를 수집해야 한다. 계속해서 더 나은 방식이 나를 유혹한다. 지식의 늪에 빠져 방향을 잡기 어렵다. 고민하는 동안 더 많은 해결책이 제시되고 아직 계획을 세우고 있다. 안정적인 계획 안에서 유연하게 플랜을 조정할 수 있다. 이 두 가지 프로세스를 나열해 보면서 하향식 구조는 역설적이게도 이미 체계가 잡힌 전문가나, 갈피를 잡지 못하는 초보자에게만 유용한 양극단의 도구처럼 느껴졌다.\n상향식은 \u0026ldquo;어?\u0026ldquo;에서 시작한다. 이게 뭐지? 왜? 좋은가? 사실 계획하지 않은 것들은 시간을 허비할 확률이 크다. 반대로 시도할 생각도 않다가 더 큰 수확이 생기기도 한다. 무계획도 계획이다.\nTip [세컨드 브레인은 옵시디언 중에서]\n삶에서 대부분의 지식 활동은 커리큘럼을 따르는 것처럼 하향식으로 이루어지지 않습니다. 반대로 우연, 창발, 귀납이 큰 부분을 차지하죠. 코끼리를 볼 수 없어도 부분 부분 더듬어 가며 코끼리의 전체 생김새를 추측하듯 우리는 세상을 상향식으로 배우고 깨닫습니다.\n내가 중요하게 생각한 건 \u0026ldquo;나에게 무엇이 남는가?\u0026rdquo; 그게 중요하다.\n개개인의 열정은 무한하다. 언제든 다시 채워지고 얼마든 땡겨 쓸 수 있다고 생각한다. 하지만 \u0026lsquo;번아웃\u0026rsquo;이란 단어 하나가 이 믿음을 무색하게 만들기도 한다.\n노력 없이 얻었지만 가장 귀하며 한정적 재화인 시간과 노력으로 얻어야 하지만 무한한 열정을 잘 조율해서 본인이 아름답게 사용하도록 하자.\n완벽주의와 수집벽의 충돌 PARA 정리의 쾌감, Zettelkasten 문득 떠오른 아이디어부터 확장하는 재미. 두 마리 토끼를 한 번에 잡지 못하는 이유는 과도한 디테일과 수집벽에 있다.\n하나의 Masterpiece를 만들어서 링크로 연결하는 게 아니다. 아주 사소한 메모로 시작해서 링크가 이어지고 링크와 링크를 통해 완전해지는 것이다.\n접하고, 더듬고, 상상하고, 이해하고, 실행하고, 기록하고, 체득하고, 떠올리고, 어지르며 내 것으로 만들자.\n평생을 바쳐 \u0026lsquo;단 하나의 걸작\u0026rsquo;을 갈망했지만, 결국 완성된 그림이 아닌 치열했던 고뇌의 흔적이 예술로 남게 된 사례는 예술사에서 매우 비극적이기도 하지만 낭만적인 테마로 꼽힌다. 나만의 방식으로 나아가자.\n이제 무엇부터 시작할까? 생각보다 매우 간단하다.\n날것의 생각을 담는 안전한 공간 마련 Draft, Sandbox, Moonshot 등 내가 마음껏 어지를 수 있는 공간을 만든다. Draft - 과정의 유연함: 초안, 시안 등으로 언제든 다시 수정될 수 있는 의미로 사용된다. Sandbox - 안전한 실험: 아이들이 노는 공간인 놀이터 바닥을 의미하며 놀다가 크게 다치지 않도록 마련된 푹신한 모래 바닥을 의미한다. Moonshot - 확장된 가능성: \u0026lsquo;달을 향해 로켓을 쏘아 올린다\u0026rsquo;는 뜻으로 단순히 조금 더 나은 개선이 아니라 거대하고 불가능해 보이는 목표에 도전하는 것 PARA 안에서 관리해도 되고 최상위에 생성하는 것도 좋다. 이걸 고민하는 것 자체가 이 시스템을 이해하지 못한 것이다. \u0026lsquo;나의 언어\u0026rsquo;로 요약하여 PARA에서 관리 모국어와 외국어를 구분하는 게 아니라 내가 빠르게 이해할 수 있는 \u0026lsquo;나의 언어\u0026rsquo;가 중요하다. Data view, Smart Connections 등 나의 문서를 DB로 활용하고 메모를 손쉽게 연결하자. 필터와 연결을 통해 끊임 없이 나의 메모가 나에게 신호를 줄 수 있도록 하자. 도서관이 아닌 생태계를 만들자.\n🎯결론 지식 관리는 완성된 결과물을 모으는 것이 아니라 불완전한 생각들이 서로 부딪히며 성장하는 생태계를 가꾸는 과정이다.\n\u0026ldquo;이 메모가 나중에 나에게 어떤 가치를 줄 것인가?\u0026rdquo; 질문해 보자. 어디 내놓기도 부끄러운 생각이 모여서 계속해서 부끄러움을 유지한다면 성장하지 못한 결과고 그것대로 받아들여야 한다.\n어떤 방향으로든 움직여야 잘못된 방향인 것도 알 수 있고 다시 방향을 잡을 수 있다고 생각한다.\n","date":"2025-12-30T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-12-30-the-zettelkasten-x-para/","title":"Zettelkasten x PARA"},{"content":"📌개요 옵시디언을 사용하며 블로그는 따로 정리하거나 재작성해서 Velog, Tistory 등 개발 블로그를 업로드 했었다. 그러다 문득 학습하고 정리하면서 블로그에 배포도 되면 편하겠다 생각했다.\nQuartz, Jekyll, Hugo 다양한 정적 사이트 생성기를 알아보며 선택한 기준과 배포해서 사용하다가 불편한 점을 만나 전환한 계기도 간략하게 정리한다.\n거창하게 표현하자면 옵시디언을 SSoT로 삼아 플랫폼에 종속되지 않는 지속 가능한 블로그 아키텍처를 설계한다.\n요약하면 낭만 쉽지 않다.\n📌내용 블로그 선택 기준 Velog, Tistory 등 내가 작성하고 게시만 하면 되고 첨부 파일도 신경 쓸 게 없다. 간편하고 좋다. SSG로 블로그를 운영하다가 다시 돌아가거나 둘 다 운영하는 유저도 많은 것 같다.\n나만의 블로그라는 게 좋아서 정보를 찾아보다가 가장 쉬운 건 Quartz였는데 테마가 아쉬웠다. 적당히 쉽고 테마가 다양하고 빠르게 시작할 수 있어서 처음엔 Jekyll 블로그를 배포했다.\n다루기 쉬운가? 구분 Quartz Jekyll Hugo 타겟 옵시디언 사용자 단순 블로그 입문자 대규모 사이트/전문가 설치 편의성 매우 쉬움 보통(Ruby 설치 필요) 쉬움(파일 하나면 끝) 커스텀 난이도 쉬움(JS/TS 기반) 보통(Liquid 문법) 약간 높음(Go 문법, JS 파츠 가능) 테마가 다양한가? Jekyll 가장 오래되어 테마가 수천 개에 달한다. 웬만한 디자인은 이미 누군가 만들었다. Hugo 인기가 많아지면서 세련되고 현대적인 테마가 많아졌다. Quartz 옵시디언 퍼블리싱이라는 특수한 목적에 집중하기 때문에 일반적인 블로그 테마 종류는 적은 것 같다. 대신 소스 수정을 통한 커스텀이 쉽다. Jekyll에서 Hugo로 전환한 이유 빌드가 너무 느리다. 포스팅 100개도 안 되는데 빌드 시간이 5분 이상 걸리는 것 보고 위기감을 느꼈다. 캐싱이나 빌드 최적화 방법도 다양한데 일단 이런 작업을 해주어야 할 정도로 느리다는 게 불편했다.\nHugo 압도적 1위 기술 기반: Go 언어(컴파일 언어) 성능: 세계에서 가장 빠른 SSG라는 슬로건을 내걸 만큼 빠르다. 체감 속도: 문서가 1,000개일 때 빌드 시간이 수 초 내외다. 문서가 10,000개가 넘어가도 수 초 안에 빌드가 완료된다고 한다. 대규모 기술 문서 사이트에 Hugo가 가장 많이 쓰이는 이유라고 한다. Quartz 준수한 성능 2위 기술 기반: TS/Node.js(하이브리드, 컴파일/트랜스파일 + JIT 컴파일) 성능: Hugo만큼은 아니지만 상당히 빠르다. 내부적으로 캐싱 시스템이 잘 되어 있어 변경된 부분만 빠르게 업데이트한다. 체감 속도: 수백~수천 개의 옵시디언 노트를 처리하기에 충분히 쾌적하다. 다만 복잡한 JS 플러그인을 많이 추가할 수록 조금씩 느려질 수 있다. Jekyll 비교적 느림 3위 기술 기반: Ruby (인터프리터 언어) 성능: 구조적인 한계로 문서 양이 많아지면 속도가 눈에 띄게 저하된다. 체감 속도: 문서가 수천 개를 넘어가면 빌드에 몇 십 분 단위가 걸리기도 한다. 증분 빌드 기능이 있지만 다른 SSG에 비해 느리다. 전환하며 만난 문제 블로그 전환하면서 문서의 속성 값이 문제가 됐었다. 카테고리를 작성하는데 4~5레벨 이상 허용하는 테마도 있고 그렇지 않은 것도 있다.\n그냥 세분화한 카테고리가 제대로 적용만 안 되는 거면 수정 없이 사용하겠는데 빌드가 안 되거나 문서가 깨지는 등 문제가 있었다.\n근데 작성된 문서가 많다면 일괄 편집 프로그램을 찾거나 직접 커스텀하게 만들어서 사용해야 할 것이다.\n다행히 DataView 플러그인으로 카테고리는 따로 관리하고 있었고 문서가 엄청 많은 상태는 아니어서 속성의 카테고리를 다 제거했다.\n그리고 Hugo 테마에 맞게 정리해서 일단락되었다.\nTip DataView 플러그인은 DQL(Dataview Query Language, SQL과 굉장히 비슷하다), DataviewJS(JavaScript) 언어로 사용할 수 있다. 쿼리가 익숙하든 JS가 익숙하든 편한 걸로 사용하면 될 것 같다.\n직관적인 이름에 걸맞게 옵시디언에서 관리하는 문서들의 데이터를 가지고 내가 원하는 뷰를 만들 수 있다.\n무엇을 하고자 하는가? 플랫폼에 종속되지 않는 문서 관리가 가능한지 고민하고 최대한 표준화하려고 한다. 첨부 파일을 양쪽에서 관리하는데 이건 좀 문제가 있다. PARA 정리법을 적용해서 잘 사용하고 있기 때문에 경로가 메인이 되면 안 된다. 관리될 수 있는 속성을 정의해야 한다. Jekyll에서 Hugo로 전환하면서 느낀 것 Templater 플러그인을 사용해서 문서 성격에 따라 개인적으로 필요한 양식은 표준화 했지만 이런 전환점에선 수정이 불가피하다.\n정착할 수 있다면 좋겠지만 언제든 맘 편히 전환할 수 있는 기반을 만들고 싶다.\n첨부 파일 관리도 불편 어디에 둬야 하는 거야? 호스팅으로 빼서 URL을 사용하는 등 써드 파티를 적용한 사례도 많다. 무료 이미지 호스팅 사용하다가 사이트가 없어져서 이미지 다 날린 케이스도 있고 비교적 안전하게 운영되고 있는 imgur 플러그인 소개도 가끔 보인다. 나는 가장 간편하고 안전하게 볼트 내에서 첨부 파일 폴더를 따로 관리하려고 한다.\n문법은 왜 달라? 왜 불편하냐면 옵시디언에서 붙여 넣거나 링크한 건 이런 위키링크 형태다.\n![[location/image.png]] Markdown 문법이 기대하는 이미지 첨부는 이런 형태다.\n![alt](location/image.png) 사실 이건 설정에서 끌 수 있고 직접 만들어서 간단히 치환하는 방법을 사용할 수도 있다. 설정 \u0026gt; Options/Files and Links \u0026gt; Use[[WikiLinks]] 이거 비활성화 하면 기본 마크다운 형태로 삽입할 수 있다.\n지금 가장 불편한 건 낭만 따라간답시고 일 벌린 다음 잘못 관리하고 있는 것이다.\n옵시디언에서 신나게 이미지 넣고 작성한다. Hugo 블로그 저장소에 문서 옮기고 이미지도 옮기고 링크 맞추고.. 오탈자 없나, 깨지는 건 없나 검토하는 단계로 합리화하기. 이렇게 불편함이 많아지면 포스팅 하나 작성해볼까? 하는 데에 결심이 필요해진다. 안 된다. 개선이 필요하다.\nTip PARA란? (PARA method) 디지털 자료를 관리하는 하나의 방법론이다.\n하나의 경로에 고정되는 데이터가 아니라 현재 내 관심사에 따라 Project, Resources 폴더로 왔다가 Area, Archive로 갔다가 유연하게 문서가 움직인다.\nPARA의 자료들은 계속해서 이동하며 장기 기억으로 가져가야 할 데이터, 데드라인을 명확히 하고자 하는 프로젝트, 우선 순위를 재조정 한다거나 계속한 상관 관계에 있다. 한 곳으로 옮긴 이후 완료되거나 포기하면 죽는 데이터가 아니라 계속해서 돌아가는 자원이 되는 방식쯤으로 이해한다.\nProject 데드라인이 정해진 작업 즉, 완료가 필요한 것들 Area 데드라인이 명확하지 않은 관리하는 것들의 영역 ex) 건강, 재정, 공부 등.. Resource 지속적으로 관심을 가지는 주제의 자료들 Area를 이루기 위한 것들 Archive 위의 것들 중 마무리 지었거나 더이상 사용하지 않는 것들을 보관 어떻게 할 수 있는가? 내가 게시할 문서만 동기화하는 건 그리 복잡하지 않겠지만 외부 호스팅 의존이 필요한 이미지가 문제였다.\n그냥 이미지 Base64로 박아버리면 첨부 파일 따로 안 해도 되잖아? 미리 보기 모드가 아닌 수정 모드에서 엄청난 스압을 만나고 문서 용량도 커진다. 확장자를 달고 링크된 이미지가 아니라서 CPU가 열일하는 모습을 볼 수 있다. 이미 이미지 압축, 스압 줄여주는 플러그인도 있다. ㅋㅋ 이것저것 고민해보다가 나중에 또 개선해야 하는 거 너무 깊게 고민하지 않기로 했다.\n준비물 옵시디언 볼트 GitHub 저장소로 올리고 Private/Public 나눠야 할 수도 있다. GitHub 저장소의 이미지 서빙 기능을 활용하기 위해 이미지 저장소를 따로 만들어야 할 수도 있다. Hugo 블로그 저장소 GitHub Pages로 배포 PoC(Proof of Concept)를 정의해보자 이번 아키텍처의 핵심 목적은 문서를 복사하거나 이미지를 옮기는 수동 작업을 없애고 옵시디언의 PARA 체계를 유지하면서 특정 속성만으로 블로그를 배포하는 것.\n검증 과제 파일 선별: 보관함 전체가 아닌 publish: true 속성을 가진 파일만 추출 가능한가? 공개되지 않아야 하는 것들을 분리할 수 있는가? 경로 동기화: Git Submodule을 통해 옵시디언의 첨부 파일과 Hugo 저장소의 첨부 파일을 물리적으로 일치 시킬 수 있는가? GitHub 저장소 자체가 호스팅 서버 역할을 하며 Hugo가 빌드 과정에서 상대 경로를 기반으로 URL을 생성하는가? 문법 변환: 옵시디언 전용 문법(Callouts 등)을 Hugo가 이해하는 표준 마크다운이나 숏코드로 자동 치환 가능한가? 배포 자동화: 가벼운 명령어로 빌드 및 배포가 완료되는가? 다이어그램 graph TD %% 상단: 소스 관리 (Private) subgraph Source_Private Obs[Obsidian_Repo] end %% 중단: 공유 자원 (Public) subgraph Shared_Resources Assets[Assets_Repo] end %% 하단: 배포 엔진 (Public) subgraph Deployment_Public Hugo[Hugo_Repo] end %% 관계 설명 (서브모듈 포함 관계) Assets --- |Submodule| Obs Assets --- |Submodule| Hugo %% 데이터 흐름 (CI/CD) Obs -.-\u003e |\"Filter(publish:true) \u0026 Push\"| Hugo %% 스타일링 style Obs fill:#fff4dd,stroke:#d4a017,stroke-width:2px style Assets fill:#e1f5fe,stroke:#01579b,stroke-width:3px style Hugo fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px sequenceDiagram participant Obs as Obsidian_Repo (Private Vault) participant ObsAction as GitHub_Actions (Obsidian Repo) participant AssetRepo as Assets_Repo (Submodule / Public) participant HugoRepo as Hugo_Repo (Public, GitHub Pages) Note over Obs: 문서 작성 (Properties에 publish: true 추가) Obs-\u003e\u003eObs: git push (Private Repo로) Obs-\u003e\u003eObsAction: 1. Push 이벤트 트리거 rect rgb(240, 240, 240) Note over ObsAction: [추출 및 변환 단계] ObsAction-\u003e\u003eObsAction: 2. 보관함 스캔 (publish: true 검색) ObsAction-\u003e\u003eObsAction: 3. 마크다운 변환 (WikiLink -\u003e MarkDown, 또는 설정으로 대체) end ObsAction-\u003e\u003eAssetRepo: 4. 신규 이미지 Push (Submodule 업데이트) ObsAction-\u003e\u003eHugoRepo: 5. 추출된 글(md) \u0026 이미지 링크 전달 (Git Push) Note over HugoRepo: [자동 배포 단계] HugoRepo-\u003e\u003eHugoRepo: 6. Hugo 빌드 프로세스 시작 Note over HugoRepo: 7. 빌드 결과물(HTML) 생성 및 호스팅 시작 Note over HugoRepo: https://ID.github.io 접속 가능 첨부 파일을 어떻게 관리해야 할까? 옵시디언 볼트에서 관리 중인 Assets 폴더를 저장소로 만들어 원격에 업로드 Assets 원격 저장소를 옵시디언 볼트의 Submodule로 지정 하게 되면 git은 Submodule을 특별하게 취급기 때문에 따로 .gitignore 파일에 Assets 폴더를 추가하지 않아도 된다. 이미지를 추가하는 경우 Submodule 폴더인 Assets 폴더에 추가. 옵시디언 프로그램에서 복사한 이미지 붙여 넣기의 경우 설정에 의해 지정된 경로에 이미지가 생성된다. Submodule 폴더인 Assets 폴더로 지정. 3과 동일하게 동작 변경 사항을 어떻게 업로드 하지? 이후 옵시디언 볼트에서 작업 후 변경 사항을 push하려면?\nAssets 저장소를 먼저 push. 명령어 만들어두면 편할 듯? 편해? 이게? 1 2 3 cd Assets \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -m \u0026#34;update assets: {{date}}\u0026#34; \u0026amp;\u0026amp; git push cd .. git add . \u0026amp;\u0026amp; git commit -m \u0026#34;vault backup: {{date}}\u0026#34; \u0026amp;\u0026amp; git push Assets 업데이트 된 후 옵시디언 볼트 push 옵시디언 원격 저장소의 GitHub Actions 실행된다. GitHub Actions는 기능 수행만 하는 것이 아니라 임시 서버가 된다. GitHub Actions 실행되는 환경 안에서 새롭게 포스팅 되어야 하는 문서들 필터링. 조건은 publish: true 뿐인데 이미 게시된 건 어떻게 알고 새로운 것만 배포에 사용하지? 방식 A: 전체 덮어쓰기(가장 확실) 배포할 때마다 Hugo 저장소의 content/posts 폴더를 비운다. 현재 Obsidian에서 publish: true인 모든 파일을 다시 복사해서 넣는 방식 장점: 상태 관리가 필요 없고 Obsidian에서 publish: false로 바꾼 글이 블로그에서 자동으로 사라진다. 단점: 파일이 아주 많아진다면 아주 미세하게 시간이 더 걸리지만, Hugo와 Git은 변경되지 않은 파일은 무시하므로 효율적 방식 B: 파일 비교(증분 방식) git diff 명령어를 사용하여 마지막 배포 커밋 이후 변경되거나 추가된 파일만 찾기 하지만 이 방식은 Obsidian에서 글을 수정했을 때 블로그에 반영하기가 까다로워 보통 방식 A를 권장합니다. Obsidian 저장소와 Hugo 저장소를 동시에 Clone 필터링 스크립트 실행 (Python이나 Shell 스크립트) 찾은 파일들을 Hugo 저장소의 content/posts 경로로 복사 Actions 서버가 사용자 권한을 가지고 Hugo 저장소에 원격 Push 이미지 관리 물리적 전송은 없다. 1번 단계에서 Assets를 원격에 업로드했기 때문에 Hugo 저장소는 빌드될 때 Assets Submodule의 최신 커밋을 바라보게만 설정 Hugo 저장소의 배포 Actions가 실행될 때 git submidule update --remote 명령어를 수행하여 최신 이미지를 스스로 긁어오기 첨부 파일 저장소 만들고 Submodule 지정 이 저장소는 이제 옵시디언 볼트 저장소와 블로그 배포 저장소에서 서브 모듈로 활용하게 된다.\n첨부 파일 저장소 업로드 [Obsidian Vault]/[attachment location] 안의 소스를 잠시 백업 별도 저장소로 업로드 부모 저장소에서 Submodule 지정 Obsidian 루트 폴더에서 해당 폴더를 삭제하거나 이름을 변경 서브 모듈 등록 1 git submodule add [원격_저장소_URL] [로컬에서_사용할_경로] 등록 중 멈추는 경우 한 번에 주고 받을 수 있는 데이터의 용량 제한 때문일 수 있다. 부모 저장소에서 데이터 용량 제한을 늘린다. 1 2 # 버퍼 크기를 낭낭하게 500MB로 설정 git config --global http.postBuffer 524288000 그럼 로컬 앱에선 이미지가 잘 표현된다. 서브 모듈에 변경이 발생하면 부모 저장소에서도 변경 사항이 있다고 표시된다. 내용은 없지만 서브 모듈의 최신 상태를 바라보기 위한 포인터가 변경되는 거라고 한다.\n서브 모듈 커밋 업로드, 부모 저장소에서 업데이트 두 번의 작업을 해야 하니 그냥 부모 저장소에선 서브 모듈의 변경을 무시하고 GitHub Actions에서 항상 서브 모듈의 최신 상태를 바라보도록 설정해야겠다.\n1 2 # 서브모듈의 포인터 변화(커밋 변경)를 무시하도록 설정 git config -f .gitmodules submodule.\u0026#34;[로컬에서_사용할_경로]\u0026#34;.ignore all 명령어가 정상적으로 실행되면 .gitmodules 파일에 다음과 같이 기록된다.\n1 2 3 4 [submodule \u0026#34;[로컬에서_사용할_경로]\u0026#34;] path = [로컬에서_사용할_경로] url = [원격_저장소_URL] ignore = all 만약 지정하다가 실수하면 .gitmodules, .git/config 여기저기 기록이 남기 때문에 깔끔하게 지우는 절차가 중요하다.\n해당 경로를 제거 .git/modules/ 내부 보관소에서 서브 모듈 데이터 삭제 실제 폴더가 남아있다면 물리적으로 삭제 필요하다면 변경 사항 커밋 올바르게 다시 등록 만약 이미 존재하는 폴더라는 오류가 발생한다면 캐시를 비워야 할 수도 있다. 블로그 저장소에서 Submodule 지정 또 문제가 있다. 계획대로라면\n이제 첨부 파일은 서브 모듈로 분리해서 관리 배포 시 post 경로는 새롭게 비운 뒤 업데이트 수동으로 낭만 있게 관리한다고 합리화하며 다음과 같은 트리로 관리하고 있던 구조를 바꿔야 한다.\n1 2 3 4 5 6 7 8 9 10 Hugo_Blog/content/post ├─ [YEAR] │ ├─ [MONTH] │ │ └─ [DATE-TITLE] │ │ ├─ index.md │ │ ├─ image1.[jpg|png] │ │ └─ image2.[jpg|png] │ └─ ... ├─ ... └─ README.md 일단 백업해두고 한 번 비우고 배포 돌리면 채워지겠지 뭐.\n문서는 content/post 정적 파일은 static/assets/images, 이게 서브 모듈 성능이 우려되는데 일단 되는지 보자 지금 계획은 Obsidian 커밋과 업로드 이후 GitHub Actions가 동작한다. 다음과 같은 작업이 필요하고 $O(n)$의 복잡도를 가지게 된다.\n그나마 최적화를 위해 모든 문서의 상단만 읽고 게시 가능한지 판단 http, https 등으로 시작하는 이미 외부 URL인 것을 제외한 로컬에서 등록한 이미지라고 판단되는 것들을 Hugo 구조에 맞게 치환 간단히 최적화 방안이 있는지만 확인\nPARA 체계를 이용하지만 대부분 Resource에서 작업하니까 여기만 스캔 Area, Archive 갔다가 문서 개정이 필요한 경우 Resource로 옮겨서 배포 후 정리 grep을 활용한 고속 필터링: 모든 파일을 열어서 내용을 확인하는 대신 파일 시스템 레벨에서 게시 가능 플래그를 포함한 파일 목록만 먼저 뽑기 1 2 # 내용에 \u0026#39;publish: true\u0026#39;가 포함된 파일 리스트만 추출 grep -l \u0026#34;publish: true\u0026#34; *.md \u0026gt; publish_list.txt sed 또는 perl 이용한 일괄 치환: 파일을 하나씩 열어 저장하는 방식이 아니라 스트림 방식으로 메모리에서 바로 치환. 이런 식으로 1 2 # http 또는 https로 시작하지 않는(?!http) 이미지 링크만 타겟팅 perl -i -pe \u0026#39;s/!\\[\\[(?!https?:\\/\\/)(.*?)\\]\\]/!\\[\\[\\/assets\\/$1\\]\\]/g\u0026#39; [대상파일] Git Diff를 활용한 증분 배포 1 2 # 마지막 커밋 이후 바뀐 파일만 골라내기 git diff --name-only HEAD~1 이걸로 기대할 수 있는 것? 복잡도는 $O(n)$이지만 실제 작업량$(n)$을 필터링으로 줄이고 작업 속도를 스트림 처리로 극대화\nPARA 체계와 grep의 시너지 전체 문서에서 Resource로 스캔 범위 제한 grep은 파일을 메모리에 다 올리지 않고 바이너리 수준에서 패턴만 훑고 지나가기 때문에 수천 개의 파일을 찾아도 거의 파일 시스템 탐색 속도와 맞먹음 perl 스트림 처리로 속도 최적화 파일을 열고 닫는 오버헤드가 없음 한 줄씩 흐르듯 지나가며 치환하므로 텍스트 데이터 처리에 있어서는 이론적으로 최대 속도 Git Diff 증분 배포 $(n)$을 전체 문서-Resource-오늘 내가 수정한 문서로 범위 제한 문서가 많아져도 오늘 3개만 작업했다면 작업량은 3개뿐 그 다음으로 고민해볼 것\n이미지 존재 여부 체크: 치환된 이미지 파일이 실제 서브 모듈에 있는지 체크하여 누락된 이미지가 있다면 빌드 타임에 경고 rsync 활용: 파일이 변경된 경우에만 복사하므로 I/O를 한 번 더 아낄 수 있음 Obsidian 저장소에서 GitHub Actions 작성 publish: true인 파일만 골라낸다. 위키 링크를 표준 마크다운으로 바꾸고 이미지 경로를 assets로 변경 가공된 파일을 Hugo 저장소로 보내기, 이후 Hugo 저장소에서 빌드 트리거 GitHub Token 설정 Obsidian 저장소가 Hugo 저장소에 접근하여 파일을 push해야 하므로 PAT(Personal Access Token) 필요\nGitHub 설정 \u0026gt; Developer settings \u0026gt; Personal access tokens에서 생성 권한: repo 전체 생성한 토큰을 Obsidian 저장소의 Settings \u0026gt; Secrets and variable \u0026gt; Actions에 등록 .github/workflows/deploy.yml Info v1.0.2까지 기록됨\n현재 버전은 위키링크 변환 시 OS에서 허용하는 파일명의 특수 기호, 공백 등을 고려하지 않아 문제가 발생할 수 있음.\n이후 저장소 문서로 관리 예정.\n옵시디언의 위키링크와 로컬 이미지 경로를 AST 기반 Node 변환 스크립트로 표준 마크다운 및 형식으로 변환하도록 구현 deploy.yml에서 기존 Perl 치환을 제거하고 해당 스크립트 호출로 교체 트리거 및 타겟팅 제어 특정 경로 감시: 02.Resource/** 폴더 내의 변경 사항이 발생할 때만 배포가 실행되도록 제한하여 불필요한 빌드를 방지한다. 멀티 저장소 체크아웃: 옵시디언 저장소뿐만 아니라 배포 대상인 Hugo 저장소를 동시에 관리한다. 파일 스캐닝 (awk 활용) 조건부 배포 필터링: 모든 파일을 배포하는 것이 아니라, 파일 상단 15줄 이내에 publish: true 또는 false 설정이 있는 파일만 추출한다. 경로 확장성: 주석 처리를 통해 03.Area, 04.Archive 등 다른 PARA 폴더로의 확장 가능성 마크다운 변환 (Perl 정규식 활용) 코드 블록 보호: 변환 시 코드 블록(\u0026hellip;) 내부나 인라인 코드는 건드리지 않도록 스킵 로직 적용 링크 체계 자동 보정: 옵시디언 전용 위키링크를 표준 마크다운 이미지 링크로 변환 모든 로컬 이미지 경로를 Hugo 기준으로 자동 보정 Front Matter 동적 변환: publish 상태값에 따라 Hugo가 인식하는 draft 필드(true/false)를 자동으로 삽입하여 배포 상태를 동기화 동기화 및 업서트 (UPSERT) rsync 체크섬 비교: 단순히 파일을 덮어쓰는 것이 아니라, --checksum 옵션을 통해 실제 내용의 변경이 있는 파일만 선별적으로 업데이트 임시 스테이징: temp_posts 폴더에서 모든 변환 작업을 마친 후 최종본만 대상 저장소에 동기화하여 안정성 확보 자동 커밋 및 배포 최적화 변경 감지: git diff --cached --quiet 명령을 사용하여 실제로 변경된 내용이 있을 때만 커밋을 생성 이력 추적: 커밋 메시지에 배포 시각을 포함하여 버전 관리를 용이하게 한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 name: Deploy Posts to Hugo on: push: branches: - main # 브랜치명 paths: # PARA 체계 중 배포 대상이 포함된 폴더만 감시 - \u0026#39;02.Resource/**\u0026#39; # - \u0026#39;03.Area/**\u0026#39; # - \u0026#39;04.Archive/**\u0026#39; jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout Obsidian Vault uses: actions/checkout@v4 with: fetch-depth: 0 # 증분 배포(diff)를 위해 전체 이력 가져오기 - name: Set up Hugo Repo uses: actions/checkout@v4 with: repository: [계정]/[Hugo_저장소_이름] token: ${{ secrets.HUGO_DEPLOY_TOKEN }} path: hugo-dest # Hugo 저장소를 임시 폴더에 체크아웃 - name: Filter and Transform Posts run: | # 1. 폴더를 비우지 않고, 변환 작업을 위한 임시 스테이징 폴더 생성 mkdir -p temp_posts mkdir -p hugo-dest/content/post/ # 대상 폴더가 없을 경우 대비 # 2. 하위 폴더 탐색 및 awk 실행 # shopt -s globstar: ** 패턴 사용을 위한 셸 옵션 활성화 shopt -s globstar PUBLISH_FILES=$(awk \u0026#39; FNR \u0026lt;= 15 \u0026amp;\u0026amp; /publish: (true|false)/ { print FILENAME; nextfile } FNR \u0026gt; 15 { nextfile } # 스캔 범위 02.Resource로 제한할 때 활성화 \u0026#39; 02.Resource/**/*.md || true) # 스캔 범위 일시적으로 늘릴 때 활성화 #\u0026#39; 02.Resource/**/*.md 03.Area/**/*.md 04.Archive/**/*.md || true) if [ -z \u0026#34;$PUBLISH_FILES\u0026#34; ]; then echo \u0026#34;배포 대상 파일이 없습니다.\u0026#34; exit 0 fi # 3. 파일 복사 및 변환 대상을 temp_posts로 지정 # 공백이 포함된 파일명을 안전하게 처리하기 위해 IFS 설정 SAVEIFS=$IFS IFS=$\u0026#39;\\n\u0026#39; for file in $PUBLISH_FILES; do # 파일명만 추출 filename=$(basename \u0026#34;$file\u0026#34;) # hugo-dest가 아닌 임시 폴더(temp_posts)에 먼저 복사 dest=\u0026#34;temp_posts/$filename\u0026#34; # 복사 (파일 경로에 따옴표 필수) cp \u0026#34;$file\u0026#34; \u0026#34;$dest\u0026#34; # 4. Perl 통합 치환 (코드 블록 보호 및 경로 보정) # - 동적 백틱 개수 대응: 시작한 백틱 개수만큼 닫는 백틱이 나올 때까지 스킵 # - 인라인 코드 보호: 한 줄 내의 백틱 영역 스킵 # - 위키링크 변환 `![[]]` -\u0026gt; `![]()` 및 표준링크 프리픽스 보정을 한 번의 스캔으로 처리 perl -i -0777 -pe \u0026#39; s/ (?:^|\\n)(`{3,})[\\s\\S]*?\\n\\1(?:\\n|$) (*SKIP)(*F) | `[^`\\n]+` (*SKIP)(*F) | (?: !\\[\\[(?!https?:\\/\\/)(.*?)\\]\\] | !\\[(.*?)\\]\\((?!https?:\\/\\/|\\/assets\\/images\\/)(.*?)\\) ) / $2 ? \u0026#34;![](\\/assets\\/images\\/$2)\u0026#34; : \u0026#34;!\\[$3\\](\\/assets\\/images\\/$4)\u0026#34; /gex\u0026#39; \u0026#34;$dest\u0026#34; # [추가] publish: false 발견 시 바로 다음 줄에 draft: true 삽입, Hugo에서 제공하는 front matter 활용 # s/찾을패턴/대체패턴/g 활용 perl -i -pe \u0026#39;s/^(publish:\\s*false)/$1\\ndraft: true/g\u0026#39; \u0026#34;$dest\u0026#34; perl -i -pe \u0026#39;s/^(publish:\\s*true)/$1\\ndraft: false/g\u0026#39; \u0026#34;$dest\u0026#34; perl -i -pe \u0026#39;s/^image:\\s*$/# image: /g\u0026#39; \u0026#34;$dest\u0026#34; done # 5. rsync를 이용한 UPSERT (변경 상태 비교 및 동기화) # --checksum: 파일 내용이 실제로 변했는지 체크 rsync -av --checksum temp_posts/ hugo-dest/content/post/ # IFS 복구 IFS=$SAVEIFS - name: Push to Hugo Repo run: | cd hugo-dest git config user.name \u0026#34;github-actions[bot]\u0026#34; git config user.email \u0026#34;github-actions[bot]@users.noreply.github.com\u0026#34; git add . # 변경사항이 있을 때만 커밋 if ! git diff --cached --quiet; then git commit -m \u0026#34;Update posts from Obsidian (at $(date +\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;))\u0026#34; git push origin master else echo \u0026#34;변경 사항이 없어 배포를 진행하지 않습니다.\u0026#34; fi Tip 마크다운 표준에서는 코드 블록 내부에 ```이 들어갈 경우 외부 감싸개는 그보다 많은 백틱 4개(````)를 사용하도록 권장한다.\n만약 코드 내부에 백틱 4개가 들어있다면 외부는 5개로 감싸야 하며, 이 개수에는 제한이 없다. 반드시 바깥쪽이 안쪽보다 최소 1개는 더 많아야 한다.\n파일 탐색 방식 기술 비교 방식 기술적 명칭 특징 (비유) 효율성 grep 고속 전수 조사 책을 아주 빨리 넘기며 단어를 찾지만, 결국 끝까지 다 읽음. 중간 (단순하지만 낭비 있음) find + head 개별 정밀 조사 책 한 권을 꺼내 앞부분만 보고 다시 꽂기를 반복. (책을 꺼내고 넣는 동작이 너무 많음) 낮음 (오버헤드 큼) awk 조건부 조기 종료 스캔 책을 펼쳐 앞부분에 단어가 없으면 즉시 다음 책으로 넘어감. 최상 (가장 지능적임) 이미지가 왜 자꾸 안 나와? Obsidian 저장소, Hugo 저장소에서 Asssets 저장소를 서브 모듈로 등록했다. 로컬 앱에선 이미지가 잘 보인다. 근데 Hugo 빌드 및 배포하면 포스팅에 이미지가 안 나온다?\nHugo는 내부적으로 루트에 assets 폴더가 있고 static 폴더는 정적 리소스로 빌드에 포함된다. assets 하위 img 폴더는 사이트 UI에 사용되는 것 같은데 static 폴더에 등록해둔 서브 모듈의 소스는 왜 배포된 포스팅에 제대로 노출이 안 되지?\nGitHub 저장소의 소스를 다른 곳에 서빙하기 위한 권한 문제인가? 프리픽스가 잘못됐나? 그냥 루트의 assets 폴더에 배치해야 하나? 근데 static 폴더가 정적 리소스로 사용하기 위해 설계된 구조인데 왜지?\nHugo 저장소 GitHub Actions의 yml에서 submodule 정의를 하지 않아서 빌드할 때 포함되지 않았던 것. 빌드할 때 submodule에 대한 정의가 없어서 그냥 빈 폴더처럼 다루고 있었던 것이었다. 경로는 내가 계획한 게 맞았고 Actions 실행되는 동안 내부적으로 그 소스에 접근해서 빌드를 생성할 수 있게 설정이 필요했다.\n올바르게 구성하고 이미지 확인까지 한 후 기존의 포스팅을 다시 점검하는 시간을 가지게 됐다.\nProperties 관리 문서 상단 Properties를 표준화해서 관리한다면 이후에 다른 모듈로 변경할 때 상당히 간편한 전환을 기대할 수 있겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 publish: true draft: false title: 문서 제목 description: 문서 설명 author: 작성자 date: 작성일시 categories: - Level1 - Level2 tags: - tag1 - tag2 Status: ToDo 상태 유지형 업데이트 개선 방향 현재는 문서량이 적으므로 전수 조사 후 전체 복사 방식이 가장 에러가 없고 관리하기 편하다.\n지금처럼 모두 지우고 새로 복사하는 것이 아니라 파일의 변경 상태를 비교하여 필요한 동작만 수행하는 방식으로 개선해보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # AS-IS: 전체 삭제 후 복사 # 1. 일단 다 지움 (이 때문에 Git 히스토리가 매번 끊김) rm -rf hugo-dest/content/post/* mkdir -p hugo-dest/content/post/ # 2. 파일마다 바로 Hugo 저장소로 복사 for file in $PUBLISH_FILES; do dest=\u0026#34;hugo-dest/content/post/$filename\u0026#34; cp \u0026#34;$file\u0026#34; \u0026#34;$dest\u0026#34; # 여기서 치환을 수행 (치환 중 실패하면 Hugo 저장소 파일이 오염됨) done # TO-BE: 임시 폴더 작업 후 rsync 동기화 # 1. 기존 파일을 지우지 않고 임시 작업장 생성 mkdir -p temp_posts # 2. 임시 폴더에서 안전하게 모든 변환 완료 for file in $PUBLISH_FILES; do dest=\u0026#34;temp_posts/$filename\u0026#34; cp \u0026#34;$file\u0026#34; \u0026#34;$dest\u0026#34; # Perl 치환 수행 (성공한 결과물만 임시 폴더에 쌓임) done # 3. rsync로 실제 바뀐 파일만 골라서 Hugo 저장소에 \u0026#39;동기화\u0026#39; rsync -av --checksum temp_posts/ hugo-dest/content/post/ \u0026ldquo;신규 게시, 업데이트하고 싶은 건 Resource로 가져온다\u0026quot;는 단 하나의 원칙만 지키면 된다. Obsidian은 UPSERT에 집중하고 Hugo는 게시 상태를 판단한다.\n수정(Update): Resource에서 내용을 고치고 배포하면 Hugo에 이미 있는 같은 이름의 파일을 덮어쓴다. 누적(Accumulate): Resource에서 작업이 끝나 Area로 옮겨버려도 다음 배포 시 Resource만 스캔하므로 Hugo에 이미 들어가 있는 파일은 건드리지 않고 그대로 유지된다. Info Hugo - Front matter 문서 상단 속성의 publish 값으로 게시 상태를 판단하려고 했는데 Hugo에서 기본적으로 draft 값으로 게시 상태를 다룰 수 있어서 이걸 활용한다.\n🎯결론 옵시디언 데이터를 SSoT로 삼아 GitHub Actions 환경에서 입맛에 맞게 소스를 가공한 뒤 원하는 배포 툴을 이용해 빌드 및 배포하며 GitHub 저장소를 활용해서 첨부 파일을 서빙하는 구조를 완성했다.\n각 저장소가 어떤 책임을 가지는지, GitHub Actions에서 어떤 처리를 해주면 처리가 간편해지는지 고민해볼 수 있어서 좋았다.\n호스팅 쓰자. 🙂\n⚙️EndNote 사전 지식 Static Site Generator (SSG): 정적 사이트 생성기. 마크다운 같은 텍스트 파일을 빌드 시점에 정적 HTML로 변환해주는 도구로 Hugo, Jekyll, Quartz 등이 대표적이다. Git Submodule: 하나의 Git 저장소 안에 다른 Git 저장소를 하위 폴더로 포함하는 기능이다. GitHub Actions (CI/CD): 코드가 Push될 때 특정 작업을 자동으로 수행하는 도구다. 여기서는 문서 필터링, 문법 치환, 파일 전송의 자동화 엔진 역할을 한다. Regular Expression (Regex): 정규표현식. 특정 패턴의 텍스트를 찾고 바꾸는 규칙이다. 위키링크(![[]])를 마크다운 링크(![]())로 변환하는 등에 핵심적으로 사용된다. 더 알아보기 PARA Method: 디지털 정보를 관리하는 4단계 체계(Project, Area, Resource, Archive)로, 효율적인 노트 관리를 돕는 프레임워크. Tiago Forte의 공식 웹사이트에서 더 상세한 내용을 볼 수 있다. Hugo Documentation: 세계에서 가장 빠른 SSG인 Hugo의 공식 문서. gohugo.io에서 다양한 테마와 빌드 옵션을 확인할 수 있다. GitHub Fine-grained PAT: 기존 Classic 토큰보다 보안이 강화된 세분화된 접근 토큰 설정 방법이다. 특정 저장소에만 읽기/쓰기 권한을 부여하는 방법 등 권한 관련 학습하면 좋다. Perl One-liner: 스크립트 파일 없이 터미널에서 즉시 실행하는 Perl 명령어 활용법이다. 대용량 파일의 스트림 기반 텍스트 처리에 강력한 성능을 발휘한다. Incremental Builds: 모든 파일을 다시 빌드하지 않고 변경된 파일만 처리하는 방식이다. git diff 활용 전략을 확장하여 빌드 시간을 더 단축하는 방법을 탐구해 볼 수 있다. ","date":"2025-12-19T03:24:06+09:00","permalink":"https://blog.b9f1.com/p/2025-12-19-obsidian-and-blog-wrting-a-note-post-it/","title":"옵시디언과 블로그"},{"content":"📌개요 누구나 한 번쯤 음수와 음수를 곱할 때($(-) \\times (-)$) 또는 음수 앞에 마이너스 기호가 있을 때($-(-)$) 왜 그 결과가 양수가 되는지 근원적인 의문을 가져봤을 것이다.\n이 현상은 단순히 외워야 할 규칙이 아니라, 수학적 일관성이라는 거대한 논리적 체계를 유지하기 위해 필연적으로 도출된 정의의 산물이다. 이 정의가 어떻게 확립되었는지 그리고 그 논리적 배경을 알아 보고 싶다.\n📌내용 0.먼저 정수 연산의 약속들을 보자 Info 공리(公理, Axiom): 증명 없이 참으로 받아들이는 가장 기본적인 원리나 가정 이론 체계의 출발점이 되는 \u0026lsquo;자명한 진리\u0026rsquo; 즉, 근거가 된다. 정의(定義, Definition): 어떤 사물이나 개념의 뜻을 명확하게 정해 놓은 약속 \u0026lsquo;개념을 규정\u0026rsquo;하는 것 이름 붙이기, 불릴 수 있는 이름이 생긴다. 우리가 다루는 정수의 사칙연산 구조를 이해하기 위해서는 이 구조가 성립하기 위해 필수적으로 필요한 몇 가지 공리와 정의를 짚고 넘어가야 한다.\n공리: 연산의 근본 규칙 공리는 어떤 수학적 체계를 세울 때 증명 없이 참이라고 인정하고 출발하는 근본적인 규칙이다. 정수의 연산을 규정하는 핵심 공리 중 하나가 바로 분배 법칙이다.\n분배 법칙: 곰셈과 덧셈을 연결하는 기본 규칙 역할: 이 법칙은 정수의 세계가 무너지지 않고 일관성 있게 작동하기 위한 최소한의 약속이다. $(a(b + c) = ab + ac)$ 정의: 용어와 기호의 약속 정의는 특정 개념이나 기호가 수학적 맥락에서 무엇을 의미하는지 명확하게 약속하는 것\n덧셈에 대한 항등원($0$의 정의): 어떤 수와 더해도 그 수 자체가 되는 수 $a+0 = a$ 덧셈에 대한 역원($-a$의 정의): 어떤 수 $a$와 더했을 때 반드시 $0$이 되게 하는 수. 우리는 이 역원을 $-a$라고 표기하기로 약속한다. $a+(-a) = 0$ 1.의문: 이 규칙은 정의인가? 설명 가능한가? 질문은 다음과 같다.\n$(-a) \\times (-b) = ab$ 가 성립하는 이유. $-(-a) = a$ 가 성립하는 이유. 이 두 가지 현상은 수학적 일관성을 유지하기 위한 정의인 동시에, 그 정의가 필연적으로 도출되는 명확한 설명이 가능한 현상이다.\n만약 이 규칙이 아니라면, 우리가 이미 알고 있는 분배 법칙 등의 기본적인 수학 규칙이 정수 전체에서 성립하지 않는 모순에 빠지게 된다.\n2.해결: 이중 마이너스 부호의 필연성 두 가지 의문 중 $-(-a) = a$ 규칙은 덧셈의 역원 정의만 가지고도 직관적으로 해결된다.\n이중 마이너스의 의미 $-(-a)$는 $-a$라는 수의 덧셈에 대한 역원을 의미한다.\n논리적 필연성 우리는 이미 정의에 의해 $-a+a = 0$ 임을 알고 있다.\n이 식은 $-a$라는 수에 $a$를 더했더니 $0$이 되었다는 뜻이다. 따라서 $-a$의 덧셈에 대한 역원은 필연적으로 $a$일 수밖에 없다.\n$$ \\therefore -(-a) = a $$이것은 수직선 상에서 반대의 반대로 돌아와 원래 위치가 되는 것과 같다.\n3.해결: 음수 곱하기 음수의 필연적 증명 $(-a) \\times (-b) = ab$가 성립해야 하는 이유는 우리가 가장 신뢰하는 분배 법칙을 정수 전체에 걸쳐 일관성 있게 유지하기 위함이다.\n우리는 가장 기본적인 형태인 $(-1) \\times (-1) = 1$을 증명함으로써 그 필연성을 확인한다.\n1.0을 통한 출발 곱셈에서 어떤 수와 0을 곱하면 0이 되어야 한다는 규칙을 사용한다.\n$$ (-1) \\times 0 = 0 $$2.역원 정의와 분배 법칙의 적용 $0$ 대신 역원의 정의$(1+(-1)=0)$를 대입한다.\n$$ (-1) \\times (1+(-1)) = 0 $$이제 공리인 분배 법칙을 적용하여 괄호를 풀어준다.\n$$ ((-1) \\times 1) + ((-1) \\times (-1)) = 0 $$3.일관성 유지를 위한 대입 우리는 이미 $양수 \\times 음수 = 음수$라는 규칙이 일관성을 위해 필연적으로 정해졌음을 알고 있다. 따라서 $(-1) \\times 1 = -1$이다.\n$$ (-1) + ((-1) \\times (-1)) = 0 $$4.결론: 역원의 위치 위 식은 $-1$에 $(-1) \\times (-1)$을 더했더니 $0$이 되었다는 뜻이다. 따라서 $(-1) \\times (-1)$은 $-1$의 덧셈에 대한 역원이어야 한다.\n$$ \\therefore (-1) \\times (-1) = 1 $$이 증명은 $음수 \\times 음수 = 양수$ 규칙이 수학적 모순을 피하고 기존의 분배 법칙이라는 공리를 지키기 위한 선택이었음을 보여준다.\n🎯결론 음수의 부호가 양수로 변하는 조건은 덧셈의 역원의 정의와 분배 법칙이라는 근본적인 공리를 모든 정수에서 일관성 있게 적용하려는 수학적 필연성이다.\n이 규칙들은 단순한 암기가 아닌 모순이 없는 완벽한 논리 구조를 구축하고자 했던 수학자들의 노력이 담긴 정의의 산물이 아닐까\n⚙️EndNote 사전 지식 공리 (Axiom): 증명 없이 참으로 간주되는 수학적 출발점 수학에는 분배 법칙 외에도 결합 법칙, 교환 법칙 등 다양한 공리가 있다. 정의 (Definition): 특정 개념이나 기호의 의미를 명확하게 약속하는 것 덧셈에 대한 역원 (Additive Inverse): 어떤 수 $a$와 더했을 때 $0$이 되게 하는 수 ($-a$) 더 알아보기 수학적 일관성(Mathematical Consistency) 환(Ring)과 체(field)의 공리 ","date":"2025-12-08T23:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-12-08-why-is-the-negative-number-multiplied-by-the-negative-number-positive/","title":"음수 * 음수 = 양수?"},{"content":"📌개요 현재 프로젝트에서는 PostgreSQL(RDS), Kafka(Confluent Cloud)와 함께 Redis를 세션/캐시용으로 운영해야 하는 상황이다.\nRedis를 로컬에서 docker-compose로 테스트는 했지만 운영 환경에서의 연결 구조와 설정 원리를 명확히 이해하고자 한다.\n현재 계획은 ECS Fargate 기반 분산 환경이며 Redis를 어떻게 배포하고 연결하는지 간단히 정리해본다.\n📌내용 Redis는 기본 구성만으로 동작한다 Redis는 HTTP 기반이 아닌 TCP 기반의 Key-Value 서버다. HTTP처럼 경로 개념이 없고 단순히 정의된 포트(기본 6379)로 열려 있는 소켓을 통해 명령을 주고 받는다.\n즉, Redis는 복잡한 인증이나 핸드셰이크 없이도 클라이언트가 IP와 포트를 알고 있으면 바로 접근 가능한 구조다.\nRedis는 host:port 기반의 TCP 프로토콜만 있으면 동작한다. 별도의 서비스 레이어가 없으므로 설정의 핵심은 어디로 붙을 것인가. Spring Boot에서 Redis 연결 구조 Spring Boot는 spring-boot-starter-data-redis 의존성만 추가하면 간단하게 LettuceConnectionFactory, RedisTemplate, CacheManager 등을 등록한다.\n1 2 3 4 5 spring: data: redis: host: redis-cluster.xxxxxx.apn2.cache.amazonaws.com port: 6379 별도의 Bean을 등록하지 않아도 Spring Boot는 이를 감지해 자동 연결한다. 설정을 생략하면 기본값(localhost:6379)으로 시도한다.\nElastic Redis 배포 개요 AWS에서는 ElastiCache Redis OSS를 사용하면 서버 설치 없이 관리형 Redis 클러스터를 사용할 수 있다.\n구성 요소 설명 Primary Node 쓰기 담당 Redis 노드 Replica Node 읽기 전용 복제본 (선택사항) Cluster 여러 샤드로 데이터를 분산 저장 Endpoint 연결용 주소 (.cache.amazonaws.com) ECS Fargate 연결 구조 보안 그룹 규칙 Inbound: 6379 허용 (ECS -\u0026gt; Redis) Outbound: ECS에서 Redis SG(Security Group)로 접근 허용 VPC: 같은 VPC 또는 피어링된 네트워크에 존재해야 함 로컬 Docker와 운영 Redis의 관계 구분 로컬 환경 운영 환경 호스트 localhost:6379 Elasticache Endpoint 목적 개발 및 단위 테스트 실제 서비스 운영 공유 여부 완전히 분리 완전히 분리 차이점 단일 인스턴스 멀티 AZ, 클러스터링, 모니터링 제공 즉, 코드상으로는 동일한 RedisTemplate을 사용하지만 환경 변수(SPRING_DATA_REDIS_HOST, SPRING_DATA_REDIS_PORT)만 달라진다.\n호스트와 포트를 맞추어 TCP 레벨에서 연결만 되면 Redis는 별도 복잡한 설정 없이 바로 동작한다.\n운영 시 고려사항 운영 환경에서는 다음 항목들을 통해 보안과 안정성을 강화할 수 있다.\n구분 설명 보안 그룹 제한 Redis 포트(6379)는 오직 ECS 서비스의 보안 그룹에서만 접근 가능하도록 설정한다. VPC 내부 통신 퍼블릭 액세스를 비활성화하고, VPC 내부 트래픽으로만 접근을 제한한다. 암호화 옵션 Elasticache 생성 시 ‘전송 중 암호화(Encryption in-transit)’ 및 ‘저장 시 암호화(Encryption at-rest)’를 활성화한다. Redis AUTH (선택) OSS 기본은 인증 미지원이지만, 필요 시 Redis Enterprise 또는 프록시 계층을 통해 비밀번호 인증을 구성할 수 있다. 세션 공유 설정 Spring Boot 세션 스토어를 Redis로 변경 (spring.session.store-type=redis). TTL 관리 캐시 만료시간을 반드시 설정 (Duration.ofMinutes(...) 등). 모니터링 CloudWatch, Redis SlowLog, CloudWatch Metrics 등을 활용해 지표와 성능을 추적한다. 🎯결론 Redis는 기본적으로 포트만 맞으면 동작하지만,\n운영 환경에서는 보안 그룹, VPC, 암호화 옵션을 통해 최소한의 방어선을 만들 수 있다.\nSpring Boot에서는 host와 port만 지정하면 자동으로 연결되고 Elasticache Redis는 이를 VPC 내부에서 안전하게 관리해준다.\n⚙️EndNote 사전 지식 AWS ECS / VPC / Security Group 개념 Redis Key-Value 구조 및 기본 명령 Spring Boot 자동 설정 (spring-boot-starter-data-redis) 더 알아보기 Redis OSS 호환 Amazon ElastiCache Spring Data Redis Reference Redis serialization protocol specification VPC 피어링을 사용하여 VPC 연결 Kafka 와 Redis 의 Pub/Sub 비교 ","date":"2025-10-16T17:17:14+09:00","permalink":"https://blog.b9f1.com/p/2025-10-16-deploy-aws-elasticache-redis/","title":"AWS ElastiCache Redis 배포"},{"content":"📌개요 이벤트 기반 아키텍처에서 Kafka는 시스템 간 메시지 정합성을 유지하는 핵심 역할을 한다. 그런데 이 Kafka를 어떻게 배포할 것인지 선택해야 한다.\n직접 서버에 구축할 수도 있고 AWS MSK 같은 클라우드 관리형 서비스를 쓸 수도 있고 Confluent Cloud처럼 Kafka 전문 업체의 완전 관리형 서비스를 이용할 수도 있다.\n각 배포 방식의 특징과 장단점을 비교하여 프로젝트에 맞는 Kafka 인프라 선택 기준을 알아보자.\n📌내용 Kafka 배포 방법 Kafka를 배포하는 방법은 크게 세 가지로 나눌 수 있다.\n1. 자체 배포 직접 서버에 Kafka 설치 및 운영\n배포 환경: 온프레미스 서버, AWS EC2, Azure VM, GCP Compute Engine 등 설치 방법: 바이너리 다운로드 후 수동 설치 Docker/Kubernetes를 통한 컨테이너 배포 Ansible, Terraform 등 IaC 도구 활용 구성: Kafka 브로커 클러스터 (보통 3대 이상) 메타데이터 관리 (Kafka 4.x는 KRaft 자체 관리) 모니터링 스택 (Prometheus, Grafana 등) Kafka Connect, Schema Registry 등 별도 구축 장점 완전한 제어권: 모든 설정과 튜닝 가능 비용 최적화: 대규모 트래픽에서 인프라 비용 절감 데이터 주권: 온프레미스나 특정 리전에 데이터 보관 커스터마이징: 특수한 요구사항 반영 가능 단점 높은 운영 부담: 2-3명의 전담 인력 필요 전문성 요구: Kafka 내부 구조 이해 필수 장애 대응: 24/7 온콜 체계 구축 필요 초기 구축 시간: 안정화까지 수주~수개월 소요 2. 클라우드 관리형 서비스 클라우드 제공자의 Kafka 관리 서비스 이용\nAWS MSK (Managed Streaming for Kafka) AWS가 Kafka 클러스터 및 인프라 관리 VPC 내부에서 프라이빗 연결 CloudWatch 통합 모니터링 자동 패치 및 버전 업그레이드 Azure Event Hubs for Kafka Kafka 프로토콜 호환 이벤트 스트리밍 서비스 Azure 네이티브 통합 Google Cloud Managed Kafka (Preview) GCP에서 관리하는 Kafka 서비스 장점 인프라 관리 자동화: 서버 관리, 패치 불필요 클라우드 생태계 통합: Lambda, S3, IAM 등 쉽게 연결 빠른 프로비저닝: 클릭 몇 번으로 클러스터 생성 자체 배포보다 낮은 운영 부담 단점 제한된 제어권: 일부 고급 설정 불가 Kafka 지식 여전히 필요: 토픽, 파티션, 컨슈머 그룹 관리는 직접 추가 도구 별도 구축: Kafka Connect, Schema Registry 등 비용: 자체 배포보다 비쌈 (편의성 프리미엄) 3. 완전 관리형 서비스 Kafka 전문 업체의 SaaS형 서비스\nConfluent Cloud: Kafka 창시자들이 만든 엔터프라이즈 플랫폼 Aiven for Apache Kafka: 멀티 클라우드 지원 Instaclustr: 관리형 Kafka 서비스 Confluent Cloud 중심 설명\n제공하는 것 Kafka 클러스터 완전 관리 (인프라부터 운영까지) 120+ 사전 구축 커넥터 (DB, SaaS, 클라우드) Schema Registry 관리형 제공 ksqlDB 및 Apache Flink 통합 (스트림 처리) Stream Catalog (데이터 계보 추적) 고급 보안 및 컴플라이언스 (SOC 2, HIPAA 등) 24/7 전문가 지원 장점 제로 운영 부담: 인프라 걱정 없이 바로 사용 개발 생산성 극대화: 커넥터, 스키마 관리 등 즉시 활용 자동 스케일링: 트래픽 변화에 자동 대응 멀티 클라우드/리전: 글로벌 서비스 쉽게 구축 빠른 실험: PoC부터 프로덕션까지 빠르게 전환 단점 비용: 소규모에서는 경제적이나 대규모에서는 비쌈 벤더 락인: Confluent 고유 기능 사용 시 이전 어려움 제어권 제한: 내부 설정 접근 불가 데이터 외부 전송: 온프레미스 전용 환경에서는 사용 불가 배포 방식별 비교 매트릭스 비교 항목 자체 배포 클라우드 관리형 (AWS MSK) 완전 관리형 (Confluent Cloud) 운영 복잡도 높음 (전담 팀 필요) 중간 (Kafka 지식 필요) 낮음 (제로 운영) 초기 구축 시간 수주~수개월 수일~1주 수분~수시간 전문 인력 필요 필수 (2-3명) 필요 (1명) 선택 (개발자만) Kafka 지식 요구 깊은 이해 필수 중급 수준 기본 개념만 비용 (소규모) 높음 (인력+인프라) 중간 낮음~중간 비용 (대규모) 낮음 (최적화 시) 중간 높음 확장성 수동 계획 필요 반자동 완전 자동 보안/컴플라이언스 직접 구현 기본 제공 엔터프라이즈급 커넥터/도구 직접 구축 직접 구축 120+ 즉시 사용 글로벌 복제 수동 구성 복잡함 클릭 몇 번 적합한 규모 월 10TB+ 중소 규모 스타트업~중견 실전 선택 가이드 자체 배포를 선택해야 할 때 대규모 트래픽: 월 10TB 이상의 안정적인 트래픽 온프레미스 필수: 데이터 외부 반출이 불가능한 환경 특수 커스터마이징: Kafka 내부 동작을 깊이 제어해야 할 때 장기적 비용 최적화: 인프라 비용을 최소화해야 할 때 이미 운영 중: Kafka 운영 경험과 전담 팀이 있는 경우 비용 예시 (AWS 기준, 2025년 10월)\nEC2 브로커 3대 (t3.medium): 월 약 25만원 EBS 스토리지 500GB: 월 약 5만원 네트워크 비용: 변동 인력 비용: 월 500만원+ (DevOps 엔지니어 1명) 총 비용: 월 530만원+ (인프라만 30만원) 클라우드 관리형(AWS MSK)을 선택해야 할 때 AWS 중심 아키텍처: Lambda, S3 등과 긴밀한 통합 필요 중간 규모: 월 1TB~10TB 정도의 트래픽 운영 부담 경감: 인프라 관리는 자동화하고 싶지만 Kafka는 직접 관리 클라우드 벤더 선호: AWS 생태계 내에서 모든 것을 해결 보안 요구사항: AWS IAM, VPC와의 통합 필요 비용 예시 (AWS MSK, 2025년 10월)\nkafka.t3.small 3대: 월 약 $150 (약 20만원) 스토리지: 별도 과금 인력 비용: 월 300만원+ (Kafka 지식 보유 개발자) 총 비용: 월 320만원+ 완전 관리형(Confluent Cloud)을 선택해야 할 때 빠른 MVP 검증: 이벤트 스트리밍 아키텍처를 신속하게 실험 전담 인력 부족: Kafka 전문가 채용이 어렵거나 비용 부담 소규모 스타트업: 월 100GB~1TB 정도의 트래픽 글로벌 서비스: 멀티 리전 복제가 필요한 경우 개발 집중: 인프라 걱정 없이 비즈니스 로직에 집중 규제 산업: SOC 2, HIPAA 등 컴플라이언스 인증 필요 비용 예시 (Confluent Cloud, 2025년 10월)\nUSD 1달러 = 약 1,419원 기준\nBasic 클러스터: 월 $730 (약 100만원) 스토리지 50GB: 월 $5 (약 7천원) 네트워크 100GB: 월 $9 (약 1만원) 인력 비용: 0원 (일반 개발자만 있으면 됨) 총 비용: 월 101만원 (소규모) 프리 크레딧: $400 제공 (약 55만원 상당) 하이브리드 전략: 단계적 접근 많은 기업이 채택하는 현실적인 전략:\nPhase 1: 초기 단계 (Confluent Cloud)\nPoC 및 MVP 빠르게 검증 초기 서비스 런칭 (월 트래픽 ~1TB) 팀 규모: 개발자 2-3명 비용: 월 100~200만원 Phase 2: 성장 단계 (계속 Confluent 또는 MSK 검토)\n트래픽 증가 (월 1TB~10TB) 비용 분석 시작 Kafka 전문 인력 1명 채용 고려 비용: 월 200~500만원 Phase 3: 성숙 단계 (자체 배포 전환 검토)\n대규모 트래픽 (월 10TB+) 전담 인프라 팀 구성 자체 배포로 전환하여 TCO 60% 절감 비용: 월 500~1,000만원 (하지만 트래픽은 10배+) 병행 사용 예시\n핵심 서비스: 자체 배포 (안정성, 비용 최적화) 실험적 프로젝트: Confluent Cloud (빠른 실험) 글로벌 리전: Confluent Cloud (멀티 리전 자동 복제) 🎯결론 세 가지 배포 방식은 각각 명확한 장단점이 있으며 정답은 조직의 상황에 달려 있다.\n자체 배포 는 대규모 트래픽과 전문 인력이 있을 때 최적의 선택이다. 완전한 제어권과 비용 최적화를 얻지만 운영 복잡도와 인력 투자가 크다.\n클라우드 관리형(AWS MSK) 은 AWS 생태계를 쓰면서 운영 부담을 줄이고 싶을 때 중간 지점이다. 인프라는 자동화되지만 Kafka 지식은 여전히 필요하다.\n완전 관리형(Confluent Cloud) 은 빠른 실행과 제로 운영 부담이 필요할 때 최선이다. 소규모에서는 인력 비용 고려 시 가장 경제적이지만 대규모에서는 비용이 높아질 수 있다.\nKafka 인프라를 관리하는 데 시간을 쓸 것인지 아니면 고객 가치를 만드는 데 집중할 것인지 스스로 질문해보자.\n⚙️EndNote 사전 지식 필수 개념\nApache Kafka 기본: Topic, Partition, Producer, Consumer, Broker 이벤트 기반 아키텍처: 비동기 메시징, 이벤트 소싱 클라우드 서비스 모델: IaaS, PaaS, SaaS 차이 알아두면 좋은 개념\n분산 시스템: 복제, 파티셔닝, 합의 알고리즘 DevOps: CI/CD, 모니터링, IaC (Infrastructure as Code) Kafka 생태계: Kafka Connect, Kafka Streams, Schema Registry 더 알아보기 공식 문서\nApache Kafka Documentation - 오픈소스 공식 문서 Confluent Documentation - Confluent Platform 문서 AWS MSK Documentation - AWS 관리형 Kafka CONFLUENT의 어필 - Confluent와 Apache Kafka® 비교 핵심 개념 심화\nKafka Connect: 외부 시스템(DB, SaaS)과 데이터 파이프라인 구축 Schema Registry: Avro/Protobuf 스키마 버전 관리 Kafka Streams: 자바 기반 스트림 처리 라이브러리 ksqlDB: SQL로 스트림 처리 Apache Flink: 대규모 상태 기반 복잡한 스트림 처리 AWS 연동 가이드\nAWS Lambda + Kafka 연동 VPC 피어링으로 Confluent Cloud 연결 MSK vs Confluent Cloud 비교 ","date":"2025-10-16T14:48:14+09:00","permalink":"https://blog.b9f1.com/p/2025-10-16-kafka-vs-confluent-open-source-and-managed-data-streaming-infrastructure/","title":"Kafka vs Confluent"},{"content":"📌개요 PC에서 여러 GitHub 계정과 프로젝트에서 SSH 키를 효율적으로 관리하는 방법을 정리한다.\n📌내용 SSH 키 구성 예시 SSH 키 파일들 1 2 3 4 5 6 ~/.ssh/ ├── id_ed25519 # 기본/개인 계정용 개인 키 ├── id_ed25519.pub # 기본/개인 계정용 공개 키 ├── id_ed25519_company # 회사 계정용 개인 키 ├── id_ed25519_company.pub # 회사 계정용 공개 키 └── config # SSH 설정 파일 SSH Config 설정 (~/.ssh/config) Tip ~ 경로는 대부분의 OS에서 사용자 폴더를 의미한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 기본 GitHub 설정 (개인 계정용) Host github.com HostName github.com User git IdentityFile ~/.ssh/id_ed25519 # 개인 계정 명시적 설정 (기본과 동일, 필요 시 변경) Host github.com-personal HostName github.com User git IdentityFile ~/.ssh/id_ed25519 # 회사 계정용 설정 Host github.com-company HostName github.com User git IdentityFile ~/.ssh/id_ed25519_company 사용 방법 1. 기본 사용 일반적인 GitHub URL을 그대로 사용:\n1 2 git clone git@github.com:username/my-project.git git remote add origin git@github.com:username/repository-name.git 2. 명시적 개인 계정 사용 1 2 git clone git@github.com-personal:username/repository-name.git git remote add origin git@github.com-personal:username/repository-name.git 3. 회사 계정 사용 1 2 git clone git@github.com-company:CompanyOrg/repository-name.git git remote add origin git@github.com-company:CompanyOrg/repository-name.git 새로운 계정/키 추가 방법 1. 새 SSH 키 생성 1 ssh-keygen -t ed25519 -C \u0026#34;your-email@example.com\u0026#34; -f ~/.ssh/id_ed25519_newaccount 2. SSH Config에 설정 추가 1 2 3 4 5 # ~/.ssh/config에 추가 Host github.com-newaccount HostName github.com User git IdentityFile ~/.ssh/id_ed25519_newaccount 3. GitHub에 공개 키 등록 1 2 # 공개 키 내용 복사 cat ~/.ssh/id_ed25519_newaccount.pub GitHub Settings \u0026gt; SSH and GPG keys에서 등록\n4. 사용 1 git clone git@github.com-newaccount:NewAccount/repository-name.git SSH 연결 테스트 각 계정별로 SSH 연결을 테스트할 수 있다.\n1 2 3 4 5 6 7 8 # 기본 계정 ssh -T git@github.com # 개인 계정 명시적 테스트 ssh -T git@github.com-personal # 회사 계정 ssh -T git@github.com-company 성공시 다음과 같은 메시지가 출력된다.\n1 Hi [계정명]! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. 프로젝트별 설정 확인 현재 프로젝트가 어떤 SSH 키를 사용하는지 확인\n1 2 git remote -v git config --list | grep remote.origin.url 권한 설정 SSH 파일들의 올바른 권한 설정은 Unix/Linux 기반 시스템에서 필수다.\n권한 설정이 필요한 환경 ✅ Linux/Ubuntu ✅ macOS ✅ WSL (Windows Subsystem for Linux) ✅ Docker 컨테이너 ✅ 개발 컨테이너 (Dev Container) ❌ Windows 네이티브 (NTFS 권한으로 자동 관리) Windows vs Unix/Linux 차이점 환경 권한 시스템 SSH 권한 설정 Windows 네이티브 NTFS ACL 불필요 (자동) WSL/Linux/macOS Unix 권한 필수 Docker/Dev Container Unix 권한 필수 Windows에서 SSH 사용하기 Windows 네이티브 환경 1 2 3 4 5 6 # PowerShell 또는 Command Prompt에서 # SSH 키 생성 ssh-keygen -t ed25519 -C \u0026#34;your-email@example.com\u0026#34; # SSH 키는 보통 C:\\Users\\[username]\\.ssh\\ 에 저장 # 권한 설정은 Windows가 자동으로 관리 Windows 네이티브 특징:\nchmod 명령어 없음 NTFS 권한으로 자동 관리 SSH 클라이언트가 적절한 권한 자동 설정 수동 권한 설정 불필요 WSL 환경 1 2 3 4 5 # WSL에서는 Linux와 동일하게 권한 설정 필요 chmod 700 ~/.ssh chmod 600 ~/.ssh/config chmod 600 ~/.ssh/id_* chmod 644 ~/.ssh/*.pub WSL 특징:\nLinux 파일 시스템 사용 Unix 권한 모델 적용 권한 설정 필수 macOS: Darwin SSH가 권한을 엄격하게 요구하는 이유 (Unix/Linux) Unix/Linux 시스템에서 SSH가 엄격한 권한을 요구하는 이유: 보안 정책: 개인 키가 다른 사용자에게 노출되는 것을 방지 인증 무결성: 권한이 올바르지 않으면 SSH 연결 자체가 거부됨 파일 보호: 중요한 인증 정보의 무단 접근 차단 올바른 권한 설정 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # SSH 디렉토리 권한 (소유자만 접근 가능) chmod 700 ~/.ssh # config 파일 권한 (소유자만 읽기/쓰기) chmod 600 ~/.ssh/config # 개인 키 파일들 권한 (소유자만 읽기/쓰기) chmod 600 ~/.ssh/id_* # 공개 키 파일들 권한 (소유자 읽기/쓰기, 다른 사용자 읽기만) chmod 644 ~/.ssh/*.pub # 소유권 설정 (현재 사용자로 설정) sudo chown -R $USER:$USER ~/.ssh 권한 확인 1 2 3 4 5 6 7 # 현재 권한 상태 확인 ls -la ~/.ssh/ # 올바른 출력 예시: # drwx------ (700) 소유자만 SSH 디렉토리 접근 # -rw------- (600) 소유자만 개인 키/config 파일 접근 # -rw-r--r-- (644) 공개 키는 다른 사용자도 읽기 가능 문제 해결 권한 오류 해결 오류 메시지\n1 2 Bad owner or permissions on /home/node/.ssh/config fatal: Could not read from remote repository. 원인: SSH 파일들의 권한이 너무 개방적이거나 소유권이 잘못됨 해결 단계: 권한 확인: 1 ls -la ~/.ssh/ 권한이 777 (rwxrwxrwx)인 경우 수정: 1 2 3 4 sudo chmod 700 ~/.ssh sudo chmod 600 ~/.ssh/config sudo chmod 600 ~/.ssh/id_* sudo chmod 644 ~/.ssh/*.pub 소유권 문제인 경우: 1 sudo chown -R $USER:$USER ~/.ssh 권한 설정 후 테스트: 1 ssh -T git@github.com SSH 키가 인식되지 않는 경우 SSH Agent에 키 추가\n1 2 3 eval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_ed25519 ssh-add ~/.ssh/id_ed25519_company 잘못된 키 사용 원격 URL 확인 및 수정\n1 2 3 4 5 # 현재 설정 확인 git remote -v # 원격 URL 변경 git remote set-url origin git@github.com-[account]:owner/repository.git 예시 프로젝트 설정 my-blog: 기본 설정 사용 (git@github.com:username/my-blog.git)\nSSH 키: ~/.ssh/id_ed25519 GitHub 계정: 개인 계정 확장 계획 향후 새로운 계정이나 프로젝트가 추가될 때는 패턴으로 관리\n1 2 3 4 Host github.com-[account-name] HostName github.com User git IdentityFile ~/.ssh/id_ed25519_[account-name] 이를 통해 체계적이고 확장 가능한 SSH 키 관리가 가능하다.\n현재 구성 검증 현재 SSH 구성이 올바르게 작동하는지 확인해본다.\n권한 설정 (필수!):\n1 2 3 4 5 # 모든 권한을 한 번에 설정 sudo chmod 700 ~/.ssh sudo chmod 600 ~/.ssh/config ~/.ssh/id_* sudo chmod 644 ~/.ssh/*.pub sudo chown -R $USER:$USER ~/.ssh SSH 연결 테스트:\n1 2 ssh -T git@github.com # 성공시: \u0026#34;Hi [username]! You\u0026#39;ve successfully authenticated...\u0026#34; Git push 테스트:\n1 git push origin master ⚠️ 중요: 권한 설정 없이는 SSH가 작동하지 않을 수 있다. Bad owner or permissions 오류가 발생하면 반드시 위의 권한 설정을 먼저 수행하자.\n🎯결론 로컬/WSL/컨테이너 등에서 SSH 충돌 없이 깔끔하게 멀티 계정을 운용하는 방법을 정리해봤다.\n새 계정 추가는 키 생성, ~/.ssh/config에 Host 추가, 공개키 등록, ssh -T 테스트, 원격 URL 설정의 5단계 WSL은 리눅스 홈 경로에 .ssh를 두고, 필요 시 includeIf/url.insteadOf로 디렉터리별 자동 라우팅을 적용하면 실수할 일이 줄어든다. ⚙️EndNote 사전 지식 SSH 키 쌍: ed25519가 권장(짧고 안전, 빠름). 공개키는 등록·공유, 개인키는 비밀 유지. SSH 설정 파일: ~/.ssh/config로 Host 단위 설정(별칭, IdentityFile, IdentitiesOnly). 퍼미션 기초: ~/.ssh(700), 개인키(600), 공개키/config(644~600). WSL/NTFS 차이: NTFS 마운트 경로는 퍼미션 판정이 달라 OpenSSH가 거부할 수 있음. 리눅스 홈 사용. ssh-agent: 메모리 상에 키를 보관·서명. 키가 많으면 인증 실패가 늘어남 → IdentitiesOnly로 해결. Git 원격 URL 형태: git@HOST:OWNER/REPO.git(SCP 구문) 또는 ssh://git@HOST/OWNER/REPO.git. Git 라우팅 자동화(선택): ~/.gitconfig의 includeIf(디렉터리 기준 설정), url.\u0026lt;base\u0026gt;.insteadOf(호스트 치환). 더 알아보기 GitHub Docs: Connecting to GitHub with SSH, About SSH certificate authorities, Managing multiple accounts OpenSSH Manual: ssh(1), ssh_config(5), ssh-agent(1), ssh-add(1) Git Documentation: git-config(1)(특히 includeIf, url.\u0026lt;base\u0026gt;.insteadOf), git-remote(1) Microsoft Docs: WSL 파일 권한 \u0026amp; NTFS 상호작용 가이드 보안 심화: ed25519-sk(FIDO2/보안키 기반), ProxyJump(점프호스트), KnownHosts 고급 설정 ","date":"2025-10-15T18:30:33+09:00","permalink":"https://blog.b9f1.com/p/2025-10-15-git-multi-ssh-key-management/","title":"Git 멀티 SSH 키 관리"},{"content":"📌개요 우아한테크코스 8기 프리코스 과제 진행 요구 사항에서 제공하는 커밋 메시지 컨벤션인 AngularJS Git Commit Message Conventions의 번역 내용을 정리하고 요약해본다.\n📌AngularJS Commit Message Convention 요약 및 의의 왜 이런 컨벤션이 필요한가 AngularJS는 오픈소스 프로젝트로 수백 명의 개발자가 동시에 기여했기 때문에 모든 커밋이 일관된 규칙 없이 작성되면 히스토리를 추적하기 어렵고 릴리스마다 어떤 변경이 있었는지 정리하기가 굉장히 복잡했을 것.\n즉, “코드 변경 이력의 가독성”과 “자동화 가능한 기록 관리”를 위한 규칙이었다.\n이 컨벤션이 제공하는 주요 목적 목적 설명 CHANGELOG 자동 생성 커밋 메시지의 패턴이 일정하기 때문에, feat, fix, docs 등 키워드를 기준으로 자동으로 릴리스 노트를 만들 수 있다. 의미 없는 커밋 무시 포맷 변경이나 주석 수정 같은 비핵심 커밋을 구분해 bisect(이진 검색) 과정에서 제외할 수 있다. 히스토리 탐색 효율화 커밋만 봐도 “어떤 모듈(scope)에서, 어떤 변경(type)이 있었는지”를 바로 이해할 수 있다. 팀 간 의사소통 통일 기여자가 많더라도 커밋 메시지의 구조가 같기 때문에 리뷰, 코드 히스토리, 배포 관리가 명확해진다. 자동화 친화적 구조 릴리스 스크립트나 CI 파이프라인에서 커밋을 분석해 changelog, 버전 태깅, 배포 메시지를 자동 생성할 수 있다. AngularJS 방식의 구체적 특징 항목 설명 형식화된 구조 \u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt; 구조를 고정 → 시각적 일관성 유지 Type 구분 명확 feat, fix, docs, refactor, style, test, chore 등으로 의도를 명확히 구분 Scope 활용 $compile, $http처럼 수정된 영역을 명시해 검색과 필터링이 용이 Breaking Change 규정화 BREAKING CHANGE: 키워드로 하위 호환성 깨짐을 명시 → 릴리스 관리 안정화 Issue 연동 Closes #123 등으로 GitHub 이슈 자동 닫기 가능 이런 컨벤션의 장점 자동화 가능한 릴리스 파이프라인 커밋 로그만으로 “어떤 변경이 포함되었는지” 기계적으로 분류 가능 semantic-release, standard-version 같은 도구와 궁합이 좋다 버전 관리(예: Semantic Versioning: major/minor/patch)를 자동화 가능 커뮤니케이션 비용 절감 리뷰어나 동료가 커밋만 보고도 “무엇을, 왜 바꿨는지” 즉시 이해 코드 리뷰 시 불필요한 맥락 설명이 줄어듦 장기 유지보수에 강함 수년 뒤에도 “이 변경이 왜 필요했는가?”를 추적 가능 git blame, git log 활용 시 문맥 파악이 쉬움 대규모 팀, 오픈소스 프로젝트에서 특히 효과적 컨벤션 기반 문화 정착 일관된 규칙은 “프로젝트의 품질 기준”이 된다 코드뿐 아니라 커밋도 문서화의 일부가 된다 📌AngularJS Commit Message Convention 이 규칙은 The AngularJS commit conventions에서 채택한 것이다.\n목표 (Goals) 스크립트를 이용해 CHANGELOG.md를 자동으로 생성할 수 있도록 한다. git bisect 시 중요하지 않은 커밋(예: 포맷 변경 등)을 무시할 수 있도록 한다. 히스토리를 탐색할 때 더 나은 정보를 제공한다. CHANGELOG.md 생성 변경 로그(changelog)는 다음 세 가지 섹션으로 구성된다:\n새로운 기능 (new features) 버그 수정 (bug fixes) 호환성 깨짐 (breaking changes) 이 목록은 릴리스 시 관련 커밋 링크와 함께 스크립트를 통해 자동으로 생성될 수 있다.\n물론 실제 릴리스 전에 수동으로 수정할 수 있지만, 기본 뼈대를 자동으로 만들 수 있다.\n이전 릴리스 이후 모든 커밋 메시지의 첫 줄 목록을 확인하려면 다음 명령을 사용한다:\n1 git log \u0026lt;last tag\u0026gt; HEAD --pretty=format:%s 현재 릴리스의 새로운 기능만 확인하려면:\n1 git log \u0026lt;last release\u0026gt; HEAD --grep feature 중요하지 않은 커밋 식별하기 형식 변경(공백 추가/제거, 들여쓰기 수정), 세미콜론 누락, 주석 변경 등은 논리적 변화가 없는 커밋이다.\n이러한 커밋은 코드 변경을 추적할 때 무시할 수 있다.\nbisect 중 이러한 커밋을 건너뛰려면 다음 명령을 사용할 수 있다:\n1 git bisect skip $(git rev-list --grep irrelevant \u0026lt;good place\u0026gt; HEAD) 히스토리 탐색 시 더 많은 정보 제공 이 방식은 “컨텍스트” 정보를 추가한다.\n다음과 같은 커밋 메시지들을 보자 (Angular의 실제 커밋 일부이다):\nFix small typo in docs widget (tutorial instructions) Fix test for scenario.Application - should remove old iframe docs - various doc fixes docs - stripping extra new lines Replaced double line break with single when text is fetched from Google Added support for properties in documentation 이 메시지들은 모두 변경된 위치를 나타내려 하지만, 일정한 규칙을 공유하지 않는다.\n다른 예를 보자:\nfix comment stripping fixing broken links Bit of refactoring Check whether links do exist and throw exception Fix sitemap include (to work on case sensitive linux) 이 메시지들로는 변경된 위치를 추측하기 어렵다.\n코드의 특정 부분(예: docs, docs-parser, compiler, scenario-runner 등)을 명시하는 것이 좋다.\n물론 어떤 파일이 변경되었는지는 diff로 확인할 수 있지만, 속도가 느리다.\ngit 히스토리를 볼 때 대부분의 사람들이 위치를 명시하려고 하지만, 공통된 규칙이 없을 뿐이다.\n커밋 메시지 형식 1 2 3 4 5 \u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;body\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;footer\u0026gt; 커밋 메시지의 모든 줄은 100자를 넘지 않아야 한다.\n이렇게 하면 GitHub 및 다양한 git 도구에서 읽기 쉬워진다.\n제목 줄 (Subject line) 제목 줄에는 변경 사항에 대한 간결한 설명이 포함된다.\n허용되는 \u0026lt;type\u0026gt; feat (feature): 새로운 기능 fix (bug fix): 버그 수정 docs (documentation): 문서 관련 변경 style (formatting, missing semi colons, …): 코드 포맷 등 비논리적 변경 refactor: 코드 리팩토링 test: 누락된 테스트 추가 chore: 유지보수, 빌드, 환경 관련 변경 허용되는 \u0026lt;scope\u0026gt; Scope는 변경된 코드의 위치를 나타내며, 다음과 같은 예시가 있다:\n$location, $browser, $compile, $rootScope, ngHref, ngClick, ngView, 등등.\n\u0026lt;subject\u0026gt; 텍스트 규칙 명령형, 현재 시제로 작성 (“change”, “fix” 등) 첫 글자는 대문자가 아니어야 함 끝에 마침표(.)를 붙이지 말 것 메시지 본문 (Message body) 제목과 마찬가지로 명령형, 현재 시제를 사용 (“change” not “changed”) 변경의 동기, 이전 동작과의 차이점을 포함한다 참고:\nhttp://365git.tumblr.com/post/3308646748/writing-git-commit-messages\nhttp://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html\n메시지 푸터 (Message footer) 호환성 깨짐 (Breaking changes) 모든 호환성 깨짐은 푸터에 반드시 명시해야 하며, 변경 설명, 이유, 마이그레이션 방법을 포함한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 BREAKING CHANGE: isolate scope bindings definition has changed and the inject option for the directive controller injection was removed. To migrate the code follow the example below: Before: scope: { myAttr: \u0026#39;attribute\u0026#39;, myBind: \u0026#39;bind\u0026#39;, myExpression: \u0026#39;expression\u0026#39;, myEval: \u0026#39;evaluate\u0026#39;, myAccessor: \u0026#39;accessor\u0026#39; } After: scope: { myAttr: \u0026#39;@\u0026#39;, myBind: \u0026#39;@\u0026#39;, myExpression: \u0026#39;\u0026amp;\u0026#39;, // myEval - usually not useful, but in cases where the expression is assignable, you can use \u0026#39;=\u0026#39; myAccessor: \u0026#39;=\u0026#39; // in directive\u0026#39;s template change myAccessor() to myAccessor } The removed `inject` wasn\u0026#39;t generaly useful for directives so there should be no code using it. 이슈 참조하기 (Referencing issues) 해결된 버그는 “Closes” 키워드로 푸터에 작성한다.\n1 Closes #234 여러 개의 이슈를 한 번에 닫는 경우:\n1 Closes #123, #245, #992 예시 (Examples) Tip 여기서 feat($compile) 같은 표현에서 $compile 같은 달러 기호는 예약어나 변수 같은 건가 했는데 Git 커밋 메시지 규칙상 ( ) 안에 들어가는 scope(스코프) 부분이고, 달러($) 표시는 AngularJS 내부 네이밍 규칙에서 비롯된 것 같다.\n1 2 3 4 5 6 7 8 feat($browser): onUrlChange event (popstate/hashchange/polling) Added new event to $browser: - forward popstate event if available - forward hashchange event if popstate not available - do polling when neither popstate nor hashchange available Breaks $browser.onHashChange, which was removed (use onUrlChange instead) 1 2 3 4 5 6 7 8 fix($compile): couple of unit tests for IE9 Older IEs serialize html uppercased, but IE9 does not... Would be better to expect case insensitive, unfortunately jasmine does not allow to user regexps for throw expectations. Closes #392 Breaks foo.bar api, foo.baz should be used instead 1 2 3 4 5 6 feat(directive): ng:disabled, ng:checked, ng:multiple, ng:readonly, ng:selected New directives for proper binding these attributes in older browsers (IE). Added coresponding description, live examples and e2e tests. Closes #351 1 style($location): add couple of missing semi colons 1 2 3 4 5 6 7 docs(guide): updated fixed docs from Google Docs Couple of typos fixed: - indentation - batchLogbatchLog -\u0026gt; batchLog - start periodic checking - missing brace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 feat($compile): simplify isolate scope bindings Changed the isolate scope binding options to: - @attr - attribute binding (including interpolation) - =model - by-directional model binding - \u0026amp;expr - expression execution binding This change simplifies the terminology as well as number of choices available to the developer. It also supports local name aliasing from the parent. BREAKING CHANGE: isolate scope bindings definition has changed and the inject option for the directive controller injection was removed. To migrate the code follow the example below: Before: scope: { myAttr: \u0026#39;attribute\u0026#39;, myBind: \u0026#39;bind\u0026#39;, myExpression: \u0026#39;expression\u0026#39;, myEval: \u0026#39;evaluate\u0026#39;, myAccessor: \u0026#39;accessor\u0026#39; } After: scope: { myAttr: \u0026#39;@\u0026#39;, myBind: \u0026#39;@\u0026#39;, myExpression: \u0026#39;\u0026amp;\u0026#39;, // myEval - usually not useful, but in cases where the expression is assignable, you can use \u0026#39;=\u0026#39; myAccessor: \u0026#39;=\u0026#39; // in directive\u0026#39;s template change myAccessor() to myAccessor } The removed `inject` wasn\u0026#39;t generaly useful for directives so there should be no code using it. 🎯결론 AngularJS 커밋 컨벤션은 단순한 메시지 형식이 아니라 자동화·가독성·협업 효율성·장기 유지보수성을 극대화하기 위한 개발 문화의 표준화 도구이다.\n⚙️EndNote 더 알아보기 이 AngularJS 규칙은 나중에 아래 규칙들의 직접적인 기반이 되었다고 한다.\n표준 설명 Conventional Commits AngularJS 규칙을 표준화한 버전, 현재 대부분의 오픈소스 프로젝트가 채택 Semantic Release 커밋 메시지를 분석해 자동 버전 태깅 및 릴리스 노트를 생성 Commitizen 커밋 시 대화형으로 Angular 스타일을 유도하는 CLI 툴 ","date":"2025-10-15T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-10-14-angularjs-git-commit-message-conventions/","title":"AngularJS 커밋 컨벤션"},{"content":"📌개요 개발 문서를 작성하다 보면 계층 구조를 표현하거나 흐름을 명확하게 정리하거나 문서 포맷을 시각적으로 구조화해야 할 때가 많다. 문서 작성 시 자주 사용하는 특수 문자 모음을 종류별로 간략히 정리한다.\n📌내용 분류 개요 분류 목적 활용 예 계층 구조 표현 디렉토리/구조 표시 트리 구조, 파일 구조 흐름/논리 표현 동작 흐름 연결 알고리즘, 프로세스 강조 및 주석 중요 포인트 전달 설계 문서 상태/태스크 표시 작업 상태 관리 TODO 문서 관리 코드/개발 표현 문법 표기 코드 스타일 수학/논리 조건/수식 표현 알고리즘 명세 1. 계층 구조(Tree) 표현 1 2 3 4 5 6 프로젝트 구조 예시 ├─ src │ ├─ main │ │ └─ java │ └─ test └─ README.md 기호 용도 ├─ 중간 노드 └─ 마지막 노드 │ 계층 연결 / 경로 구조 표현 \u0026gt; 종속 또는 흐름 표시 2. 흐름/관계 표현 기호 의미 예시 → 단순 흐름 로그인 → 인증 → 응답 ⇒ 결과 Cache Miss ⇒ DB 조회 -\u0026gt; 단방향 관계 A -\u0026gt; B \u0026lt;-\u0026gt; 양방향 Client \u0026lt;-\u0026gt; Server ==\u0026gt; 강한 흐름 요청 ==\u0026gt; 처리 ~~\u0026gt; 비동기 흐름 요청 ~~\u0026gt; 큐 3. 강조 및 설명 기호 의미 사용 예 ※ 중요 포인트 ※ 환경 변수 필수 ! 주의 ! 삭제 주의 ⚠ 경고 ⚠ 민감 정보 노출 위험 💡 힌트 💡 캐시 적용 가능 ★ 핵심 ★ 주요 설계 포인트 4. 상태/태스크 관리 기호 의미 [ ] TODO [x] 완료 5. 코드/문법 표현용 기호 기호 설명 { } 코드 블록/JSON [ ] 배열/옵션 ( ) 파라미터 \u0026lt; \u0026gt; 제네릭/HTML 태그 :: 네임스페이스/스코프 =\u0026gt; 람다/매핑 표현 6. 수학/논리 표현 기호 설명 = 같다 ≠ 같지 않다 ∧ AND ∨ OR ≥ 이상 ∴ 따라서 ∵ 왜냐하면 ","date":"2025-10-14T18:19:54+09:00","permalink":"https://blog.b9f1.com/p/2025-10-14-clean-up-frequently-used-special-characters/","title":"자주 사용하는 특수 문자 정리"},{"content":"📌개요 MSA에서 각 서비스는 독립적인 데이터베이스와 메시지 브로커를 사용하며 이를 통해 비동기 이벤트를 교환한다.\n하지만 이 구조는 하나의 트랜잭션 내에서 데이터 저장과 이벤트 발행이 분리되기 때문에 데이터 일관성과 메시지 신뢰성을 보장하기 어렵다.\n예를 들어 결제 서비스가 결제 정보를 DB에 저장한 후 kafka에 결제 완료 이벤트를 발행한다고 하자. 만약 DB 저장은 성공했지만 kafka 발행이 실패한다면? 결제는 되었는데 알림 서비스나 배송 서비스는 이를 모르게 된다.\n이런 문제를 해결하기 위한 안전한 전략이 바로 Outbox 패턴이다.\nInfo Outbox는 보낼 편지함이라는 뜻이며 Transactional Outbox Pattern으로 조회했을 때 다양한 정보가 있었다.\n📌내용 문제 상황 MSA 환경에서 다음과 같은 불일치가 자주 발생한다.\nDB에는 반영되었지만 이벤트 발행 실패 이벤트는 발행되었지만 데이터 저장 실패 중복 발행 및 순서 불일치 서비스 간 재시도 로직으로 인한 중복 처리 이는 DB 트랜잭션과 메시지 발행이 원자적(atomic) 으로 묶여있지 않기 때문에 발생한다.\n기존 접근의 한계 2PC(Two-Phase Commit) DB와 메시지 브로커가 XA 트랜잭션을 지원해야 하며, 락 경합과 네트워크 오버헤드로 인해 확장성 측면에서 부적합하다.\nXS 트랜잭션 표준화된 분산 트랜잭션 구현 XA(eXtended Architecture)는 2PC(2 phase commit)을 통한 분산 트랜잭션 처리를 위해 X-Open에서 명시한 표준이다.\nApplication-Level Retry 애플리케이션에서 발행 실패 시 재시도를 구현할 수 있지만 네트워크 장애나 장애 복구 시점에 따라 중복 이벤트 또는 순서 역전이 발생할 수 있다.\nOutbox 패턴으로 해결할 수 있는 것 Outbox 패턴은 비즈니스 데이터와 이벤트를 동일 트랜잭션 내에서 처리하고 이후 별도의 Message Relay 프로세스가 메시지 브로커(Kafka, RabbitMQ 등)에 발행하는 구조다.\n이 방식은 트랜잭션 일관성과 메시지 신뢰성을 모두 확보한다.\n처리 흐름 요약 비즈니스 트랜잭션 수행 ORDER 테이블에 INSERT / UPDATE / DELETE 수행 같은 트랜잭션 내에서 OUTBOX 테이블에 이벤트 메시지(페이로드) 저장 두 테이블은 하나의 트랜잭션 단위로 커밋됨, 원자성(Atomicity) 확보 메시지 릴레이(Message Relay) 트랜잭션이 커밋된 후, 별도 프로세스가 OUTBOX 테이블을 주기적으로 읽음 발행되지 않은 이벤트(status = NEW)를 찾아 메시지 브로커에 Publish 발행 성공 시 status = PUBLISHED로 갱신 실패 시 재시도 로직 또는 DLQ(Dead Letter Queue)로 이동 메시지 브로커 Kafka, RabbitMQ, AWS SNS/SQS 등으로 메시지 전달 구독 서비스들이 해당 이벤트를 비동기적으로 처리 flowchart LR A[Order Service] --\u003e|INSERT/UPDATE/DELETE| B[(ORDER table)] A --\u003e|INSERT event| C[(OUTBOX table)] subgraph Database B C end C --\u003e|Read Outbox| D[Message Relay] D --\u003e|Publish Event| E[Message Broker] classDef highlight fill:#f4f4f4,stroke:#666,stroke-width:1px; class B,C highlight 구성 요소 Sender (Order Service): 비즈니스 로직 수행 및 Outbox에 이벤트 기록 Database: 비즈니스 데이터(ORDER)와 이벤트 로그(OUTBOX)를 함께 저장 Message Outbox: 발행 대기 중인 이벤트를 저장하는 테이블 또는 컬렉션 Message Relay: Outbox 테이블에서 이벤트를 읽어 메시지 브로커로 발행 Message Broker: 이벤트를 다른 서비스에 전달 (Kafka, RabbitMQ 등) 주요 장점 항목 설명 트랜잭션 일관성 보장 DB 저장과 이벤트 기록이 하나의 트랜잭션으로 처리됨 재처리 가능 Outbox 테이블을 기준으로 실패한 이벤트를 재전송 가능 멱등성(idempotency) 이벤트 발행 시 event_id로 중복 처리 방지 확장성 Kafka Connect / Debezium 등을 통해 Change Data Capture(CDC) 기반으로 확장 가능 Outbox + CDC(Change Data Capture) 단순 폴링 기반은 부하가 크기 때문에 Debezium + Kafka Connect 조합을 사용하기도 한다. DB 트랜잭션 로그를 구독하여 outbox 테이블의 변경 사항만 캡처해 이벤트를 발행하므로 효율적이다.\n유사 패턴과 비교 패턴 설명 한계 Saga 패턴 여러 서비스 트랜잭션을 보상 트랜잭션으로 관리 복잡한 보상 로직, 순차적 지연 Event Sourcing 상태를 이벤트 스트림으로 저장 이벤트 재생 비용, 복잡한 쿼리 Outbox 패턴 이벤트를 별도 테이블에 기록 후 발행 Outbox 테이블 관리 필요 적용 시 고려사항 Outbox 테이블은 주기적으로 정리(cleanup) 필요 이벤트 발행 실패 시 재시도 정책 및 DLQ(Dead Letter Queue) 구성 발행 순서 보장을 위한 파티셔닝 키 설계 (aggregate_id 기반) Kafka 등 메시지 브로커의 전달 보증 설정 (at-least-once vs exactly-once) 발행 중복 대비를 위한 멱등 처리 전략 (unique event_id) Outbox 패턴 적용 관련 트러블슈팅 날씨 정보로 사용자의 OOTD를 추천하는 서비스(피드 공유, DM 등 SNS 기능 포함) 날씨 도메인을 담당하는 팀원의 PR을 리뷰하다가 특이한 상황을 만났다.\n배치 잡을 실행하며 특별한 날씨 변화가 감지되면 알림을 생성하는 케이스 해당 테스트는 다른 테스트의 DB와 충돌이 없도록 독립된 인메모리 DB로 실행. 로컬 테스트 코드에서 실행 시 성공 또는 무한 루프로 인한 실패. 성공 보장이 안 됨. 특이하게 CI 이력엔 성공 케이스만 있음. 로직 점검 outboxes가 비었을 때만 FINISHED, 기본은 CONTINUABLE 모든 이벤트를 발송하기 위함 수신 대상이 없어도 SENDING 상태로 저장하는 결함 발견 수신 대상이 없어서 outbox가 처리되지 않고 계속 남아있으며 CONTINUABLE 무한 루프 하지만 간헐적인 성공과 CI에서만 성공하는 건 설명되지 않음.\n성공 조건과 실패 조건 점검 왜 로컬에선 무한 루프로 실패하는 케이스가 있고 CI에선 정상 동작하지?\n성공의 조건 배치 실행 시점에 PENDING outbox가 비어있다. 실패의 조건 배치 시작 후, PENDING outbox 조회 쿼리와 수신 대상 조회 쿼리가 반복 수신 대상이 0명인 경우: SENDING으로 바꾸지 않음, 여전히 PENDING Step이 종료되지 않고 같은 쿼리를 계속 내며 무한 반복 질문이 잘못됐다..! 로컬과 CI로 구분해서 볼 게 아니라 로직 점검에서 확인했던 outbox의 생성 여부를 봤어야 했다. \u0026ldquo;outboxes가 비었을 때만 FINISHED\u0026rdquo; 였으니까..\n즉, 특별한 날씨 변화가 있는가? 였다.\n날씨 조회에 대한 것도 Mocking 하거나 수신 대상을 보장하는 등 다양한 방법이 있었을텐데 정답이 없다 보니 발생했던 문제 같다.\n특이하게 CI에 성공 이력만 있던 것도 마침 그때는 특별한 날씨 변화가 없던 것\u0026hellip;\n🎯결론 Outbox 패턴은 MSA 환경에서 데이터 일관성과 이벤트 신뢰성을 확보하는 현실적인 대안이 될 수 있다.\n⚙️EndNote 사전 지식 트랜잭션 ACID 속성 메시지 브로커의 전달 보증 (at-least-once, exactly-once) Saga, 2PC, Event Sourcing 패턴 더 알아보기 분산 트랜잭션과 XA 트랜잭션에 대해 [MSA 패턴] SAGA, Transactional Outbox 패턴 활용하기 Pattern: Transactional outbox 추가 정리 단일 인스턴스(Monolithic) 환경 하나의 DB, 하나의 트랜잭션에서 모든 로직이 처리된다. 2PC(2-Phase Commit) 같은 표준 트랜잭션 메커니즘으로 DB 저장과 이벤트 처리(예: 메시지 큐 발행)를 하나의 논리적 단위로 묶을 수 있다. 따라서 원자성(Atomicity) 과 일관성(Consistency) 이 보장된다. 확장 시 한계 (락 경쟁 \u0026amp; 처리 병목) 트래픽 증가 → DB Connection Pool 경쟁 → 락 경합(lock contention) 증가 대규모 트랜잭션으로 인해 응답 지연 발생 DB가 트랜잭션 코디네이터 역할까지 하므로 병목이 심화된다. 이 시점에서 단일 DB 트랜잭션 구조는 확장성의 한계에 부딪힌다. 분산 환경(= MSA 도입) 각 서비스가 독립된 DB를 가지게 되고 서비스 간에는 비동기 메시지 큐(Kafka, RabbitMQ, SNS/SQS) 로 이벤트를 전달한다. 이로 인해 처리 속도는 비약적으로 향상된다. (주요 비즈니스 로직만 빠르게 커밋, 나머지는 비동기로 후처리) 하지만 데이터 일관성 문제가 새롭게 등장한다. 새로운 문제: 원자성 붕괴 비즈니스 트랜잭션(DB 커밋)과 이벤트 발행(Message Send)이 서로 다른 시스템에서 발생 두 작업 중 하나라도 실패하면 데이터 불일치 발생 메시지 순서가 바뀌거나 중복 이벤트가 발생할 수도 있음 Outbox 패턴의 해결 방식 서비스 로직 트랜잭션 안에서 비즈니스 데이터 + 이벤트 로그(Outbox)를 함께 커밋 이로써 “이벤트 기록”까지는 원자성 보장 이후 별도 프로세스(Message Relay)가 Outbox 테이블을 읽어 메시지 브로커(Kafka 등)에 발행 발행이 성공하면 상태를 PUBLISHED로 변경 실패하면 재시도 또는 DLQ(Dead Letter Queue)로 처리 sequenceDiagram autonumber participant Service as Order Service participant DB as Database participant Relay as Message Relay participant Kafka as Kafka Broker rect rgb(240, 248, 255) Note over Service,DB: [서비스 트랜잭션 - 동기 처리] Service-\u003e\u003eDB: INSERT INTO orders (...) Service-\u003e\u003eDB: INSERT INTO outbox_event (...) DB--\u003e\u003eService: COMMIT (단일 트랜잭션으로 원자성 확보) end rect rgb(250, 250, 250) Note over Relay,Kafka: [비동기 메시지 릴레이 - Outbox Poller] Relay-\u003e\u003eDB: SELECT * FROM outbox_event WHERE status = 'NEW' Relay-\u003e\u003eKafka: Publish Event to Kafka Kafka--\u003e\u003eRelay: ACK Relay-\u003e\u003eDB: UPDATE outbox_event SET status = 'PUBLISHED' end 결과적으로 구분 기존 구조 Outbox 적용 후 원자성 DB ↔ MQ 분리되어 깨짐 동일 트랜잭션 내 Outbox 기록으로 보장 확장성 단일 DB 락 병목 분산 환경 + 비동기 처리로 개선 신뢰성 발행 실패 시 유실 가능 Outbox 재시도 / CDC로 회복 가능 운영 복잡도 단순 Outbox 테이블 관리 필요(운영비 증가) ","date":"2025-10-10T19:41:23+09:00","permalink":"https://blog.b9f1.com/p/2025-10-10-outbox-patterns-for-data-consistency-and-event-reliability-in-msa/","title":"MSA Outbox 패턴"},{"content":"📌개요 Spring Boot에서 애플리케이션 설정을 구성할 때 자주 쓰이는 @Configuration과 테스트 환경에서 활용되는 @TestConfiguration은 비슷해 보이지만 동작 방식과 적용 범위가 다르다.\n두 어노테이션의 자동 등록 여부, 프로필 영향, 스캔 시점을 기준으로 비교해보고 어떤 상황에서 적절히 선택해야 하는지 정리한다.\n📌내용 1. 자동 등록 여부 @Configuration @Component 계열 어노테이션처럼 Spring의 컴포넌트 스캔 대상에 포함된다. 별도의 import 없이도 ApplicationContext 초기화 시 자동으로 등록된다. @TestConfiguration 자동 스캔 제외 대상이다. 테스트 컨텍스트에선 기본적으로 로드되지 않으며, 다음과 같은 경우에만 적용된다: 테스트 클래스 내부 static class 로 선언했을 때 @Import 등을 통해 명시적으로 가져왔을 때 2. 프로필(Profile) 영향 두 어노테이션 모두 @Profile을 함께 사용하면 활성화된 profile에 따라 적용 여부가 결정된다. 하지만 @TestConfiguration은 기본적으로 스캔되지 않기 때문에, import되거나 테스트 내부에 선언된 경우에만 프로필 조건이 평가된다. 3. 적용 시점 / 스캔 @Configuration 애플리케이션 실행 시 Spring Context 초기화 과정에서 컴포넌트 스캔, Bean 정의 등록 순서로 처리된다. @TestConfiguration 테스트 실행 시 컨텍스트에 주입된다. @Import된 경우에는 일반 컴포넌트보다 먼저 적용되며 이후 테스트 컨텍스트에 필요한 빈들을 우선적으로 교체하거나 오버라이드할 수 있다. 4. 사용 목적 @Configuration 프로덕션 및 애플리케이션 전역 설정을 관리한다. 예: DataSource, Service Bean, MessageConverter 설정 등. @TestConfiguration 테스트 전용 설정을 분리하기 위한 용도로 사용한다. 예: MockBean, Stub, Testcontainers 초기화, 테스트용 Repository 대체 설정 등. 🎯결론 @Configuration 애플리케이션 전역 설정 컴포넌트 스캔 대상 프로덕션/테스트 어디든 포함될 수 있음 @TestConfiguration 테스트 전용 설정 자동 스캔 제외 (명시적 import 또는 static inner class로 사용) 운영 환경에서는 로딩되지 않음. \u0026ldquo;운영 코드와 테스트 설정의 경계\u0026quot;를 안전하게 보장 ⚙️EndNote 사전 지식 Spring IoC Container (Bean 관리 메커니즘) Spring Boot TestContext Framework Profile 기반 환경 분리 더 알아보기 Spring Framework @Configuration 공식 문서 Spring Boot @TestConfiguration 공식 문서 테스트 설정 패턴: @MockitoBean, @Import, @ActiveProfiles ","date":"2025-09-13T21:03:30+09:00","permalink":"https://blog.b9f1.com/p/2025-09-13-configuration-vs-testconfiguration/","title":"설정 어노테이션 비교"},{"content":"📌개요 네트워크를 이해할 때 가장 먼저 마주하는 개념이 바로 계층 모델(Layered Model) 이다.\n대표적으로 TCP/IP 4계층 모델과 OSI 7계층 모델이 있는데, 두 모델은 네트워크 통신을 구조적으로 설명하는 기준이 된다.\n두 모델을 각각 설명하고, 어떤 차이가 있으며 실무에서는 어떻게 이해하면 좋은지 다뤄본다.\n📌내용 OSI 7계층 모델 국제표준화기구(ISO)에서 제정한 OSI(Open Systems Interconnection) 모델은 네트워크 통신 과정을 7단계로 나눈다.\n물리 계층 (Physical): 전기 신호, 케이블, 하드웨어 전송 매체 데이터 링크 계층 (Data Link): MAC 주소, 프레임 전송, 오류 검출 (예: Ethernet, Switch) 네트워크 계층 (Network): IP 주소 기반 라우팅 (예: IP, Router) 전송 계층 (Transport): 종단 간 통신, 신뢰성 보장 (예: TCP, UDP) 세션 계층 (Session): 세션 관리, 연결 유지/종료 (예: NetBIOS) 표현 계층 (Presentation): 데이터 형식 변환, 암호화 (예: SSL/TLS, JPEG) 응용 계층 (Application): 사용자 서비스 제공 (예: HTTP, FTP, SMTP) flowchart LR %% 송신 측 subgraph Sender[송신 측 - Encapsulation] direction TB A1[응용 계층] --\u003e A2[전송 계층] --\u003e A3[네트워크 계층] --\u003e A4[데이터 링크 계층] --\u003e A5[물리 계층] A1_side[Data] --- A1 A2_side[TCP/UDP Header + Data] --- A2 A3_side[IP Header + TCP Segment] --- A3 A4_side[MAC Header + IP Packet + CRC] --- A4 A5_side[010101 비트 스트림] --- A5 end %% 수신 측 subgraph Receiver[수신 측 - Decapsulation] direction TB B5[물리 계층] --\u003e B4[데이터 링크 계층] --\u003e B3[네트워크 계층] --\u003e B2[전송 계층] --\u003e B1[응용 계층] B5_side[010101 비트 스트림] --- B5 B4_side[프레임 해제 → IP Packet] --- B4 B3_side[패킷 해제 → TCP Segment] --- B3 B2_side[세그먼트 해제 → Data] --- B2 B1_side[Application Data 복원] --- B1 end %% 송수신 연결 A5 --전송--\u003e B5 TCP/IP 4계층 모델 실제 인터넷 프로토콜에서 사용되는 구조는 TCP/IP 모델이다. OSI보다 단순화되어 있으며, 실무에서는 이 모델을 주로 따른다.\n네트워크 액세스 계층 (Network Access): 하드웨어 인터페이스, 데이터 링크 \u0026amp; 물리 계층 포함 인터넷 계층 (Internet): IP 주소 기반 라우팅 (IP, ICMP 등) 전송 계층 (Transport): TCP/UDP 기반 통신 제어 응용 계층 (Application): 애플리케이션 서비스 (HTTP, FTP, DNS, SMTP 등) 오늘날 인터넷이 동작하는 실제 표준 구조다.\n두 모델의 비교 구분 OSI 7계층 TCP/IP 4계층 특징 계층 수 7 4 TCP/IP는 실용적으로 단순화 정의 기관 ISO ARPANET/DoD 목적 차이: 이론 vs 실무 표현/세션 독립 계층 존재 응용 계층에 통합 OSI는 세밀, TCP/IP는 실용 전송 계층 TCP/UDP 모두 포함 동일 신뢰성(연결형 vs 비연결형) 제공 보급 이론/교육 중심 인터넷 표준 TCP/IP가 사실상 전세계 표준 graph TD subgraph OSI_7 OSI1[\"응용 계층(Application)\"] OSI2[\"표현 계층(Presentation)\"] OSI3[\"세션 계층(Session)\"] OSI4[\"전송 계층(Transport)\"] OSI5[\"네트워크 계층(Network)\"] OSI6[\"데이터 링크 계층(Data Link)\"] OSI7[\"물리 계층(Physical)\"] end subgraph TCPIP_4 TCP1[\"응용 계층(Application)\"] TCP2[\"전송 계층(Transport)\"] TCP3[\"인터넷 계층(Internet)\"] TCP4[\"네트워크 액세스 계층(Network Access)\"] end %% 매핑 관계 OSI1 --\u003e TCP1 OSI2 --\u003e TCP1 OSI3 --\u003e TCP1 OSI4 --\u003e TCP2 OSI5 --\u003e TCP3 OSI6 --\u003e TCP4 OSI7 --\u003e TCP4 Tip 네트워크 공부 초기에 \u0026ldquo;OSI 7계층\u0026quot;을 무조건 외워야 하는지 의문이었다. 하지만 실무에서는 “이론적 설명”보다 TCP/IP 모델이 더 쓰인다. 결국 “OSI는 네트워크 교과서의 언어, TCP/IP는 인터넷의 언어”라고 이해하면 쉽다. 즉, OSI는 이상적인 개념도, TCP/IP는 현실 세계 지도라고 비유할 수 있다.\n🎯결론 OSI 7계층은 개념적 학습과 문제 진단에 유용하고, TCP/IP 4계층은 인터넷의 실제 동작 표준이다.\n즉, 공부할 때는 OSI로 세밀하게 이해하고, 실무에서는 TCP/IP로 단순하게 적용하는 것이 가장 효율적이다.\n⚙️EndNote 사전 지식 네트워크 기본 용어 (패킷, 프레임, 포트, 프로토콜) 인터넷 프로토콜(IP), TCP/UDP 개념 OSI와 TCP/IP 모델의 역사적 배경 더 알아보기 Wikipedia - OSI model ","date":"2025-08-30T13:33:52+09:00","permalink":"https://blog.b9f1.com/p/2025-08-30-comparison-of-network-layer-models/","title":"네트워크 계층 모델 비교"},{"content":"📌개요 애플리케이션의 성능 최적화에서 캐시(Cache) 는 빠질 수 없는 핵심 요소다.\n캐시는 데이터를 반복적으로 계산하거나 DB에서 가져오지 않고, 빠른 접근이 가능한 저장소에 보관해 재사용함으로써 응답 속도를 개선한다.\n이번 글에서는 로컬 캐시(Local Cache) 와 분산 캐시(Distributed Cache) 의 개념적 차이를 살펴보고, 각각의 장단점, 그리고 실무에서 어떤 기준으로 선택해야 하는지 다룬다.\n📌내용 1. 로컬 캐시(Local Cache) 정의: 애플리케이션 프로세스 내부 메모리에 데이터를 저장하는 캐시. 예시: Spring Boot에서 @Cacheable과 함께 사용하는 ConcurrentHashMap, Guava Cache, Caffeine 등. 장점 DB나 네트워크를 거치지 않고 메모리 접근 속도로 응답 → 극한의 성능. 설치나 운영이 간단하며, 외부 시스템 의존도가 없음. 트래픽이 적거나 단일 서버 애플리케이션에 적합. 단점 확장성 부족: 서버가 여러 대라면 각 인스턴스마다 캐시를 따로 관리해야 함 → 데이터 불일치(Inconsistency). 서버 재시작 시 캐시 데이터 손실. 캐시 메모리 크기가 서버 메모리에 직접 의존. 2. 분산 캐시(Distributed Cache) 정의: 네트워크를 통해 여러 애플리케이션 인스턴스가 공유하는 외부 캐시 서버. 예시: Redis, Memcached, Hazelcast, AWS ElastiCache. 장점 여러 서버 간 캐시 일관성 보장 (모든 노드가 같은 캐시 데이터 참조). 서버 재시작에도 데이터 유지 가능(특히 Redis 같은 Persistent Cache). 대규모 트래픽 처리와 수평 확장에 적합. 단점 네트워크를 거치므로 로컬 캐시보다 접근 속도는 느림. 별도의 인프라 운영이 필요하며, 설정 및 비용 부담이 있음. 네트워크 장애 시 캐시 미스(cache miss) 폭발 가능성. 3. 실무에서의 선택 기준 기준 로컬 캐시 적합 분산 캐시 적합 트래픽 규모 소규모, 단일 서버 대규모, 다중 서버 일관성 요구 데이터 변동이 적거나 중요하지 않은 경우 강한 일관성이 필요한 경우 운영 복잡도 간단한 아키텍처 추구 별도 인프라 운영 가능 성능 마이크로초 단위 응답 밀리초 단위 응답 (네트워크 오버헤드) 장애 복원력 서버 재시작 시 캐시 유실 영속성 옵션을 통해 데이터 유지 가능 보통 초기 단계에서는 로컬 캐시(Caffeine 등)로 간단히 시작하고, 트래픽이 증가해 서버를 여러 대 띄우는 순간 분산 캐시(Redis) 로 전환하는 전략이 가장 합리적이다.\n실제로 Spring에서도 @Cacheable 같은 어노테이션은 캐시 추상화를 제공해 로컬 ↔ 분산 캐시 전환이 용이하도록 설계되어 있다.\n🎯결론 Tip 지금 운영하는 서비스는 “속도\u0026quot;가 중요한가, “규모와 일관성”**이 중요한가?\n이 질문에 대한 답이 로컬 vs 분산 캐시 선택의 시작점이다.\n프로젝트 초반에는 로컬 캐시로 충분하다. 그러나 트래픽 증가·수평 확장·데이터 일관성 요구가 생기는 순간 분산 캐시로 전환해야 한다.\n⚙️EndNote 사전 지식 캐시의 기본 동작 원리 (Cache Hit / Cache Miss) Spring Cache 추상화 (@Cacheable, @CachePut, @CacheEvict) Redis, Memcached 같은 인메모리 DB의 개념 더 알아보기 Spring Framework Docs - Caching Abstraction Redis 공식 문서 Caffeine Cache GitHub PostgreSQL의 캐시 활용 및 쿼리 최적화 AWS ElastiCache ","date":"2025-08-30T13:25:20+09:00","permalink":"https://blog.b9f1.com/p/2025-08-30-local-cache-vs-distributed-cache/","title":"로컬 캐시 vs 분산 캐시"},{"content":"📌개요 Spring Framework에서 캐싱은 데이터베이스 조회나 외부 API 호출처럼 비용이 큰 연산 결과를 저장해두고 재사용할 수 있게 도와준다.\n이를 간단히 적용할 수 있는 방법이 바로 Spring Cache 어노테이션(@Cacheable, @CachePut, @CacheEvict)이다.\n이 세 가지 어노테이션의 차이점과 각각을 언제 사용하는 게 좋은지 정리한다.\n📌내용 1. @Cacheable 동작 방식: 캐시에 값이 있으면 그대로 반환하고, 없으면 메서드를 실행 후 결과를 캐시에 저장한다. 적절한 상황: 조회 성능 최적화가 필요한 경우 (DB 조회, 외부 API 호출) 결과가 자주 변하지 않고 재사용 가치가 큰 경우 예시 코드: 1 2 @Cacheable(cacheNames = \u0026#34;books\u0026#34;, key = \u0026#34;#isbn\u0026#34;) public Book findBook(String isbn) { ... } 2. @CachePut 동작 방식: 항상 메서드를 실행하고, 실행 결과를 캐시에 저장한다. 적절한 상황: 업데이트가 발생했을 때 최신 결과를 캐시에 반영해야 할 때 캐시 미스를 줄이기보다 캐시 동기화가 중요한 경우 주의점: @Cacheable과 같은 메서드에 함께 사용하는 것은 비추천 (동작 충돌 가능) 예시 코드: 1 2 @CachePut(cacheNames = \u0026#34;books\u0026#34;, key = \u0026#34;#isbn\u0026#34;) public Book updateBook(String isbn, BookDescriptor descriptor) { ... } 3. @CacheEvict 동작 방식: 캐시에서 특정 엔트리나 전체 엔트리를 제거한다. 적절한 상황: 데이터 삭제/갱신 후, 오래된 캐시를 반드시 무효화해야 할 때 정기적으로 캐시를 초기화할 때 옵션: allEntries = true: 캐시 전체 삭제 beforeInvocation = true: 메서드 실행 전 캐시 삭제 (예외 발생 시에도 캐시 정리 보장) 예시 코드: 1 2 3 4 5 @CacheEvict(cacheNames = \u0026#34;books\u0026#34;, key = \u0026#34;#isbn\u0026#34;) public void deleteBook(String isbn) { ... } @CacheEvict(cacheNames = \u0026#34;books\u0026#34;, allEntries = true) public void clearCache() { ... } 🎯결론 조회는 @Cacheable 갱신은 @CachePut 삭제/무효화는 @CacheEvict 이 세 가지를 상황에 맞게 조합하면, DB 부하를 줄이면서도 데이터 정합성을 유지하는 효율적인 캐싱 전략을 설계할 수 있다.\n⚙️EndNote 사전 지식 Java, Spring Boot 애플리케이션 기본 구조 캐시(Cache)의 개념과 동작 원리 Redis, Caffeine 등 Spring Cache 지원 캐시 제공자 더 알아보기 Spring Framework Docs - Caching Abstraction Redis 공식 문서 Spring Boot Reference: spring-boot-starter-cache ","date":"2025-08-30T13:12:30+09:00","permalink":"https://blog.b9f1.com/p/2025-08-30-spring-cache-core-annotation/","title":"Spring Cache 핵심 어노테이션 정리"},{"content":"📌개요 트래픽이 두 배가 되는 순간, 가장 먼저 터지는 건 성능이 아니라 정합성이다.\n멀티스레드 환경의 대표적 복병 경쟁 상태(Race Condition) 는 재현도 어렵고 한 번 새면 데이터 신뢰도가 무너진다.\n운영 환경에서 빈번하게 마주치는 경쟁 상태의 원인, 재현 패턴, 해결 전략의 우선순위를 정리한다.\n📌내용 경쟁 상태란 무엇인가? 정의\n여러 스레드가 공유 상태(shared state) 를 동시에 읽고/쓰기 하며 실행 타이밍에 따라 결과가 달라지는 상황.\n핵심 원인 축: AVR - Atomicity(원자성), Visibility(가시성), Reordering(재배치).\nAtomicity: x = x + 1 같은 RMW(Read–Modify–Write) 연산이 중간에 끼어들기로 깨지면서 lost update 발생 Visibility: 한 스레드의 쓰기가 다른 스레드에 늦게 보여 stale value(오래된 값) 관측, 잘못된 분기 stale value: 다른 스레드가 최신 값을 썼음에도 불구하고, 캐시/레지스터 등 중간 계층에 남아있던 이전 값 이 때문에 중복 및 누락뿐 아니라 if (value==0) 같은 조건 분기 오류가 발생 Reordering: CPU out-of-order 실행이나 JIT 최적화로 happens-before 순서가 무너짐 위험 신호와 재현 패턴 증상: 카운터 불일치, 중복/누락, “가끔” 실패하는 테스트, 운영 환경에서만 나타나는 버그(Heisenbug) 패턴: if(없으면 저장) 후 put (TOCTOU) 캐시 초기화 동시 접근 통계 카운터 증가 잘못된 Lazy init Heisenbug 하이젠버그는 프로그래밍에서 테스트를 수행할 때 발생되는 버그의 형태 중의 하나로서 문제를 발견하고 수정하기 위한 디버깅을 수행하려고 하면 문제점이 사라지는 형태의 버그를 말한다.\nTOCTOU (Time Of Check to Time Of Use)\n검사 시점과 사용 시점 사이의 틈새에서 다른 스레드가 상태를 바꿔 예상치 못한 버그 및 보안 취약점을 유발하는 클래식 경쟁 조건 유형.\n최소 예제로 보는 버그 원자적이지 않은 연산\nvalue++ 는 단일 연산처럼 보이지만, JVM 바이트코드 레벨에서는 getfield → iconst_1 → iadd → putfield 로 분해된다. 중간에 다른 스레드가 끼어들어 lost update 발생. volatile은 가시성만 보장해도 원자성은 보장 못 한다.\n1 2 3 4 5 class BrokenCounter { private int value = 0; void inc() { value++; } // 경쟁 상태 int get() { return value; } } 바이트코드 관점(필드 증가) getfield value → iconst_1 → iadd → putfield value 단일 연산이 아니라 여러 명령어로 분해됨 → 원자성 깨짐 Interleaving 예 (두 스레드) 두 번 증가 의도 → 최종 값 1 (한 번만 반영) DB의 Lost Update anomaly와 동일한 문제 가시성까지 얽히면 더 위험 A 스레드가 1 저장해도, B 스레드는 캐시 coherence 지연으로 여전히 0(stale value) 관측 결과: 카운터 중복·누락 조건 분기 오류 (예: if (get()==0) init()이 중복 실행) 왜 volatile로 해결되지 않나? volatile은 가시성(Visibility)과 재배치(Reordering) 방지 일부를 보장한다. 하지만 value++ 같은 RMW 원자성은 보장 못 함 1 2 volatile int value; void inc() { value++; } // 여전히 lost update 가능 해결 전략: 락보다 설계가 먼저 상태 자체를 줄여라 불변 객체(Immutable), Copy-on-Write, 메시지 패싱/Actor 모델, 이벤트 루프 스레드 컨파인먼트 ThreadLocal, 키 파티셔닝(같은 키는 동일 워커로) 원자 연산 \u0026amp; 동시 컬렉션 (J.U.C) AtomicInteger.incrementAndGet() (CAS 기반) CAS(Compare-And-Swap)는 재시도 루프(spin) 구조로 동작한다. 즉, 경쟁이 심하면 충돌이 잦아지고 성능이 급격히 저하될 수 있다. 고경합 환경엔 LongAdder 내부적으로 셀(Cell) 배열에 분산 저장하여 스레드 충돌을 줄인다. 주기적 집계를 통해 최종 값을 계산 → CAS 충돌 병목이 적음. ConcurrentHashMap.computeIfAbsent/merge로 TOCTOU 제거 CAS와 성능\nCAS는 실패 시 루프를 돌며 재시도하는 spin 기반 알고리즘이다. 경쟁이 적을 땐 락보다 빠르지만, 경쟁이 많으면 계속 충돌 → 재시도로 인해 오히려 락보다 느려질 수 있다. 이 때문에 고경합 상황에서는 LongAdder 같은 분산 구조가 더 유리하다.\n동기화 (Synchronization) synchronized (간단, 확실) ReentrantLock (tryLock, 타임아웃 지원) ReadWriteLock, StampedLock (낙관적 읽기 성능 개선) StampedLock의 낙관적 읽기는 실제 읽은 값이 도중에 다른 쓰기에 의해 깨지지 않았는지 validate(stamp) 호출로 반드시 검증해야 안전하다. StampedLock의 validate()\nlong stamp = lock.tryOptimisticRead(); → 값 읽기 → if (!lock.validate(stamp)) { ...재시도... } 낙관적 읽기는 무조건 성공하는 게 아니라, 읽은 후에 검증(validate)을 반드시 거쳐야 한다. 검증이 실패하면 일반적인 읽기 락을 다시 걸어야 한다.\n가시성 보장 \u0026amp; 안전한 게시 volatile 플래그, final 필드 Safe Publication (동기화 통해 객체를 게시) 단일 실행 \u0026amp; 멱등(Idempotency) 멱등 설계, fencing token(순서 보장 토큰) 실전 시나리오별 추천 레시피 시나리오 증상 해법 동시 카운팅/지표 증발(Lost update) LongAdder → 주기적 집계 Lazy 초기화 중복 생성 computeIfAbsent, 초기화 전용 락 캐시 프리로드 중복 로드 키 파티셔닝 + 단일 워커 읽기 99% 락 경합 Copy-on-Write, StampedLock 낙관 읽기 고가 연산 임계구역 응답 지연 임계구역 축소, 분해락(키별 락) 키 충돌 업데이트 중복·경합 키별 락/Striped Lock, merge 성능 vs 정확성 트레이드오프 정확성 고정: synchronized/ConcurrentHashMap으로 정합성 우선 확보 핫스팟 튜닝: 임계구역 축소, 자료구조 교체 마지막에: 락-프리/낙관적 기법 적용 테스트·검증 전략 jcstress (OpenJDK 동시성 경계 테스트 프레임워크) 확률 테스트 (스레드 수·코어 수·JVM 옵션 다양화) 페일패스트 계측 (불가능 상태 assert) 장시간 soak test (운영 유사 환경) 이런 버그는 Heisenbug 특성이 강함 → 반드시 장기간·다양 환경에서 검증 필요 🎯결론 공유 상태를 최소화하라. 남는 공유 상태는 반드시 J.U.C(java.util.concurrent) 와 명시적 동기화로 보호하라. 먼저 정합성을 보장하고, 이후 성능을 최적화하는 순서가 바람직하다.\n⚙️EndNote 사전 지식 Java Memory Model(JMM), happens-before monitor/synchronized, CAS(compare-and-swap) J.U.C(java.util.concurrent): 원자 클래스, 동시 컬렉션 더 알아보기 Java Concurrency in Practice (Brian Goetz) Effective Java 동시성 아이템 OpenJDK jcstress Oracle Concurrency 튜토리얼 Martin Kleppmann: Idempotency / Exactly-once ","date":"2025-08-16T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-08-16-from-the-basics-of-race-condition/","title":"Race condition 뿌리부터 잡기"},{"content":"📌개요 멀티스레드와 비동기 환경에서 흔히 사용하는 MDC(Logback Mapped Diagnostic Context) 나 SecurityContext 는 내부적으로 ThreadLocal 기반으로 동작한다.\n문제는 요청을 처리하는 스레드가 바뀌는 순간(예: @Async, Executor, CompletableFuture, Reactor 등) 이 컨텍스트 정보가 사라지거나 누락되기 쉽다는 것이다.\n따라서 로그 추적성과 보안 인증 상태를 잃지 않고 스레드를 건너뛰어 전파하는 방법이 필요하다. 서블릿 기반(Spring MVC + @Async/Executor/CompletableFuture)과 리액티브 기반(Reactor/WebFlux)에서 각각 컨텍스트를 안전하게 전달하는 실전 패턴을 살펴본다.\n📌내용 왜 사라지나? ThreadLocal의 본질 MDC(Logback MDC), SecurityContextHolder는 기본적으로 ThreadLocal 기반이다. 작업이 다른 스레드(스레드풀, ForkJoin)에서 실행되면 값이 비어 있거나 이전 요청의 쓰레기 값이 남을 수 있다(스레드 재사용). 해결 핵심: (캡처) → (전파) → (정리) 의 생명주기를 명시한다. 서블릿(동기/비동기) 스택: 6가지 정석 패턴 A. Runnable/Callable 래핑(핵심 원리) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class MdcPropagatingRunnable implements Runnable { private final Runnable delegate; private final Map\u0026lt;String, String\u0026gt; captured; MdcPropagatingRunnable(Runnable delegate) { this.delegate = delegate; this.captured = MDC.getCopyOfContextMap(); } @Override public void run() { Map\u0026lt;String, String\u0026gt; old = MDC.getCopyOfContextMap(); try { if (captured != null) MDC.setContextMap(captured); delegate.run(); } finally { if (old != null) MDC.setContextMap(old); else MDC.clear(); } } } 캡처 → set → 실행 → 복원/clear. SecurityContext도 동일 원리로 적용 가능. B. TaskDecorator로 전역 적용(@Async/ThreadPoolTaskExecutor) 1 2 3 4 5 6 7 8 9 10 11 12 @Bean TaskDecorator mdcTaskDecorator() { return (Runnable r) -\u0026gt; new MdcPropagatingRunnable(r); } @Bean Executor taskExecutor(TaskDecorator decorator) { ThreadPoolTaskExecutor ex = new ThreadPoolTaskExecutor(); ex.setTaskDecorator(decorator); ex.initialize(); return ex; } 장점: @Async, CompletableFuture, 커스텀 Executor 모두 자동 전파. C. Spring Security 전파 전용 유틸 1 2 3 4 5 Executor delegate = Executors.newFixedThreadPool(8); Executor secExecutor = new DelegatingSecurityContextExecutor(delegate); secExecutor.execute(() -\u0026gt; { /* SecurityContext 보존됨 */ }); DelegatingSecurityContext* 계열(Executor, ExecutorService, Runnable, Callable)을 사용하면 인증 컨텍스트만 확실히 전달된다. MDC도 필요하면 B 패턴과 조합. D. SecurityContextHolder 전략 옵션 1 2 SecurityContextHolder.setStrategyName( SecurityContextHolder.MODE_INHERITABLETHREADLOCAL); 자식 스레드에 한해 상속. 한계: 스레드풀 재사용과는 맞지 않는다(“새로 만든 자식 스레드”만 상속). E. CompletableFuture에서의 주의 1 2 3 4 5 6 var ctx = MDC.getCopyOfContextMap(); CompletableFuture.supplyAsync(() -\u0026gt; { if (ctx != null) MDC.setContextMap(ctx); try { return doWork(); } finally { MDC.clear(); } }, taskExecutor); 기본 공급자는 ForkJoinPool → 문맥 유실. 항상 전파 가능한 Executor와 명시적 set/clear 사용. F. 작업 종료 시 정리(clean-up) 는 의무 실패, 취소 케이스까지 finally에서 clear/복원. 그렇지 않으면 컨텍스트 누수로 다른 요청 로그가 오염된다. 리액티브(Reactor/WebFlux) 스택: 컨텍스트는 데이터다 A. SecurityContext 리액티브 전용 API 사용 인증은 ReactiveSecurityContextHolder를 통해 Reactor Context에 저장/조회. 예) WebFilter에서 contextWrite로 주입, 다운스트림에서 자동 사용. B. MDC Reactor Context, MDC 브리징 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // 컨텍스트 키 정의 record TraceCtx(String traceId) { static final String KEY = \u0026#34;TRACE\u0026#34;; } // 컨텍스트 주입 Mono.deferContextual(ctxView -\u0026gt; { TraceCtx t = ctxView.get(TraceCtx.KEY); return Mono.fromRunnable(() -\u0026gt; log.info(\u0026#34;trace={}\u0026#34;, t.traceId())); }) .contextWrite(ctx -\u0026gt; ctx.put(TraceCtx.KEY, new TraceCtx(\u0026#34;abc-123\u0026#34;))); // 공통 브리징 연산자(예시) public static \u0026lt;T\u0026gt; Function\u0026lt;Publisher\u0026lt;T\u0026gt;, Publisher\u0026lt;T\u0026gt;\u0026gt; mdc() { return pub -\u0026gt; Flux.from(pub).doOnEach(signal -\u0026gt; { if (!signal.isOnNext() \u0026amp;\u0026amp; !signal.isOnError() \u0026amp;\u0026amp; !signal.isOnComplete()) return; var ctx = signal.getContextView(); String traceId = ctx.getOrDefault(TraceCtx.KEY, new TraceCtx(\u0026#34;-\u0026#34;)).traceId(); try (var ignored = MDC.putCloseable(\u0026#34;traceId\u0026#34;, traceId)) { // MDC는 이 시점 로깅에만 반영됨 if (signal.getType() == SignalType.ON_NEXT) { log.debug(\u0026#34;processing...\u0026#34;); } } }); } 핵심: Reactor 로그는 시그널 순간에만 MDC에 넣고 즉시 닫는다(MDC.putCloseable). 장점: 스레드 hops가 잦아도 오염, 누수 없음. 리액터 체인 가장 바깥에서 transform(mdc())로 한 번만 적용. 무엇을 언제 사용하는 게 좋을까 상황 권장 패턴 비고 Spring MVC + @Async/Executor TaskDecorator(B) + 필요 시 DelegatingSecurityContext(C) 전역 일관성, 적용 쉬움 단발 커스텀 스케줄러/쓰레드 Runnable/Callable 래핑(A) 최소 오버헤드 CompletableFuture 전파 가능한 Executor + 명시적 set/clear(E) ForkJoinPool 지양 자식 스레드만 생성 INHERITABLETHREADLOCAL(D) 풀 재사용 환경에 부적합 WebFlux/Reactor ReactiveSecurityContextHolder + MDC 브리징(B 리액티브판) ThreadLocal 직접 접근 금지 체크리스트 캡처 시점은 요청 입구(필터/인터셉터/웹필터)에서. 전파 책임은 스케줄링/실행 시점(Executor/연산자)에서. 정리는 항상 finally 또는 try-with-resources(MDC.putCloseable)로. 풀 크기, 큐 적재량을 조정해 컨텍스트 스와핑 비용을 최소화. 성능 민감 구간에선 필드형 추적 ID(메서드 파라미터로 전달)도 고려 — 오버헤드와 가독성의 트레이드오프. 캡처, 전파, 정리 시각화 sequenceDiagram participant IN as Request Inbound participant CAP as Capture participant EXEC as Executor participant RUN as Task IN-\u003e\u003eCAP: traceId, auth 캡처 CAP-\u003e\u003eEXEC: Runnable/Callable 래핑 EXEC-\u003e\u003eRUN: set(MDC, SecurityContext) RUN--\u003e\u003eEXEC: finally { clear()/restore() } 🎯결론 비동기의 본질은 스레드 이동이고, 해법의 본질은 캡처-전파-정리의 규율이다. 이 원칙만 지키면 MDC와 SecurityContext는 어느 실행 모델에서도 흔들리지 않는다.\n⚙️EndNote 사전 지식 ThreadLocal과 스레드풀 재사용 모델 Logback MDC 기본 사용법 (MDC.put, MDC.clear, MDC.putCloseable) Spring Security: SecurityContextHolder, Authentication Reactor의 Context와 신호(시그널) 기반 훅 더 알아보기 Spring Security: DelegatingSecurityContextExecutor, DelegatingSecurityContextCallable/Runnable Spring Framework: TaskDecorator, ThreadPoolTaskExecutor, @Async Reactor: Context, contextWrite, doOnEach, SignalType 로깅: 분산 트레이싱(TraceId/SpanId)와 MDC 연동, MDC.putCloseable 활용 패턴 ","date":"2025-08-16T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-08-16-addressing-contextual-information-such-as-mdc-and-securitycontext-in-asynchronous-environments/","title":"컨텍스트 안전 전파"},{"content":"📌개요 최근 백엔드 인증/인가 시스템을 설계하거나 OAuth 2.0 기반의 로그인 시스템을 구축할 때 가장 많이 등장하는 키워드 중 하나가 JWT(JSON Web Token)이다.\nJWT의 3단계 구조를 정확히 이해하고, 각 구성 요소가 왜 존재하는지, 어떤 역할을 하는지 예제를 통해 상세히 알아본다.\n📌내용 JWT는 기본적으로 세 부분으로 구성된 문자열이다.\n1 \u0026lt;Header\u0026gt;.\u0026lt;Payload\u0026gt;.\u0026lt;Signature\u0026gt; 각각의 의미를 뜯어보자.\n1. Header (헤더) 이 부분은 토큰을 어떻게 검증할 것인지에 대한 메타 정보를 제공한다. Base64Url로 인코딩되어 토큰의 첫 번째 파트를 구성한다.\n1 2 3 4 { \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } alg(algorithm): 토큰 서명을 생성하기 위한 알고리즘을 명시. 보통 HS256 또는 RS256. typ(type): 토큰의 타입을 나타냄. JWT를 사용하는 경우 \u0026quot;JWT\u0026quot;로 고정. 2. Payload (페이로드) 이 페이로드는 서명되어 있지만 암호화되어 있진 않다. 즉, 누구나 내용을 볼 수 있다.\n따라서 민감 정보(password, 주민번호 등)는 절대 포함시키면 안 된다.\n1 2 3 4 5 6 7 { \u0026#34;sub\u0026#34;: \u0026#34;user1234\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;iat\u0026#34;: 1691432621, \u0026#34;exp\u0026#34;: 1691436221, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34; } sub(subject): 주체 식별자 (ex. 사용자 ID). iat(issued at): 발급 시간 (Unix timestamp). exp(expiration): 만료 시간. role, email 등 커스텀 클레임: 인증 또는 인가에 필요한 사용자 속성 값. 3. Signature (서명) 서버는 이 서명을 사용하여 토큰이 위조되지 않았음을 검증할 수 있다.\n서명이 다르면 페이로드가 조작된 것이다. 유효하지 않은 토큰으로 처리된다.\n1 2 3 4 HMACSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret ) 앞서 인코딩한 Header와 Payload를 .으로 연결한 후, 비밀 키(secret)를 이용해 알고리즘(HS256 등)으로 서명한 값. JWT 예시 1 2 3 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9. eyJzdWIiOiIxMjM0NTYiLCJyb2xlIjoiYWRtaW4iLCJleHAiOjE3MDAwMDAwMDB9. TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 각 부분을 디코딩하면:\nHeader: 1 { \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } Payload: 1 { \u0026#34;sub\u0026#34;: \u0026#34;123456\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;exp\u0026#34;: 1700000000 } Signature: 서버에서 secret key로만 확인 가능. JWT의 주요 보안 고려사항 서명만 존재하고, 페이로드는 암호화되지 않는다. 민감 정보는 넣지 말 것. 만료 시간(exp)을 꼭 설정하자. 토큰 탈취 시 무한히 사용할 수 없도록 하기 위해. 서버는 반드시 Signature를 검증해야 한다. 서명 검증을 하지 않으면 누구나 Payload만 바꿔도 토큰이 유효해진다. 🎯결론 JWT는 “신뢰할 수 있는 정보를 클라이언트에 안전하게 전달하기 위한 구조화된 문자열”이다.\nHeader는 토큰의 형식과 알고리즘, Payload는 전달하고자 하는 정보, Signature는 위조 여부를 판별하는 핵심 키이다.\n⚙️EndNote 사전 지식 Base64 인코딩/디코딩 대칭/비대칭 키 개념 (HMAC vs RSA) HTTP 인증 방식 (Bearer Token) 더 알아보기 jwt.io RFC 7519: JSON Web Token (JWT) Spring Security에서 JWT 사용하기 공식 가이드 OAuth 2.0과 JWT의 관계 JWT vs Session 기반 인증 비교 포스팅 ","date":"2025-08-05T01:58:05+09:00","permalink":"https://blog.b9f1.com/p/2025-08-05-lets-find-out-the-structure-and-each-component-of-json-web-token-in-detail/","title":"JWT 구조를 구체적으로 알아보자"},{"content":"📌개요 Spring 기반 웹 애플리케이션을 개발하거나 운영할 때 반드시 고려해야 할 대표적인 4가지 보안 위협을 다뤄본다.\nCSRF, XSS, 세션 고정, JWT 탈취는 대부분의 웹 시스템에서 발생할 수 있는 위험이다. 각 공격의 특성과 Spring Security를 활용한 대응 전략을 정리하여 실제 서비스에 적용할 수 있는 인사이트를 알아보자.\n📌내용 1. CSRF (Cross-Site Request Forgery) 공격 개요: 사용자가 의도치 않게 공격자의 요청을 자신의 권한으로 서버에 보내게 하는 공격. 예시: 로그인한 사용자가 의심 없는 사이트를 방문했는데, 그 사이트에 포함된 악성 스크립트가 사용자의 계정으로 돈을 송금하는 요청을 서버에 보냄. Spring Security 대응 전략: @EnableWebSecurity 사용 시 기본적으로 CSRF 토큰이 활성화됨. HTML \u0026lt;form\u0026gt; 내에 CSRF 토큰을 자동 삽입하거나, JavaScript 요청에서는 X-CSRF-TOKEN 헤더로 전달. API 서버의 경우 csrf().disable() 후 토큰 기반 인증(JWT 등)으로 대체하는 경우가 많음. 1 2 3 http .csrf().disable() // API 서버라면 비활성화 가능 .authorizeHttpRequests(...) 2. XSS (Cross-Site Scripting) 공격 개요: 클라이언트 브라우저에서 악성 JavaScript가 실행되도록 유도하는 공격. 예시: 게시판에 \u0026lt;script\u0026gt;alert('XSS');\u0026lt;/script\u0026gt; 삽입 시, 이를 읽은 다른 사용자의 브라우저가 스크립트를 실행함. 대응 전략: 출력 시 이스케이프: JSP, Thymeleaf 등 템플릿 엔진은 기본적으로 HTML 이스케이프 적용. Spring Boot 내장 방어: Thymeleaf는 th:text=\u0026quot;${var}\u0026quot;로 출력 시 자동 escape. JSON 출력 시: XSS 필터 사용 필요 (예: XssEscapeServletFilter 추가). 입력 검증도 병행 권장 (단, 출력 시 이스케이프는 항상 해야 함). 1 2 3 4 5 6 7 @Bean public FilterRegistrationBean\u0026lt;XssEscapeServletFilter\u0026gt; xssFilter() { FilterRegistrationBean\u0026lt;XssEscapeServletFilter\u0026gt; registrationBean = new FilterRegistrationBean\u0026lt;\u0026gt;(); registrationBean.setFilter(new XssEscapeServletFilter()); registrationBean.setOrder(1); return registrationBean; } 3. 세션 고정 공격 (Session Fixation) 공격 개요: 공격자가 세션 ID를 미리 지정한 뒤 피해자가 해당 세션으로 로그인하도록 유도. 예시: 공격자가 JSESSIONID=known_id로 피해자 브라우저를 설정한 뒤 로그인 유도 → 세션 탈취. Spring Security 대응 전략: 로그인 성공 시 세션 ID를 새로 발급하도록 설정: sessionFixation().newSession() 기본적으로 Spring Security는 changeSessionId() 정책을 사용 (Java EE 7+ 환경) 1 2 3 http .sessionManagement() .sessionFixation().migrateSession(); // 또는 newSession() 4. JWT 탈취 공격 개요: JWT가 노출되면 누구나 동일한 권한으로 API 요청 가능. 예시: HTTPS 미사용 환경에서 JWT가 탈취되어 악의적인 요청 발생. 대응 전략: HTTPS 필수: 전송 시 암호화는 기본. 토큰 저장 위치: localStorage → XSS에 취약하나 CSRF 안전. HttpOnly Cookie → CSRF 취약하나 XSS에 안전. CSRF 방어 필수. JWT 구성 보완: exp(만료시간), iat, jti 등을 활용하여 재사용 방지. 서버 측에서 블랙리스트 관리 로직 구현 (Redis 활용 등). 1 2 3 4 5 6 7 8 9 10 11 // JWT 사용 시 Filter에서 인증 구현 예시 public class JwtAuthenticationFilter extends OncePerRequestFilter { @Override protected void doFilterInternal(...) { String token = resolveToken(request); if (token != null \u0026amp;\u0026amp; jwtProvider.validate(token)) { Authentication auth = jwtProvider.getAuthentication(token); SecurityContextHolder.getContext().setAuthentication(auth); } } } 🎯결론 웹 보안은 단순히 설정으로 끝나지 않는다.\n아키텍처 설계 단계부터 공격 벡터를 고려하고, Spring Security를 활용해 방어 레이어를 체계적으로 구성해야 한다.\n⚙️EndNote 사전 지식 HTTP의 Stateless 특성 쿠키/세션/토큰 인증 방식 Spring Security 설정 구조 웹 브라우저의 Same-Origin 정책 더 알아보기 Spring Security 공식 문서 OWASP Top 10 CSRF in Spring Security XSS Prevention Cheat Sheet (OWASP) ","date":"2025-08-05T01:52:32+09:00","permalink":"https://blog.b9f1.com/p/2025-08-05-spring-web-security-threats-and-response-strategies/","title":"Spring 웹 보안 위협과 대응 전략"},{"content":"📌개요 백엔드 인증 인프라를 설계하며 OAuth 2.0이 표준처럼 자리 잡았다는 사실을 체감하고 있다.\n특히 소셜 로그인이나 외부 리소스 접근 권한 위임 기능을 구현하면서 “Authorization Code Grant” 방식의 구조와 흐름을 완벽히 이해하는 것이 중요하다고 판단해 이 글을 작성하게 되었다.\n이번 포스팅에서는 OAuth 2.0의 주요 컴포넌트와 함께 Authorization Code Grant가 어떤 흐름으로 동작하는지 실전 중심으로 알아본다.\n📌내용 OAuth 2.0의 주요 컴포넌트 컴포넌트 설명 Resource Owner 리소스(정보)의 실제 소유자. 보통 사용자(User) Client 리소스 접근을 요청하는 애플리케이션 (ex. 우리가 개발하는 웹앱) Authorization Server 인증을 담당하며, Access Token을 발급하는 서버 Resource Server 보호된 리소스를 제공하는 API 서버 (Authorization Server와 분리될 수도 있음) Authorization Code Grant 흐름 이 방식은 보안성과 유연성을 모두 만족하는 방식으로, 웹 앱에서 가장 많이 사용된다.\nsequenceDiagram participant User participant Client (웹앱) participant AuthorizationServer participant ResourceServer User-\u003e\u003eClient: 로그인 요청 Client-\u003e\u003eAuthorizationServer: 인증 요청 (사용자 브라우저를 리디렉션) AuthorizationServer-\u003e\u003eUser: 로그인 + 권한 요청 User-\u003e\u003eAuthorizationServer: 로그인 \u0026 승인 AuthorizationServer-\u003e\u003eClient: Authorization Code 전달 (리디렉션 URI) Client-\u003e\u003eAuthorizationServer: Authorization Code + Client Secret로 Access Token 요청 AuthorizationServer-\u003e\u003eClient: Access Token + (선택) Refresh Token 전달 Client-\u003e\u003eResourceServer: Access Token으로 리소스 요청 ResourceServer-\u003e\u003eClient: 리소스 응답 흐름 요약 인증 요청: Client가 Authorization Server에 인증 요청 URL로 리디렉트. 사용자 승인: 사용자는 로그인 후 권한 승인. 코드 수신: Authorization Server는 Redirect URI에 Authorization Code를 포함하여 리디렉트. 토큰 요청: Client는 받은 코드를 Authorization Server에 전송하며, Access Token 요청. 토큰 수신: Authorization Server는 Access Token을 응답. API 요청: Client는 받은 Access Token을 사용해 Resource Server에 요청. 데이터 수신: 유효한 토큰이면 리소스를 응답. Refresh Token은 왜 필요할까? Access Token은 일반적으로 짧은 유효시간(예: 1시간) 을 가진다. 이때 Refresh Token이 있으면 재로그인 없이 새로운 Access Token을 발급 받을 수 있어 UX 개선에 큰 역할을 한다.\n🎯결론 OAuth 2.0의 핵심은 책임 분리와 위임이며, Authorization Code Grant는 가장 안전하고 실전적인 인증 흐름이다.\n⚙️EndNote 사전 지식 HTTP 프로토콜의 기본 흐름 RESTful API의 인증/인가 개념 HTTPS 통신 방식 더 알아보기 RFC 6749: The OAuth 2.0 Authorization Framework Spring Security OAuth2 공식 문서 JWT.io 키워드: PKCE, Client Credentials, Implicit Grant, OpenID Connect, Refresh Token Rotation ","date":"2025-08-04T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-08-04-understanding-the-oauth-2.0-core-structure/","title":"OAuth 2.0 핵심 구조 이해하기"},{"content":"📌개요 백엔드 개발자가 꼭 이해하고 있어야 할 인증의 두 축, 세션 기반 인증과 토큰 기반 인증의 구조적 차이점과 각각의 보안상 고려사항에 대해 다뤄보자.\n특히 REST API 설계, OAuth2 도입, JWT 사용 시 맞닥뜨리는 여러 결정 포인트에서 어떤 방식을 왜 선택해야 하는지, 실전 관점에서 짚어본다.\n📌내용 세션 기반 인증 (Session-based Authentication) 동작 방식 로그인 성공 시 서버는 사용자 정보를 저장한 세션 ID를 생성하고, 클라이언트에 쿠키로 전달한다. 이후 모든 요청에는 쿠키가 자동으로 첨부되어 세션 ID를 통해 인증 정보를 확인한다. 특징 서버가 세션 상태를 직접 저장 (메모리, Redis 등) 브라우저 친화적 (자동 쿠키 처리) 보안 고려사항 CSRF 공격에 취약: 쿠키가 자동 전송되기 때문 세션 탈취(Session Hijacking) 대비 필요 SameSite, Secure, HttpOnly 쿠키 옵션 사용 필수 적합한 경우 브라우저 기반의 전통적인 웹 서비스 내부망 또는 통제된 환경 토큰 기반 인증 (Token-based Authentication) 동작 방식 로그인 성공 시 서버는 JWT(JSON Web Token) 또는 커스텀 토큰을 발급하고, 클라이언트는 이를 로컬 저장소에 저장한다. 이후 요청 시 Authorization 헤더를 통해 직접 첨부해서 인증한다. 특징 서버 무상태(stateless) 인증 방식 (세션 저장 불필요) 클라이언트/서버 분리된 구조에 유리 JWT는 자체적으로 서명되어 위변조 검증 가능 보안 고려사항 XSS에 취약: 토큰을 로컬스토리지에 저장 시 노출 가능 토큰 탈취 → 장기 권한 노출 우려 만료시간, Refresh Token 전략, Token Rotation 도입 필요 HTTPS 필수 적합한 경우 모바일 앱, SPA(Single Page App) 분산 시스템, 마이크로서비스 주요 차이 정리 항목 세션 기반 인증 토큰 기반 인증 서버 상태 상태 유지 (Stateful) 상태 없음 (Stateless) 저장소 서버 메모리/DB/Redis 클라이언트 로컬 저장소 인증 전달 쿠키 (자동 전송) HTTP Header (직접 전송) 취약점 CSRF, 세션 탈취 XSS, 토큰 탈취 사용 사례 웹 사이트 모바일, API 서버 🎯결론 인증 방식은 서비스 구조와 위협 모델에 따라 선택하자. 만능은 없다.\n클라이언트가 브라우저 중심이고 보안 제어가 가능한 경우엔 세션 기반 인증이, REST API나 모바일 중심이라면 토큰 기반 인증이 적절하다.\n단, 어떤 방식을 쓰든 보안은 추가 설정과 방어 로직 없이는 무너질 수 있다.\n⚙️EndNote 사전 지식 HTTP 쿠키/헤더 JWT 구조 (Header.Payload.Signature) CSRF, XSS 개념 더 알아보기 OWASP 인증 관련 가이드 JWT 공식 문서 RFC 7519: JSON Web Token (JWT) OAuth 2.0 개념 정리 및 흐름 ","date":"2025-08-04T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-08-04-session-vs-token-authentication-difference/","title":"세션 vs 토큰 인증 차이"},{"content":"📌개요 초기 프로젝트를 구축하거나 서비스를 운영하는 과정에서 데이터베이스를 어디에 설치할지는 중요한 결정 중 하나다.\n특히 AWS 환경에서는 RDS를 사용할지, EC2에 직접 설치할지를 두고 많은 고민을 하게 된다.\n실제 운영 관점에서 RDS의 주요 이점과 EC2 직접 설치 방식과의 차이점을 정리하고 RDS가 적합하지 않은 상황까지 함께 다뤄보려 한다.\n📌내용 RDS를 사용할 때의 주요 이점 자동화된 관리: 백업, 보안 패치, 장애 복구까지 AWS가 관리 Multi-AZ 구성 가능: 자동 장애 조치(Failover) 지원으로 고가용성 확보 자동 백업 및 시점 복구(PITR): 데이터 복구를 쉽게 수행 가능 보안 강화: IAM 인증, KMS 암호화, 보안 그룹 등 보안 기능 내장 모니터링 및 알림: CloudWatch 연동으로 실시간 DB 상태 확인 가능 스케일링 유연성: 수직 확장과 읽기 전용 복제(Replica) 지원 EC2에 직접 설치와 비교 항목 EC2 직접 설치 RDS 설치 편의성 수동 자동 패치 및 유지보수 직접 관리 AWS 자동 확장성 어렵고 시간 소요 빠르고 간편 장애 조치 복잡한 구성 필요 자동 장애 복구 지원 보안 설정 수동 구성 IAM, KMS 등 통합 제공 백업/복구 직접 스크립트 작성 자동 백업 및 PITR 지원 직접 설치는 최대한의 유연성과 커스터마이징이 가능하다는 장점이 있지만, 보안과 가용성을 직접 관리해야 한다는 부담이 있다.\n특히 운영 경험이 적거나 인프라 리소스가 부족한 팀에게는 RDS가 훨씬 더 효율적이다.\nRDS가 적합하지 않은 케이스 복잡한 확장 플러그인이 필요한 경우 (예: PostgreSQL의 일부 확장 모듈 미지원) DB 튜닝을 위해 커널 파라미터까지 조정해야 하는 경우 최소비용으로 간헐적 사용을 원할 때 (EC2+Docker가 유리) 사내망 또는 외부 연결이 자유로운 구조를 구성하고자 할 때 🎯결론 복잡한 운영을 줄이고 안정성을 확보하고 싶다면, AWS RDS는 최고의 선택이 될 수 있다.\n하지만 반대로, 비용 최적화·유연성·특수 환경을 원한다면 EC2 직접 설치가 유리할 수 있다. 결국 핵심은 “어떤 서비스를 어떻게 운영할 것인가?” 에 달려 있다.\n⚙️EndNote 사전 지식 AWS 서비스 (EC2, RDS, VPC, IAM, KMS) 리눅스 서버와 DB 설치 경험 PostgreSQL 또는 MySQL과 같은 관계형 DB 운영 경험 더 알아보기 Amazon RDS 공식 문서 PostgreSQL 확장 기능 목록 AWS Well-Architected Framework: Operational Excellence Pillar ","date":"2025-08-03T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-08-03-differences-between-using-aws-rds-and-installing-db-directly-into-ec2/","title":"AWS RDS vs EC2 직접 설치"},{"content":"📌개요 CI/CD 자동화는 트리거(Trigger)를 어떻게 설정하느냐에 따라 큰 차이가 있다.\nGitHub Actions에서 사용 가능한 다양한 트리거 유형과 각 트리거가 적합한 실전 시나리오를 정리해보자. 워크플로우 실행 조건을 전략적으로 설정하면 효율적이고 예측 가능한 자동화를 구현할 수 있다.\n📌내용 GitHub Actions 트리거 유형과 용도 트리거 유형 설명 대표 시나리오 push 브랜치나 태그에 push될 때 작동 main 브랜치 병합 후 자동 배포 pull_request PR 생성/수정 시 작동 PR 단위로 빌드 및 테스트 workflow_dispatch 수동 실행 버튼 QA 테스트 후 직접 배포 schedule cron 표현식 기반 정기 실행 새벽마다 DB 백업, 주간 리포트 생성 repository_dispatch 외부 앱의 API 호출로 작동 CMS 수정 시 블로그 자동 재배포 workflow_call 다른 워크플로우에서 호출 공통 빌드/테스트 재사용 issue_comment 특정 댓글이 달릴 때 “/deploy” 댓글로 수동 배포 release GitHub Release 생성 시 태그 기반 배포 create / delete 브랜치/태그 생성 또는 삭제 시 브랜치 생성 시 테스트 환경 설정 시나리오 예시 1. 자동 테스트와 빌드 PR 생성 시마다 테스트 자동 실행\n1 2 3 on: pull_request: branches: [main, develop] 2. 배포 자동화 main 브랜치로 push 시 프로덕션 자동 배포\n1 2 3 on: push: branches: [main] 3. 수동 배포 버튼 QA 검증 완료 후 사람이 버튼을 눌러 배포\n1 2 on: workflow_dispatch: 4. 정기 실행 자동화 새벽마다 DB 백업 스크립트 실행\n1 2 3 on: schedule: - cron: \u0026#39;0 3 * * *\u0026#39; # 매일 03:00 5. 댓글 기반 배포 자동화 이슈나 PR에 /deploy 댓글 달리면 배포\n1 2 3 4 5 6 on: issue_comment: types: [created] jobs: deploy: if: github.event.comment.body == \u0026#39;/deploy\u0026#39; 🎯결론 CI/CD 자동화는 ‘언제’ 실행할지를 똑똑하게 고르는 것이 핵심이다.\nGitHub Actions의 트리거를 전략적으로 활용하면 반복 작업을 줄이고, 안정적인 자동화 파이프라인을 구성할 수 있다.\n⚙️EndNote 사전 지식 GitHub 저장소와 브랜치 개념 YAML 문법 CI/CD 개요 더 알아보기 GitHub Actions 트리거 공식 문서 GitHub Actions cron syntax Reusable workflows 공식 문서 ","date":"2025-08-03T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-08-03-github-actions-triggers-and-scenarios/","title":"GitHub Actions 트리거와 시나리오"},{"content":"📌개요 Windows에 AWS CLI를 설치했지만 WSL(Windows Subsystem for Linux)에서는 aws 명령어가 작동하지 않아 별도의 설치가 필요하다.\nWSL 환경에 AWS CLI를 설치하고 기본 설정과 인증 테스트를 진행해보자.\n📌내용 AWS CLI 설치 후 명령어 없음 처음에 Windows에만 AWS CLI를 설치했을 때 PowerShell, CMD, git bash에서는 잘 동작했지만 WSL에서 aws --version을 입력하자 command not found 오류가 발생했다.\n이는 WSL이 독립적인 Linux 환경이기 때문이다. 아래와 같은 순서로 문제를 해결했다.\nWSL에서 AWS CLI 다운로드 및 설치 curl, unzip 역시 별도의 설치가 필요하다. AWS CLI가 주목적이므로 문서 하단에 정리한다.\nAWS CLI를 다운로드 받아 설치한다.\n1 2 3 4 5 6 7 8 9 10 11 curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; # 실행 시 다운로드 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 63.1M 100 63.1M 0 0 56.5M 0 0:00:01 0:00:01 --:--:-- 56.6M # 다운로드 받은 파일 압축 해제 -\u0026gt; aws 폴더로 압축 해제된다. unzip awscliv2.zip # 압축 해제된 폴더에서 install 실행 sudo ./aws/install 설치 확인 1 2 3 aws --version # 출력 예시 aws-cli/2.16.1 Python/3.11.5 Linux/5.15.90.1-microsoft-standard-WSL2 exe/x86_64.ubuntu.20 prompt/off 자격 증명 및 기본 리전 설정 1 2 3 4 5 6 aws configure # 입력을 요구한다 AWS Access Key ID [None]: \u0026lt;AccessKey\u0026gt; AWS Secret Access Key [None]: \u0026lt;SecretKey\u0026gt; Default region name [None]: ap-northeast-2 Default output format [None]: json 설정 확인 1 2 3 4 5 6 7 8 aws configure list # 출력 예시 Name Value Type Location ---- ----- ---- -------- profile \u0026lt;not set\u0026gt; None None access_key ****************DW73 shared-credentials-file secret_key ****************KVdy shared-credentials-file region ap-northeast-2 config-file ~/.aws/config 인증 테스트 1 2 3 4 5 6 7 aws sts get-caller-identity # 출력 예시 { \u0026#34;UserId\u0026#34;: \u0026#34;AIDAEXAMPLEID\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/your-username\u0026#34; } 필요한 툴 설치 이미 WSL을 사용하고 있다면 초기에 설치를 했겠지만 만약 없다면 curl, unzip 역시 별도의 설치가 필요하다.\nsudo apt update 패키지 목록(index) 갱신 명령이다. 리눅스에서 패키지를 설치할 때는, 인터넷에서 즉시 최신 패키지를 내려받는 게 아니라\n먼저 로컬에 “이 저장소에는 어떤 패키지가, 어떤 버전으로, 어떤 의존성을 가지고 있는지”\n리스트를 저장해 두고 그걸 참조해서 설치한다.\n1 2 3 4 5 6 # 패키지 목록 갱신 sudo apt update # curl 설치 sudo apt install curl # unzip 설치 sudo apt install unzip 🎯결론 WSL에 AWS CLI를 설치하려면 Windows와 별개의 환경으로 인식하고 Linux용 CLI를 직접 설치해야 한다.\n⚙️EndNote 사전 지식 WSL (Windows Subsystem for Linux)의 개념 AWS IAM 사용자 Access Key와 Secret Key 발급 방법 기본 Linux 패키지 설치 (apt 명령) 더 알아보기 AWS CLI 공식 문서 AWS IAM 사용자 관리 WSL 공식 가이드 ","date":"2025-06-27T12:44:22+09:00","permalink":"https://blog.b9f1.com/p/2025-06-27-wsl-using-aws-cli/","title":"WSL에서 AWS-CLI 사용하기"},{"content":"📌개요 백엔드 개발을 진행하다 보면 WSL에서 psql을 사용해 Windows에 설치된 PostgreSQL에 접속하고 싶은 상황이 있다.\n그러나 단순한 클라이언트 설치만으로는 접속이 되지 않는다. 방화벽, 접속 허용 설정, 포트 개방 등 네트워크 보안 요소를 고려해야 하기 때문이다.\nWSL에서 PostgreSQL 접속 방법을 알아보자.\n📌내용 PostgreSQL 클라이언트 설치 Windows에 구성된 PostgreSQL에 접속하기 위해 클라이언트를 설치해야 한다.\n1 2 sudo apt update sudo apt install postgresql-client Tip apt-get은 더 오래된 전통적인 CLI 툴\n스크립트나 자동화에 주로 사용 apt는 apt-get과 apt-cache의 여러 기능을 합쳐 사용자 친화적으로 만든 통합 명령어\n2014년 즈음부터 Debian/Ubuntu에 포함되었고 Ubuntu 16.04부터 기본으로 권장 깔끔한 출력과 심플한 옵션 제공 일상적인 사용에 apt가 권장되며 스크립트 자동화, 하위 호환에 apt-get을 사용할 수 있다.\nWindows IP 확인 PostgreSQL 서버가 설치된 Windows 쪽 IP를 알아야 접속이 가능하다. 이때 ifconfig이 아니라 다음 명령어를 사용해야 정확하다.\n1 2 3 ip route | grep default # 예시 결과 default via 172.27.208.1 dev eth0 172.27.208.1이 WSL에서 Windows를 바라보는 IP (게이트웨이 IP) 이므로 이 주소를 접속 대상 IP로 사용한다.\nWindows PostgreSQL 서버 설정 변경 postgresql.conf Windows에 설치된 PostgreSQL 설정 디렉토리에서 아래 설정을 찾아 수정한다.\n보통은 기본 설치 경로에 있다. C:\\Program Files\\PostgreSQL\\{version}\\data\n1 2 3 listen_addresses = \u0026#39;*\u0026#39; # 또는 원하는 IP만 listen_addresses = \u0026#39;localhost,172.27.208.1\u0026#39; Tip * 와일드 카드로 전체를 여는 것보다 특정 IP에서 들어오는 것만 명시적으로 지정하는 것이 보안상 안전하다.\npg_hba.conf 해당 대역(WSL 내부 IP 포함)을 명시적으로 허용해야 접속 가능하다.\n1 host all all 172.27.208.1 md5 수정 후에는 반드시 PostgreSQL 서비스를 재시작해야 설정이 반영된다.\nWindows의 services.msc에서 PostgreSQL을 다시 시작하거나 명령어를 사용해 재시작한다.\nWindows 방화벽에서 포트 열기 WSL은 Windows의 입장에선 외부 네트워크로 간주된다. 기본적으로 TCP 5432 포트는 차단되어 있다.\n따라서 명시적으로 인바운드 규칙을 추가해야 한다. 기본적인 설정으로 가정하고 정리한다.\n설정 경로:\nWindows의 실행 \u0026gt; wf.msc (Windows Defender 방화벽 고급 보안) 또는 제어판에서 방화벽 \u0026gt; 고급 설정 인바운드 규칙에서 새 규칙 추가 유형: 포트 프로토콜: TCP 포트: 5432 작업: 연결 허용 프로필: 도메인/개인/공용 모두 선택 이름: Allow PostgreSQL 5432 for WSL 연결 테스트 WSL 터미널에서 명령 실행 확인\n1 2 3 nc -zv 172.27.208.1 5432 # 포트 정상적으로 붙게 되면 Connection to 172.27.208.1 5432 port [tcp/postgresql] succeeded! 그 다음 실제 접속\n1 2 # 호스트, 유저, DB psql -h 172.27.208.1 -U postgres -d postgres 🎯결론 WSL은 로컬처럼 보이지만 네트워크적으로는 별개의 장치로 취급된다.\n단순히 PostgreSQL을 설치하는 것만으로 접속이 불가능하다. listen_addresses, pg_hba.conf, 방화벽 인바운드 설정까지 모두 갖춰져야 제대로 동작한다.\n⚙️EndNote 사전 지식 WSL 기본 개념 PostgreSQL 서버와 클라이언트 구조 Windows 방화벽 설정 더 알아보기 WSL 공식 문서 PostgreSQL 공식 문서 listen_addresses 설명 pg_hba.conf 인증 가이드 ","date":"2025-06-27T10:11:44+09:00","permalink":"https://blog.b9f1.com/p/2025-06-27-wsl-using-psql/","title":"WSL에서 PSQL 사용하기"},{"content":"📌개요 Docker는 컨테이너 단위의 애플리케이션 패키징과 실행이 뛰어나지만 실제 운영 환경에서는 수십, 수백 개의 컨테이너를 배포하고 유지해야 하는 복잡한 상황이 발생한다.\n이 문제를 해결하기 위해 등장한 개념이 컨테이너 오케스트레이션(Container Orchestration)이다.\n오케스트레이션의 핵심 개념과 필요성, Docker 단독 환경의 한계, Kubernetes를 중심으로 오케스트레이터가 해결하는 세 가지 주요 문제를 알아보자.\n📌내용 컨테이너 오케스트레이션이란? 오케스트라(orchestra)에서 유래 오케스트라: 수십 개의 악기가 각자 연주하지만 지휘자가 이를 정해진 순서와 규칙에 따라 통제하여 하나의 음악으로 만들어낸다. 개발 시스템: 수많은 컨테이너, 서비스, 배포, 트래픽, 설정이 따로따로 존재하지만 오케스트레이터가 이를 자동으로 조율하여 하나의 애플리케이션처럼 작동하도록 한다. 컨테이너 오케스트레이션은 다음과 같은 작업을 자동화한다.\n컨테이너의 배포 및 종료 헬스 체크 및 실패 시 자동 복구 트래픽에 따라 컨테이너 자동 확장/축소 로드 밸런싱, 서비스 디스커버리 YAML을 활용한 선언적 구성(Declarative Configuration)을 통한 인프라 관리 단일 컨테이너 환경에서는 수작업이나 스크립트로 관리가 가능하지만 수십 개 이상의 컨테이너를 운영하는 클러스터 환경에서는 비효율과 오류를 피하기 어렵다.\nDocker 단독 사용 환경의 한계 Docker는 다음과 같은 측면에서 한계가 존재한다.\n수동 스케일링 docker run 명령어로 컨테이너 개수를 수동 조정 실시간 트래픽 변화에 대응 불가 제한된 복구 기능 --restart 자동 재시작은 가능하지만, 헬스 체크 기반 복구, 다중 노드 상태 관리는 불가능 서비스 수준 추상화 부족 Docker Compose로 일부 기능(네트워크, 볼륨, 환경변수)은 구성 가능 하지만 로드밸런싱, 트래픽 분산, 서비스 디스커버리 등은 부족하거나 수동 구성 필요 오케스트레이션이 해결하는 주요 문제 3가지 자동 확장 - Auto Scaling 문제: 트래픽 급증 시 컨테이너 수를 사람이 조정해야 함 해결: k8s는 Horizontal Pod Autoscaler를 통해 리소스(CPU, 메모리) 사용량 기준으로 Pod 수를 자동 조절 예시: CPU 사용률이 80% 이상일 때 3개에서 10개로 자동 확장 자가 복구 - Self-Healing 문제: 컨테이너가 비정상 종료될 경우 사람이 직접 조치해야 함 해결: k8s는 livenessProbe, readinessProbe를 통해 주기적으로 상태를 체크하고 실패한 컨테이너는 자동 재시작 또는 교체 livenessProbe: \u0026ldquo;얘가 아직 살아 있나?\u0026rdquo; 판단하는 검사 자동 재시작 컨테이너가 비정상 상태일 경우 자동으로 재시작해줌 예: 무한 루프에 빠졌거나 내부적으로는 죽었는데 프로세스는 살아 있는 경우 readinessProbe: \u0026ldquo;얘가 트래픽 받을 준비가 됐나?\u0026rdquo; 판단하는 검사 장애 확산 방지 준비되지 않은 컨테이너는 Service에 등록되지 않음 앱이 시작은 됐지만 DB 연결이 아직 안 됐다면 트래픽 받지 않도록 막아줌 선언적 인프라 - Declarative Infrastructure 문제: 수동 명령어는 현재 상태를 명확히 알기 어렵고 일관성 유지가 어려움 해결: k8s에서는 YAML 파일에 \u0026ldquo;이 시스템은 이래야 한다\u0026quot;고 선언하면 클러스터가 이를 자동으로 유지 1 2 3 4 5 6 7 8 9 10 # 선언 예시 apiVersion: apps/v1 kind: Deployment spec: replicas: 3 template: spec: containers: - name: my-app image: my-app:latest Docker Compose vs Kubernetes 비교 항목 Docker Compose Kubernetes 스케일링 수동 조정 (--scale) HPA 기반 자동 확장 복구 --restart로 기본 재시작 상태 기반 자가 복구 디스커버리 내부 DNS 미지원 (v2 기준) 서비스명 기반 자동 디스커버리 학습 곡선 낮음 높음 운영 복잡도 낮음 (로컬) 높지만 강력함 오케스트레이터 선택 기준 오케스트레이터 특징 적합한 상황 Kubernetes CNCF 주도, 생태계 광범위 MSA 기반 대규모 서비스 Docker Swarm Docker와 연동 용이, 간결함 중소 규모 단일 클러스터 AWS ECS / Fargate 서버리스, 비용 최적화 AWS 중심의 배포 전략 🎯결론 컨테이너 오케스트레이션은 현대적인 서비스 운영을 위한 기본이자 필수 인프라 기술이다.\n학습과 실험엔 Docker만으로 충분하지만 실전에서는 운영 자동화, 복구, 확장성, 일관된 인프라 구성이 가능한 오케스트레이터가 반드시 필요하다.\n특히 Kubernetes는 클라우드 네이티브 환경의 표준으로 자리 잡았다.\n⚙️EndNote 사전 지식 Docker의 기본 사용법 컨테이너 개념 (이미지, 레지스트리, 실행 등) YAML 파일의 구조 이해 더 알아보기 Kubernetes 공식 문서 Kubernetes Horizontal Pod Autoscaler CNCF Landscape ","date":"2025-06-25T22:17:57+09:00","permalink":"https://blog.b9f1.com/p/2025-06-25-infra-what-is-container-orchestration/","title":"컨테이너 오케스트레이션"},{"content":"📌개요 Docker와 컨테이너는 다르다.\n많은 개발자가 처음 접하는 컨테이너 기술은 Docker로 이어지지만 사실 Docker는 컨테이너 기술을 기반으로 만들어진 하나의 구현체일 뿐이다.\nDocker보다 훨씬 이전에 등장했던 컨테이너 기술의 뿌리를 짚고 Docker가 어떤 혁신을 만들어냈는지, 또 최근에는 어떤 대체 도구들이 등장했는지 알아보자.\n📌내용 컨테이너 기술의 본질 컨테이너란 운영체제 수준에서 프로세스를 격리(isolation)하여 마치 독립된 시스템처럼 동작하게 하는 기술이다.\n이 격리는 주로 아래 두 가지 리눅스 커널 기능을 조합해서 이루어진다.\n네임스페이스(namespace) 프로세스, 파일 시스템, 사용자 ID, 네트워크 등 다양한 시스템 리소스를 다른 컨테이너와 분리하는 기술 첫 등장: 2002년 mount namespace (Linux 2.4.19), 본격적으로 컨테이너화 가능해진 시점은 2008년 2.6.24 이후 cgroups(control groups) CPU, 메모리, 디스크 등 리소스 사용량을 제한하고 분리할 수 있도록 해주는 기능 구글이 2006년 내부적으로 개발, 2008년 리눅스 2.6.24에 정식 포함 Info 리눅스 커널 2.6.24부터 namespace와 cgroups가 안정적으로 통합되며 현대적인 컨테이너 개념이 실현 가능해졌다.\nDocker 이전에도 컨테이너는 있었다 컨테이너 기술이 단지 Docker로부터 시작되었다고 생각하지 말자. 대표적인 두 가지 사례만 봐도 충분하다.\nGoogle Borg(2006~) 구글은 자체 클러스터 관리 시스템인 Borg에서 process containers라는 이름으로 컨테이너 기술을 도입해 사용하고 있었다. 이 경험은 나중에 Kubernetes(k8s)로 이어지게 된다. LXC(Linux Containers) 2008년 등장한 LXC는 리눅스 네임스페이스와 cgroups를 조합해 독립된 사용자 공간을 제공하는 최초의 완전한 리눅스 컨테이너 런타임이었다. 즉, Docker 이전에도 컨테이너는 실제 운영 환경에서 사용되고 있었다.\nTip k8s(kubernetes), i18n(internationalization), a11y(accessibility) 같은 축약어는 단어의 첫 글자 + 생략된 글자 수 + 마지막 글자 형태로 만들어진 축약어다.\n긴 단어를 짧게 줄여서 쓰기 편하게 기술 문서나 코드에서 가독성과 공간 절약을 위해 축약하면서도 고유성을 유지 Docker는 무엇이 달랐나? Docker는 2013년에 등장하여 기존 컨테이너 기술을 개발자 친화적으로 쉽게 쓸 수 있도록 UX를 패키징한 도구로서 주목받았다.\nDocker는 컨테이너 기술 자체를 발명한 게 아니라 그것을 \u0026lsquo;쉽게 쓸 수 있도록\u0026rsquo; 만들어 낸 데 혁신이 있다.\nDocker의 핵심 혁신:\nDockerFile로 정의 가능한 이미지 기반 환경 구성 docker run 같은 직관적인 CLI Docker Hub를 통한 이미지 공유 및 배포 또 다른 컨테이너 도구들 컨테이너 기술은 OCI(Open Container Initiative)라는 표준에 의해 정의된다. 이 표준을 기반으로 다양한 런타임이 Docker 외에도 등장하고 있다.\nContainer Runtime: 컨테이너를 실행하는 역할 runc: Docker도 내부적으로 사용하는 기본 실행기 conatinerd: CNCF가 관리하는 Docker 독립형 런타임 CRI-O: Kubernetes 전용으로 설계된 경량 런타임임 Docker 대체 도구: Podman: rootless 컨테이너, systemd 통합 지원, Docker Daemon 없이 작동 Buildah: Dockerfile 없이 이미지 빌드 가능, Podman과 연동 Kubernetes: 직접 컨테이너를 실행하지 않지만 런타임 인터페이스(Container Runtime Interface - CRI)를 통해 위 런타임과 연동 Docker는 이제 표준이 아니다? 2020년 이후 Kubernetes는 Docker를 기본 런타임에서 제외했고 대신 containered, CRI-O와 같은 런타임을 사용한다.\nDocker가 문제가 아니라 Docker의 구조가 Kubernetes와 궁합이 맞지 않아서다.\nKubernetes는 CRI라는 표준 API로 런타임을 호출한다. 하지만 Docker는 이 CRI를 직접 구현하지 않고 중간 계층(containerd + shim)을 사용해 간접적으로 연동된다.\n오히려 containerd나 CRI-O처럼 CRI를 직접 구현한 런타임이 더 효율적이다.\n🎯결론 컨테이너는 기술이고 Docker는 그 기술을 손쉽게 만든 하나의 도구다.\nDocker는 훌륭한 UX 도구지만 그 자체가 컨테이너 기술의 전부는 아니다. 컨테이너 생태계는 이제 Docker를 넘어 다양화되고 있으며 오픈소스 커뮤니티와 표준화가 이를 이끌고 있다.\n⚙️EndNote 사전 지식 이미지 빌드와 레이어 개념 Kubernetes의 CRI(Container Runtime Interface) 더 알아보기 Linux namespaces - Wikipedia Cgroups - Wikipedia 리눅스 커널 2.6 - 네임스페이스 기반 컨테이너 기술 Google Borg (2006~) – Docker 이전부터 컨테이너 활용 LXC (Linux Containers) – Docker보다 선행한 사례 Open Container Initiative (OCI) Podman 공식 문서 containerd 공식 사이트 ","date":"2025-06-24T14:04:36+09:00","permalink":"https://blog.b9f1.com/p/2025-06-24-infra-same-but-different-concept-container-technology-and-docker/","title":"컨테이너 기술 vs Docker"},{"content":"📌개요 Spring Boot + PostgreSQL 앱을 AWS에 배포해 보려고 한다. 보안에 주의해야 할 부분들을 고려하면서 진행해 보자.\n📌내용 EC2 인스턴스 생성 Spring Boot 실행하기 위해 EC2를 사용한다.\n이름 및 태그 원하는 서버 이름으로 설정한다.\n애플리케이션 및 OS 이미지(Amazon Machine Image) 옵션이 다양해서 필요에 맞게 설정한다.\nAMI: Amazon Linux 또는 Ubuntu 이번엔 Ubuntu 22.04 LTS 64비트(x86) 인스턴스 유형: t2.micro FreeTier 사용 가능 키 페어 키 페어 생성:\n.pem 파일 다운로드 (SSH 접속용) 생성 시 다운로드 되는 파일 보관 네트워크 설정 Warning 테스트에만 사용하고 운영 시에는 특정 IP 또는 CloudFront, ALB 등으로 제한해야 한다. 특히 사용하지 않을 땐 아예 보안 그룹에서 제거하고 필요할 때 다시 구성해서 사용하는 것이 좋다.\n대부분 자동 설정이고 원격 접속을 위해 SSH 설정\nSSH 트래픽 허용:\n현재 필요에 맞게 내 ip만 허용 또는 위치 무관 0.0.0.0/0 추후 보안 그룹 탭에서 변경 가능 스토리지 구성 Info 프리 티어를 사용할 수 있는 고객은 최대 30GB의 EBS 범용(SSD)또는 마그네틱 스토리지를 사용할 수 있다.\n스토리지까지 설정했다면 인스턴스 생성을 클릭해서 인스턴스 생성\n보안 그룹 수정 AWS에서 제공하는 방화벽 인바운드, 아웃바운드 규칙이 존재한다.\n인바운드(Inbound): 외부에서 EC2, RDS 등의 내부로 접근할 때 사용되는 방화벽 규칙 아웃바운드(Outbound): EC2, RDS 등의 내부에서 외부로 접근할 때 사용되는 방화벽 규칙 EC2 \u0026gt; 네트워크 및 보안 \u0026gt; 보안 그룹으로 이동해서 보안 그룹 설정. 인스턴스 생성 시 설정했던 일부 보안 그룹을 확인할 수 있다.\n생성 시 구성된 것 외에 필요에 맞게 보안 그룹 생성을 눌러 새로운 보안 그룹을 생성하고 구성한다.\n인바운드 규칙 인바운드 규칙1: 유형: 사용자 지정 TCP 프로토콜: TCP 포트 범위: 8080 소스 유형: Anywhere-IPv4 소스: 0.0.0.0/0 설명 - 선택사항: Spring Boot 인바운드 규칙2: 유형: SSH 프로토콜: TCP 포트 범위: 22 소스 유형: Anywhere-IPv4 소스: 0.0.0.0/0 설명 - 선택사항: SSH 연결 인바운드 규칙3: 유형: HTTP 프로토콜: TCP 포트 범위: 80 소스 유형: Anywhere-IPv4 소스: 0.0.0.0/0 설명 - 선택사항: HTTP 요청 인바운드 규칙4: 유형: HTTPS 프로토콜: TCP 포트 범위: 443 소스 유형: Anywhere-IPv4 소스: 0.0.0.0/0 설명 - 선택사항: HTTPS 요청 인스턴스에 보안 그룹 적용 인스턴스 \u0026gt; 작업 \u0026gt; 보안 \u0026gt; 보안 그룹 변경 화면으로 이동해서\n생성했던 보안 그룹 추가 후 저장\n인스턴스 연결 예를 들어 아래와 같은 명령으로 SSH 클라이언트에서 접속 시도할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 ssh -i \u0026#34;sshKey.pem\u0026#34; ubuntu@ec1-2-34-56-789.ap-northeast-1.compute.amazonaws.com # 연결 시 서버 정보 출력 예시 Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-1029-aws x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/pro System information as of Mon Jun 23 04:47:28 UTC 2025 System load: 0.08 Processes: 101 Usage of /: 5.8% of 28.89GB Users logged in: 0 Memory usage: 20% IPv4 address for eth0: 172.31.39.182 Swap usage: 0% * Ubuntu Pro delivers the most comprehensive open source security and compliance features. https://ubuntu.com/aws/pro Expanded Security Maintenance for Applications is not enabled. 0 updates can be applied immediately. Enable ESM Apps to receive additional future security updates. See https://ubuntu.com/esm or run: sudo pro status The list of available updates is more than a week old. To check for new updates run: sudo apt update The programs included with the Ubuntu system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. To run a command as administrator (user \u0026#34;root\u0026#34;), use \u0026#34;sudo \u0026lt;command\u0026gt;\u0026#34;. See \u0026#34;man sudo_root\u0026#34; for details. 어떻게 배포하는 게 적절할까? 상황 추천 방법 빠른 실험, 개인 개발, MVP JAR 수동 업로드 팀 개발, 협업, 배포 자동화 예정 GitHub 클론 → 빌드 → 실행 CI/CD (CodeDeploy, GitHub Actions 등) 준비 GitHub 기반이 장기적으로 효율 RDS에 PostgreSQL 생성 RDS \u0026gt; 데이터베이스 생성 화면에서 진행한다.\nRDS 생성 시 주의 사항 엔진: PostgreSQL 인증 정보: 사용자명, 비밀번호 설정 퍼블릭 액세스: 비활성화 보안 그룹: EC2 인스턴스의 보안 그룹만 허용하도록 설정` 데이터베이스 생성 방식 선택 표준 생성 또는 손쉬운 생성 선택\n엔진 옵션 엔진 유형:\nAurora(MySQL Compatible) Aurora(PostgreSQL Compatible) MySQL PostgreSQL MariaDB Oracle Microsoft SQL Server IBM Db2 엔진 버전: 다중 AZ DB 클러스터를 지원하는 버전만 표시(토글) 엔진 버전: PostgreSQL 17.4-R1 RDS 확장 지원 활성화(체크) 템플릿 템플릿 선택:\n프로덕션 개발/테스트 프리 티어 가용성 및 내구성 Info 사용 사례에 필요한 가용성과 내구성을 제공하는 배포 옵션을 선택하세요. AWS는 선택한 배포 옵션에 따라 일정 수준의 가동 시간을 제공하기 위해 최선을 다하고 있습니다. Amazon RDS SLA(서비스 수준 계약)에 포함되지 않습니다. .에서 자세히 알아보세요.\n배포 옵션:\n다중 AZ DB 클러스터 배포(인스턴스 3개) 다중 AZ DB 인스턴스 배포(인스턴스 2개) 단일 AZ DB 인스턴스 배포(인스턴스 1개) 템플릿 프리 티어의 경우 1개만 선택 가능 설정 DB 인스턴스 식별자 DB 인스턴스 이름을 입력. 이름은 현재 AWS 리전에서 AWS 계정이 소유하는 모든 DB 인스턴스에 대해 고유해야 한다. 자격 증명 설정 마스터 사용자 이름: DB 인스턴스의 마스터 사용자에 로그인 ID를 입력 자격 증명 관리: AWS Secrets Manager를 사용하거나 마스터 사용자 자격 증명을 관리할 수 있다. AWS Secrets Manager에서 관리 - _가장 뛰어난 안정성_RDS는 자동으로 암호를 생성하고 AWS Secrets Manager를 사용하여 전체 수명 주기 동안 암호를 관리한다. 자체 관리사용자가 암호를 생성하거나 RDS에서 암호를 생성하고 사용자가 관리할 수 있다. 암호 자동 생성(체크): Amazon RDS에서 자동으로 암호를 생성하거나 사용자가 직접 암호를 지정할 수 있다. 마스터 암호, 마스터 암호 확인 인스턴스 구성 DB 인스턴스 구성 옵션은 위에서 선택한 엔진에서 지원하는 옵션으로 제한된다.\nDB 인스턴스 클래스(라디오):\n스탠다드 클래스(m 클래스 포함) 메모리 최적화 클래스(r 및 x 클래스 포함) 버스터블 클래스(t 클래스 포함) db.t4g.micro 2 vCPUs, 1 GiB RAM, 네트워크: 최대 2,085Mbps 스토리지 스토리지 유형:\n범용 SSD(gp2): 볼륨 크기에 따라 기준 성능 설정 범용 SSD(gp3): 스토리지와 독립적으로 성능 조정 프로비저닝된 IOPS SSD(io1): I/O 프로비저닝 유연성 프로비저닝된 IOPS SSD(io2): 지연 시간에 짧고 내구성이 뛰어나며 I/O 집약적인 스토리지 마그네틱: 최대 1,000 IOPS로 제한됨(권장되지 않음) 할당된 스토리지 얼만큼의 용량을 할당할 것인지 입력 할당된 스토리지 값은 20GiB~6,144GiB여야 한다. 이후 필요에 따라 추가 스토리지 구성 나머지 설정 EC2 컴퓨팅 리소스에 연결, EC2 인스턴스 선택 등 데이터베이스 인증 방식 선택 모니터링 설정 추가 구성 설정 필요에 맞게 구성을 설정한 후 데이터베이스 생성\nEC2 연결 후 설치 및 설정 DB 설정 먼저 EC2에 접속한 상태에서 PostgreSQL 클라이언트를 설치하고, postgres 사용자로 접속한다.\n1 2 3 4 5 6 7 8 # 1. PostgreSQL 클라이언트 설치 (Ubuntu 기준) sudo apt update sudo apt install postgresql-client -y # 2. RDS에 접속 psql -h your-rds-endpoint.rds.amazonaws.com -U postgres -d postgres # 비밀번호 입력하라고 나옴 # 비밀번호는 RDS 생성 시 설정한 `postgres` 사용자 비번 연결에 성공하면 앱 실행을 테스트를 위해 DB, Schema, 사용자를 생성한다.\nDDL 스크립트를 작성한 sql 파일을 EC2에 배치한 뒤 psql 명령으로 테이블을 생성한다.\nJava 설치 OpenLogic OpenJDK 17으로 프로젝트 테스트를 했지만 굳이 불필요하다면 openjdk-17-jdk로 충분하다.\n1 2 3 4 5 6 7 8 9 # 1. 패키지 업데이트 sudo apt update # 2. OpenJDK 17 설치 (기본은 OpenJDK, OpenLogic은 따로 설치) sudo apt install openjdk-17-jdk -y # 설치 후 터미널에서 설정 화면이 나타나는데 필요에 맞게 spacebar로 선택 후 enter # 3. 버전 확인 java -version 빠르게 수동 테스트 간단하고 빠른 확인을 위해 빌드 후 수동 업로드로 테스트해본다.\nJAR 빌드 Warning 빌드 전 RDS 인스턴스를 생성하고 DB 설정을 마무리한 후 그 정보로 application-prod.yaml을 구성한다.\n1 2 ./gradlew bootJar # 또는 ./mvnw package EC2로 전송 ssh 원격 접속 프로토콜을 기반으로 한 SecureCopy(scp)의 약자. 원격지에 있는 파일과 디렉터리를 보내거나 가져올 때 사용하는 파일 전송 프로토콜\n로컬에서 1 2 # 경로 `~`는 `/home/ubuntu`를 의미한다. scp -i \u0026#34;sshKey.pem\u0026#34; build/libs/app.jar ubuntu@ec1-2-34-56-789.ap-northeast-1.compute.amazonaws.com:~ 이후 EC2 내부에서 실행하는데 실행 시 프로파일 지정 1 2 3 4 cd ~ java -jar app.jar --spring.profiles.active=prod # 정상 실행 후 올라오는 주소 확인 # http://[퍼블릭IP]:8080 80 포트로 바로 실행하는 경우 1024 아래의 포트라서 ubuntu에서 권한 관련 오류를 만날 수 있다. Nginx를 사용해서 80 포트 접속 시 8080 포트의 사이트를 연결할 수 있도록 포트 포워딩 설정하는 방식이 안전할 수 있다. 필요에 따라 고민해보자.\n보안 그룹 설정 확인 리소스 포트 설명 EC2 8080 외부에서 접속 가능해야 함 (브라우저 확인용) RDS 5432 EC2 보안 그룹에서만 접근 허용 웹 접속 테스트 http://[EC2 퍼블릭 IP]:8080/ 또는 http://[EC2 퍼블릭 IP]:80/ 주소로 접속 테스트 Spring Boot 앱이 정상 작동하면 성공!\nDBeaver 클라이언트 테스트 (선택) RDS에서 퍼블릭 접속을 허용하지 않도록 설정했기 때문에 SSH 터널링을 통해 클라이언트 연결을 시도해야 한다.\n별도 설정이 필요했던 부분만 정리한다.\n새로운 연결 생성 \u0026gt; Main 탭 EC2 입장에서 로컬 접속할 수 있게 설정한다. SSH 탭 Use SSH 터널 체크 Settings 섹션 Host/IP: EC2 엔드포인트, Port: 22 User Name: EC2 생성 시 User Authentication Method: Public Key Private Key: 생성 시 발급 받은 *.pem 선택 Advanced settings 섹션 Remote host: RDS DB 엔드포인트, Remote Port: 5432 다음 단계 (선택) 필요한 경우 아래와 같은 설정으로 더 효율적인 운영 구조를 만들 수 있다.\nEC2에서 systemd로 백그라운드 실행 설정 PostgreSQL 보안 최적화 Route53 + HTTPS 연결 CloudWatch 로그 연결 🎯결론 보안성과 실용성을 모두 갖춘 Spring Boot + PostgreSQL의 AWS 수동 배포 구조를 성공적으로 구현했다.\n퍼블릭 IP를 열지 않고도 EC2 내부에서 RDS를 안전하게 연결하고, 외부에서는 HTTP/Nginx로만 접근 가능한 구조를 통해 실무에서도 적용 가능한 배포 경험을 확보했다.\n실습 기반의 배포 경험은 인프라 지식과 보안 개념까지 익힐 수 있는 좋은 기회였다. Spring Boot 애플리케이션을 EC2에 안전하게 배포하고, RDS 연결 및 DB 초기 설정까지 포함한 완전한 구성 흐름을 경험했다. 추후 systemd 등록, TLS 인증서 추가, CI/CD 적용까지 이어질 수 있는 확장 가능한 구성이다. ⚙️EndNote 사전 지식 AWS EC2, RDS의 기본 개념 SSH 키 기반 원격 접속 Spring Boot JAR 빌드 및 실행 방식 PostgreSQL 기초 쿼리 및 사용자 권한 설정 더 알아보기 Amazon EC2 공식 문서 Amazon RDS 공식 문서 Spring Boot 배포 가이드 Nginx 리버스 프록시 설정 DBeaver 공식 사이트 SSH 터널링 개념 ","date":"2025-06-23T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-06-23-aws-ec2-and-rds-classic-springboot-with-postgresql/","title":"AWS 배포 클래식"},{"content":"📌개요 AWS를 간단히 사용할 땐 Root 계정 하나로 로그인하고 필요한 것만 확인하고 참 편하긴 편하다. 하지만 몇 번의 프로젝트를 통해 Root 계정은 절대 일상적으로 써선 안 된다는 걸 온몸으로 배웠다.\nRoot 계정과 IAM 사용자 권한의 차이 그리고 왜 IAM 사용자로 운영하는 게 Best Practice인지에 대해 알아보자.\n📌내용 Root 계정은 일상적으로 사용하지 마라 AWS 문서에는 다음과 같이 적혀 있다.\nWarning “We strongly recommend that you don\u0026rsquo;t use the root user for your everyday tasks” docs.aws.amazon.com+8docs.aws.amazon.com+8docs.aws.amazon.com+8\nRoot 계정은 결제 수단 변경, 계정 설정 수정, Support 플랜 변경 등 IAM으로는 불가능한 민감 작업을 수행할 수 있기 때문에 AWS는 MFA 설정, 자동화 억제, 긴급 상황에서만 사용하도록 권장한다.\nIAM 사용자로 운영하는 이유 항목 설명 보안 강화 IAM 사용자는 권한을 세분화할 수 있어 최소 권한 원칙 구현 가능 역할 분리 인프라 운영, 결제 조회, 개발자 접근 등을 구분해서 관리 가능 추적과 감사 누가 어떤 리소스를 언제 썼는지 CloudTrail로 기록 가능 팀 확장 대응 실습 → 팀 운영 → 기업 운영까지 유연하게 확장 가능 비용/결제 보호 실수로 비싼 리소스를 생성하는 것 방지 가능 (예: GPU EC2, RDS Multi-AZ) IAM 사용자 권한 관리 방법 처음엔 복잡할 수 있지만 차근차근 순서대로 진행하면서 IAM 사용자를 분리해서 관리해보면 안정감이 느껴지고 권한에 대해 이해할 수 있게 된다.\n루트 계정으로 로그인 사용자 그룹 생성, IAM 사용자 생성 → 콘솔 접근 허용 + MFA 설정 역할에 따라 다음과 같이 그룹화하는 것을 권장. 조직에서 정하는 방식으로 진행하면 된다. BillingViewerGroup - 결제 정보만 조회 InfraAdminGroup - EC2, RDS 등 자원 생성/삭제 ReadOnlyGroup - 전체 리소스 조회만 가능 정책은 AWS에서 제공하는 관리형 정책부터 시작 필요 시 커스텀 정책 작성 Root 계정에만 있는 특수 작업 평소 작업엔 IAM으로 권한을 분리하여 최소 권한 원칙을 지키며 사용해야겠지만 특수한 경우는 어쩔 수 없이 Root 계정으로 로그인해야 할 것이다.\nRoot 계정 로그인 시 단일 인증이 아닌 멀티 인증 방식도 제공하는 것 같다. 즉 2명 이상의 인증을 통해 로그인을 허용한다.\nAWS에 따르면 Root 계정만 수행 가능한 작업은 다음과 같다:\nActivate IAM Access: IAM 사용자에게 Billing 콘솔 접근 활성화 루트 이메일 주소 및 결제 방식 변경 루트 계정 비밀번호 또는 액세스 키 재설정 AWS 계정 종료 이러한 작업은 Root 계정이 반드시 있어야만 할 수 있으므로 평시에는 IAM 기반 사용자/역할로 운영하고 Root는 긴급시에만 사용하는 구조가 안전하다.\n🎯결론 Root 계정은 금고 열쇠와 같다. 평소에는 꺼내지도 말아야 하며 AWS 운영은 IAM 사용자 기반으로 최소 권한 원칙을 엄수해야 한다.\nIAM 사용자 기반 운영은 번거로워 보여도 AWS에서 실수 없이 오래 살아남고 싶은 개발자라면 반드시 익숙해져야 할 안전장치다.\nAWS 문서들이 일관되게 강조하는 IAM 기반 안전 운영 방식을 실천해야 한다.\n⚙️EndNote ### 사전 지식 AWS Root 계정과 IAM 사용자 구조 이해 기본적인 AWS Console UI 조작 AWS 관리형 정책 및 인라인 정책 개념 최소 권한 원칙(Least Privilege Principle) 더 알아보기 IAM Best Practices (AWS 공식) AWS Billing 및 Cost Management 정책 예시 Well-Architected Framework AWS Free Tier 안내 ","date":"2025-06-22T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-06-22-aws-why-and-how-to-manage-permissions-as-iam-users-rather-than-root-accounts/","title":"IAM 사용자로 AWS 관리"},{"content":"📌개요 테스트 코드 리팩토링을 하면서 테스트 더블에 대해 좀 더 깊이 있게 정리할 필요를 느꼈다.\nMock이랑 Stub이랑 뭐가 다르지? 왜 Spy를 쓰지? 같은 고민을 한 개발자라면 공감할 수 있을 것이다.\n📌내용 테스트 더블이란? 테스트 더블은 테스트에서만 쓰이는 대체 객체를 말한다. 실제 객체가 아직 구현되지 않았거나 테스트에 방해가 되는 외부 의존성을 대체하기 위해 사용한다.\n용어 유래 영화에서 위험한 장면을 대신하는 스턴트 더블처럼 테스트에서도 그 역할을 대신 수행한다.\n테스트 더블 5종 시각화 요약 graph TD A[Test Double] --\u003e B(Dummy) A --\u003e C(Fake) A --\u003e D(Stub) A --\u003e E(Spy) A --\u003e F(Mock) B --\u003e|사용 안함| B1[단순 파라미터 채움] C --\u003e|간소화 구현| C1[예: 인메모리 DB] D --\u003e|출력 고정| D1[예: 외부 API 응답] E --\u003e|호출 감시| E1[예: 이벤트 기록 확인] F --\u003e|행위 명세| F1[예: 호출 순서, 인자 확인] 유형 주용도 특징 Dummy 자리는 필요하나 사용하지 않음 주로 파라미터 용 Fake 간단한 로직으로 실제 동작 대체 인메모리 DB 등 Stub 입력에 따라 고정된 출력 제공 결과 중심 테스트 Spy 실제 동작 + 호출 기록 부분 mocking Mock 상호작용 행위 검증 verify 기반 검증 각각의 실제 예제와 사용 기준 요약 정리 테스트 더블 실제 동작 유무 주 용도 주 검증 전략 Dummy ❌ 자리 채움용 없음 Fake ✅ (간소화) 동작 대체 상태 검증 Stub ❌ 응답 고정 상태 검증 Spy ✅ 호출 감시 행위 + 상태 Mock ❌ 상호작용 검증 행위 검증 Dummy - 아무 일도 하지 않는 자리 채움용 생성자나 파라미터에 꼭 필요하지만 실제 동작에 영향 주지 않는 의존성을 대신할 때\n1 2 3 4 5 6 7 8 9 interface Logger { void log(String message); } class DummyLogger implements Logger { public void log(String message) { // do nothing } } 1 2 3 4 5 6 7 @Test void sendEmail_doesNotRequireRealLogger() { Logger dummyLogger = new DummyLogger(); EmailService es = new EmailService(dummyLogger); es.sendEmail(\u0026#34;hello@test.com\u0026#34;); // 로그 기능은 테스트 대상 아님 } Fake - 간이 구현체로 테스트 성능 향상 실제 구현 대신 동작은 유지하되 속도/제약을 줄인 테스트용 구현이 필요할 때\n1 2 3 4 5 6 7 8 9 10 11 class FakeUserRepository implements UserRepository { private final Map\u0026lt;String, User\u0026gt; store = new HashMap\u0026lt;\u0026gt;(); public void save(User user) { store.put(user.getEmail(), user); } public boolean exists(String email) { return store.containsKey(email); } } 1 2 3 4 5 6 7 @Test void userIsSavedInFakeRepo() { FakeUserRepository repo = new FakeUserRepository(); repo.save(new User(\u0026#34;test@fake.com\u0026#34;)); assertTrue(repo.exists(\u0026#34;test@fake.com\u0026#34;)); } Stub - 미리 정해진 출력값 반환 특정 메서드가 고정된 값을 반환하도록 설정하고 싶을 때 외부 시스템 응답, 복잡한 연산 결과 등을 대체할 때 1 2 UserRepository stubRepo = mock(UserRepository.class); when(stubRepo.exists(\u0026#34;stub@test.com\u0026#34;)).thenReturn(true); 1 2 3 4 5 6 @Test void stubReturnsTrueWhenUserExists() { SignUpService sus = new SignUpService(..., stubRepo); assertTrue(sus.canRegister(\u0026#34;stub@test.com\u0026#34;)); // 상태 검증 } Spy - 실제 객체와 호출 감시 실제 메서드 실행은 유지하면서 호출 여부/횟수를 검증하고 싶을 때 일부 동작은 stub하고 일부는 실제 로직을 유지하고 싶을 때 1 2 3 4 5 List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; spyList = spy(list); spyList.add(\u0026#34;item\u0026#34;); verify(spyList).add(\u0026#34;item\u0026#34;); // 행위 검증 Tip 실제 메서드 호출을 방지하려면 doReturn(...).when(spy).method() 패턴 사용\nMock - 행위 검증에 최적화된 전용 객체 메서드가 어떤 인자로 몇 번 호출되었는지 명확히 검증해야 할 때 객체의 상태 변화보다 호출 자체가 테스트 목적일 떄 1 2 3 4 5 6 EmailService mockEmail = mock(EmailService.class); SignUpService sus = new SignUpService(mockEmail, ...); sus.signUp(new User(\u0026#34;mock@test.com\u0026#34;)); verify(mockEmail).sendWelcome(\u0026#34;mock@test.com\u0026#34;); 상태 검증 VS 행위 검증 비교 시나리오 회원 가입 시 이메일 발송을 예로 들면 다음과 같다.\n상태 검증 저장 결과 확인\n1 2 3 4 5 6 7 8 9 10 @Test void testUserIsSaved() { UserRepository fakeRepo = new InMemoryUserRepository(); EmailService dummyEmail = email -\u0026gt; {}; SignUpService sus = new SignUpService(dummyEmail, fakeRepo); sus.signUp(new User(\u0026#34;test@test.com\u0026#34;)); assertTrue(fakeRepo.contains(\u0026#34;test@test.com\u0026#34;)); } 행위 검증 특정 메서드 호출 여부 확인\n1 2 3 4 5 6 7 8 9 10 @Test void testWelcomeEmailSent() { EmailService mockEmail = mock(EmailService.class); UserRepository stubRepo = user -\u0026gt; {}; SignUpService sus = new SignUpService(mockEmail, stubRepo); sus.signUp(new User(\u0026#34;test@test.com\u0026#34;)); verify(mockEmail).sendWelcome(\u0026#34;test@test.com\u0026#34;); } 언제 어떤 테스트 더블을 써야 할까? 시나리오 선택 이유 1. Redis 캐시 hit 여부 확인 Spy 실제 Redis는 사용하고 hit 여부만 확인 2. 대량 주문을 처리하는 서비스 Fake In-memory OrderRepo로 테스트 3. 외부 결제 시스템 응답 시뮬레이션 Stub 실패 응답을 지정해서 테스트 4. 이메일 발송 여부 확인 Mock send() 메서드 호출 여부 검증 5. Validator에 전달되는 객체 확인 Spy 메서드 호출 인자 확인 가능 Mockito 주의사항 상황 권장 방식 이유 Spy의 실제 호출을 막고 싶을 때 doReturn().when(...) when(...).thenReturn(...)은 NPE 발생 가능 Stub 응답을 순차적으로 지정할 때 when().thenReturn(...).thenReturn(...) 순차적 응답 가능 Mock과 Spy 중 무엇을 선택해야 할지 헷갈릴 때 기본은 Mock, 일부만 다르게 하려면 Spy 전체 stub이 필요 없다면 Spy 고려 🎯결론 테스트는 코드를 검증하는 도구이자, 아키텍처를 되돌아보게 하는 거울이다.\nMock, Stub, Spy… 용어에 휘둘리기보다, 무엇을 검증하고 싶은지를 기준으로 테스트 더블을 선택해보자. 테스트는 결국 가독성, 유지보수성, 신뢰성을 확보하는 핵심 도구다.\n⚙️EndNote 사전 지식 Java 또는 Kotlin 기반 테스트 경험 JUnit 및 Mockito 기본 사용법 단위 테스트와 통합 테스트의 차이 더 알아보기 Mockito 공식 문서 Best Practices for Unit Testing in Java - baeldung Testing Strategies in a Microservice Architecture – Martin Fowler Effective Unit Testing - 테스트 설계 관점에서의 Mock, Stub Refactoring - 테스트와 리팩토링의 연결 ","date":"2025-06-21T23:29:50+09:00","permalink":"https://blog.b9f1.com/p/2025-06-21-tdd-testdouble-with-mockito-mock-and-spy/","title":"테스트 더블 - Test Doubles"},{"content":"📌개요 입력값 검증은 보안, 데이터 무결성, 사용자 경험을 모두 좌우하는 핵심 요소다. 하지만 각 계층의 책임이 명확하지 않으면 중복 검증, 누락, 책임 희석 등의 문제가 발생할 수 있다.\n각 계층에서 검증 책임을 정리하고 중복 없이 안정성을 확보하는 전략과 트레이드오프를 알아보자.\n📌내용 Presentation Layer Warning 클라이언트와 Controller에서의 검증은 빠른 피드백을 줄 수 있지만 절대 신뢰할 수 있는 검증 계층은 아니다. 반드시 하위 계층의 보완이 필요하다.\n계층: Web, Controller 책임: UI/UX 관점에서 빠른 피드백 제공 검증: 형식적 유효성(null, 공백, 정규식 등) 예: 이메일 형식, 숫자 범위, 필수 입력 여부 기술 예시: @Valid, @Validated, BindingResult, JavaScript 클라이언트 측 검증 목적: UX 개선 + 서버 리소스 낭비 방지 Service Layer 비즈니스 정책을 수반하는 검증은 Controller가 아닌 Service에서 처리해야 한다. 핵심 규칙을 책임지는 계층이다.\n계층: Application, Service 책임: 유스케이스 단위의 비즈니스 규칙 검증 검증: 상태 기반 조건 중복 등록, 권한 체크, 사용 제한 등 예: 하루 1회만 등록 가능, 좌석 수 초과 불가 등 기술 예시: 조건문, Guard Clause, Specification Pattern, 예외 발생 기반 제어 목적: 흐름 제어와 정책 보장, 응답 일관성 유지 Domain Layer Info 도메인은 단순한 데이터 보관소가 아니라 행위와 불변 조건을 포함한 책임 주체다. 도메인 객체는 자기 상태를 보호해야 하며 외부로부터 일관성을 강제 받지 않는다.\n계층: Domain Model (Entity, ValueObject, Aggregate) 책임: 객체의 내부 일관성과 상태 전이 검증 검증: 객체 생성 시 필수 조건 상태 변경 제약 예: 배송 상태는 \u0026lsquo;결제 완료\u0026rsquo; 이후에만 가능 기술 예시: 생성자/정적 팩토리 검증, 불변 조건 메서드, validateTransition() 등 목적: 외부 계층 의존 없는 무결성 유지, 테스트 가능성 향상 Persistence Layer Info 하위 계층일수록 안전 장치 역할이 강해진다. 모든 검증을 상위 계층에 의존하는 것은 위험하다. DB는 최후의 방어선으로 작동해야 한다.\n계층: Repositoiry, Database 책임: 저장 및 조회 시 스키마 기반의 데이터 무결성 보장 검증: DB 스키마 제약: NOT NULL, UNIQUE, CHECK, FK 등 ORM 수준 검증: @Column(nullable = false), @UniqueConstraint 등 기술 예시: JPA, Hibernate Validator, RDB 제약 조건 목적: 상위 계층 누락 방지, 악의적 요청 방어 Trade-off 구분 장점 단점 계층별 검증 분산 책임 분리, 시스템 안정성 증가 로직 분산 → 이해 난이도 상승 중복 검증 허용 Fail-safe 설계 가능, 안정성 강화 과도한 검증 → 응답 지연, 리소스 낭비 가능성 검증 통합 집중 유지보수 단순화 가능 변경 시 연쇄 영향 발생, 도메인 무결성 위험 🎯결론 입력값 검증은 단일 계층의 책임이 아니다. 각 계층에서 역할에 맞는 검증을 수행해야만 중복을 줄이고 안전성을 확보할 수 있다.\n계층 핵심 역할 Presentation 빠른 피드백 Service 정책 흐름 제어 Domain 불변 조건 보장 Persistence 최종 무결성 수호선 역할 ⚙️EndNote 사전 지식 MVC / 계층형 아키텍처 기본 개념 DDD의 엔티티, 밸류 객체, 애그리거트 이해 Bean Validation (@Valid, @NotBlank) 예외 처리 흐름 (@ControllerAdvice, 예외 매핑) REST 응답 코드 설계 (400, 409, 422 등) 🔍 더 알아보기 Spring Validation 공식 문서 Jakarta Bean Validation 3.0 Effective Java - Item 49: Check parameters for validity Refactoring 2nd Ed - Bad Smells in Code: Shotgun Surgery, Feature Envy Martin Fowler - Specification Pattern Validation in DDD: Where, Why, How ","date":"2025-06-21T21:08:37+09:00","permalink":"https://blog.b9f1.com/p/2025-06-21-tdd-how-far-should-i-go-to-verify-the-input-value/","title":"입력값 검증 어디까지 해야 할까?"},{"content":"📌개요 \u0026ldquo;AWS란 무엇인가\u0026rdquo; 묻고 또 묻는 것 같은데 이해하고 넘어가는 건 개발자, 기업, 비즈니스 의사 결정자 모두에게 중요한 출발점이다. AWS(Amazon Web Services)의 개념, 주요 서비스 그리고 그것이 가져다주는 실질적인 가치에 대해 알아 보자.\n📌내용 AWS란? AWS는 Amazon이 제공하는 클라우드 컴퓨팅 서비스 플랫폼으로 인터넷을 통해 온디맨드 방식으로 컴퓨팅 파워, 데이터베이스 스토리지, 컨텐츠 전송 등 IT 리소스를 제공하는 서비스다.\n개발자는 서버를 설치하고 운영하는 번거로움 없이 애플리케이션 개발과 서비스 운영에 집중할 수 있다.\n온디맨드(On-demand): 수요자가 요청하는 시점에 맞춰 즉시 서비스를 제공하는 방식 AWS의 핵심 구성 요소 AWS는 수많은 서비스를 제공하지만 대표적인 주요 구성 요소를 보면 다음과 같다.\n구성 요소 설명 EC2 가상 서버 인스턴스를 생성하고 관리할 수 있는 서비스 S3 파일 저장용 객체 스토리지 서비스 RDS 관리형 관계형 데이터베이스 서비스 VPC 사용자 전용 가상 네트워크를 구성하는 서비스 IAM 사용자 및 권한 관리를 위한 서비스 CloudWatch 모니터링 및 로깅 서비스 CloudFormation 인프라를 코드로 관리할 수 있게 해주는 템플릿 기반 리소스 생성 도구 AWS의 장점 확장성: 서비스 수요에 따라 리소스를 유연하게 확장 또는 축소할 수 있다. 비용 효율성: 사용한 만큼만 비용을 지불하는 pay-as-you-go 모델을 제공한다. 신뢰성 및 안정성: 전 세계에 걸쳐 있는 AWS 리전과 가용 영역을 통해 고가용성을 보장한다. 자동화 및 배포: CodeDeploy, Elastic Beanstalk, App Runner 등 다양한 자동 배포 서비스를 제공한다. CodeDeploy CI/CD를 직접 구축하고 배포 전략을 세밀하게 제어하고 싶을 때 사용 EC2, ECS에 직접 배포하고 싶을 때 사용 애플리케이션 코드를 EC2, Lambda, 온프레미스 서버에 자동으로 배포해주는 서비스 배포 전략(Blue/Green, In-place)을 지원하며 배포 중 오류가 나면 자동 롤백도 가능 CI/CD 파이프라인 구성에 자주 사용됨 CodePipeline과 함께 쓰면 강력 코드 배포 자체에만 집중한 서비스 서버 관리 인프라는 직접 Elastic Beanstalk 서버는 직접 쓰고 싶지만 너무 번거로운 건 피하고 싶을 때 사용 익숙한 환경에서 빠르게 배포해보고 싶은 스타트업이나 팀에서 고려해보면 좋음 애플리케이션만 업로드하면 나머지 인프라 생성부터 배포, 스케일링까지 AWS가 자동으로 해주는 PaaS (Platform as a Service) Java, Node.js, Python, PHP 등 다양한 플랫폼 지원 EC2, RDS, Load Balancer 등 구성 요소를 AWS가 자동으로 설정해줌 인프라는 자동화되지만 필요 시 EC2 설정 같은 부분도 커스터마이징 가능 App Runner 빠르게 실서비스를 만들고 싶은 스타트업, 혹은 테스트 서비스 등 인프라를 신경쓰고 싶지 않은 프론트엔드 또는 백엔드 소스 코드(GitHub) 또는 컨테이너 이미지(ECR)만 연결하면 자동으로 웹 서비스를 만들어주는 완전 관리형 서비스 서버? 인프라? 걱정할 필요가 없음 서버리스 개념: EC2, Load Balancer, Auto Scaling 등은 AWS가 전부 관리 CI/CD 자동화 가능(커밋하면 자동 배포) 기본적인 웹 서비스 운영에만 집중 가능 세 가지 요약 비교 항목 CodeDeploy Elastic Beanstalk App Runner 배포 대상 EC2, ECS, Lambda 등 EC2 기반 웹 앱 웹 애플리케이션 인프라 관리 직접 설정 자동 설정, 필요 시 조정 가능 전부 AWS가 관리 배포 전략 세밀한 전략 설정 가능 기본 배포 자동 CI/CD 지원 서버 접근 가능 가능 불가능 (서버리스) 난이도 높음 중간 가장 쉬움 추천 대상 DevOps 팀, 대규모 운영 빠른 배포 \u0026amp; 커스터마이징 빠르게 서비스 만들어보고 싶은 개발자 현실적인 과금과 보안 걱정 무료로도 충분하다 Tip 실습용이라면 t2.micro 또는 t3.micro, RDS free tier, S3 5GB 이하로만 사용해도 충분히 연습 가능\n무료 티어(Free Tier)를 활용하면 대부분의 AWS 주요 서비스를 12개월간 또는 영구적으로 무료 사용이 가능하다. 실수로 과금될까 봐 불안하다면 과금 알립(AWS Budgets)을 설정해서 월 예산을 초과하면 이메일/SMS로 알림 개발 중인 테스트 환경은 반드시 종료(terminate) 또는 자동 중지 스케줄 설정하는 습관이 중요 기본만 지켜도 안전하다 Info AWS는 기본적으로 보안 책임을 공유하는 구조 (Shared Responsibility Model) 물리적인 데이터센터 보안은 AWS가, 계정, 데이터, 접근 제어 등은 사용자가 책임진다. 하지만 AWS가 제공하는 도구와 정책을 잘 활용하면 훨씬 쉽게 보안을 유지할 수 있다.\nAWS는 세계 최고 수준의 보안 인프라를 갖추고 있다. 다만, 사용자가 기본 설정을 잘못하면 보안 이슈가 생기기 쉽다. 필수 보안 수칙 3가지 IAM Root 계정으로 로그인 금지 사용자 계정을 만들어 MFA(이중 인증) 설정 퍼블릭 액세스 차단 S3 버킷, EC2 보안 그룹 설정 확인 (기본적으로 열려 있음) CloudTrail, GuardDuty 활성화 보안 로그 기록과 이상 탐지를 자동화 🎯결론 Tip “AWS는 단순한 서버 호스팅이 아니라, 개발자의 속도를 높이고 비즈니스의 확장을 돕는 ‘기술 기반 비즈니스 플랫폼’이다.”\n이제 클라우드는 선택이 아니라 기본이다.\n여전히 각자의 방식이 있고, 로컬 서버나 온프레미스 환경이 필요한 곳도 존재하지만, AWS를 이해하는 것은 ‘현대적인 개발 생태계의 공통 언어’를 배우는 것과 같다.\n배포 속도, 안정성, 확장성, 자동화… 우리가 제품을 만들 때 부딪히는 거의 모든 인프라 문제를 AWS는 일정 수준 이상 해결해준다. 결국 중요한 건 사용자에게 더 나은 경험을 더 빠르게 전달하는 것이다.\n⚙️EndNote 사전 지식 기본적인 웹 서비스 구조에 대한 이해 (클라이언트–서버 구조) HTTP, 데이터베이스, 파일 저장소 등에 대한 기초 지식 IaaS, PaaS, SaaS 개념 더 알아보기 AWS 공식 홈페이지 AWS Well-Architected Framework 공식 문서 AWS 무료 티어 소개 AWS 과금 예측 및 관리 - Cost Explorer AWS 보안 책임 모델 보기 App Runner Developer Guide ","date":"2025-06-20T16:47:07+09:00","permalink":"https://blog.b9f1.com/p/2025-06-20-aws-what-is-amazon-web-services/","title":"AWS"},{"content":"📌개요 Spring Boot 프로젝트에서 Prometheus, Grafana를 연동해 시간의 흐름에 따라 변동되는 메트릭(Metrics) 데이터를 시각화해보자.\n📌내용 사전 준비 Spring Boot 프로젝트 spring-boot-actuator micrometer-registry-prometheus Docker 또는 Docker Desktop docker-compose.yml prometheus.yml build.gradle 1 2 3 4 5 6 dependencies { //... implementation \u0026#39;org.springframework.boot:spring-boot-starter-actuator\u0026#39; implementation \u0026#39;io.micrometer:micrometer-registry-prometheus\u0026#39; //... } application.yaml 1 2 3 4 5 6 7 8 9 management: endpoints: web: exposure: include: \u0026#34;prometheus\u0026#34; metrics: export: prometheus: enabled: true docker-compose.yml Docker 서비스 실행 자동화 설정 여러 Docker 컨테이너(Prometheus, Grafana 등)를 한 번에 실행/종료/관리하기 위한 설정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 services: prometheus: image: prom/prometheus:latest # Prometheus 공식 이미지 container_name: prometheus # 컨테이너 이름 지정, 내부 DNS로도 사용돤다. ex)http://prometheus:9090 volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml # 로컬 설정 파일을 컨테이너에 마운트 ports: - \u0026#34;9090:9090\u0026#34; # 호스트의 9090 → 컨테이너의 9090 (Prometheus 웹 UI) networks: - monitoring # 아래 정의한 사용자 정의 네트워크에 연결 grafana: image: grafana/grafana:latest # Grafana 공식 이미지 container_name: grafana # 컨테이너 이름 지정 ports: - \u0026#34;3000:3000\u0026#34; # 호스트의 3000 → 컨테이너의 3000 (Grafana 웹 UI) networks: - monitoring # 같은 네트워크에 있어야 prometheus:9090 접근 가능 # 사용자 정의 네트워크 정의 (서로 통신 가능하게 함) networks: monitoring: # 네트워크 이름만 지정하면 default 설정으로 생성됨 왜 networks.monitoring은 비어있는가?\n이건 사용자 정의 브리지 네트워크를 선언하는 부분이고 따로 옵션(예 driver)을 주지 않으면 Docoker가 기본값으로 설정된 브리지 네트워크를 생성한다. 명시적으로 정의도 가능하지만 대부분의 경우 생략해도 무방하다. 1 2 3 networks: monitoring: driver: bridge 실행 명령어 Docker Desktop이 설치되어 있어야 docker-compose 명령 사용 가능\n1 2 docker-compose up # 실행 docker-compose down # 종료 prometheus.yml Prometheus 수집 대상 설정 파일 Prometheus가 어떤 서버나 앱의 메트릭을 수집할지 설정하는 파일\nInfo host.docker.internal은 Docker 컨테이너가 로컬 호스트의 Spring Boot 앱에 접근하기 위한 주소다. (Mac/Windows 한정. Linux는 다르다)\n1 2 3 4 5 6 7 8 9 global: scrape_interval: 5s # 모든 타겟을 5초마다 스크랩(메트릭 수집)함 scrape_configs: - job_name: \u0026#39;spring-app\u0026#39; # 수집 작업 이름 (Prometheus UI에서 구분용) metrics_path: \u0026#39;/actuator/prometheus\u0026#39; # 메트릭이 노출되는 경로 (Spring Boot 기준) scrape_interval: 3s # 3초 마다 metrics_path의 경로에 접근하여 메트릭 데이터를 수집 static_configs: - targets: [\u0026#39;host.docker.internal:8080\u0026#39;] # 타겟 주소 (로컬 Spring Boot 앱) 실행 docker-compose.yml 파일이 있는 경로로 이동해서 docker-compose up 명령을 실행하면 Docker에 알맞게 Volume, Images, Container 생성되며 빌드된다.\n정상적으로 아래 출력을 확인했다면 localhost:9090에 접속하면 Prometheus UI를 확인할 수 있다.\n변경 발생 시 사용할 명령어 1 2 3 4 docker-compose down docker-compose up --build docker restart prometheus docker-compose up 명령 시 출력 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 docker-compose up [+] Running 22/22 ✔ grafana Pulled 12.2s ✔ f18232174bc9 Pull complete 1.6s ✔ 055b9255fa03 Pull complete 0.8s ✔ f61a19743345 Pull complete 2.0s ✔ b176d7edde70 Pull complete 0.4s ✔ e60d9caeb0b8 Pull complete 0.8s ✔ e032d0a5e409 Pull complete 0.8s ✔ c49e0ee60bfb Pull complete 7.3s ✔ 384497dbce3b Pull complete 9.4s ✔ c53a11b7c6fc Pull complete 0.8s ✔ 8af57d8c9f49 Pull complete 2.2s ✔ prometheus Pulled 8.1s ✔ 7df673c7455d Pull complete 0.3s ✔ 6ac0e4adf315 Pull complete 4.1s ✔ 408012a7b118 Pull complete 0.9s ✔ 1ccde423731d Pull complete 1.0s ✔ 1617e25568b2 Pull complete 1.7s ✔ 7221d93db8a9 Pull complete 2.0s ✔ 9fa9226be034 Pull complete 1.3s ✔ bf70c5107ab5 Pull complete 0.8s ✔ f3b09c502777 Pull complete 4.9s ✔ 44986281b8b9 Pull complete 0.9s [+] Running 3/3 ✔ Network resources_monitoring Created 0.1s ✔ Container prometheus Created 0.5s ✔ Container grafana Created 0.5s Attaching to grafana, prometheus prometheus | time=2025-06-17T12:40:19.321Z level=INFO source=main.go:674 msg=\u0026#34;No time or size retention was set so using the default time retention\u0026#34; duration=15d prometheus | time=2025-06-17T12:40:19.321Z level=INFO source=main.go:725 msg=\u0026#34;Starting Prometheus Server\u0026#34; mode=server version=\u0026#34;(version=3.4.1, branch=HEAD, revision=aea6503d9bbaad6c5faff3ecf6f1025213356c92)\u0026#34; prometheus | time=2025-06-17T12:40:19.321Z level=INFO source=main.go:730 msg=\u0026#34;operational information\u0026#34; build_context=\u0026#34;(go=go1.24.3, platform=linux/amd64, user=root@16f976c24db1, date=20250531-10:44:38, tags=netgo,builtinassets,stringlabels)\u0026#34; host_details=\u0026#34;(Linux 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 999bf7192f26 )\u0026#34; fd_limits=\u0026#34;(soft=1048576, hard=1048576)\u0026#34; vm_limits=\u0026#34;(soft=unlimited, hard=unlimited)\u0026#34; prometheus | time=2025-06-17T12:40:19.322Z level=INFO source=main.go:806 msg=\u0026#34;Leaving GOMAXPROCS=12: CPU quota undefined\u0026#34; component=automaxprocs prometheus | time=2025-06-17T12:40:19.324Z level=INFO source=web.go:656 msg=\u0026#34;Start listening for connections\u0026#34; component=web address=0.0.0.0:9090 prometheus | time=2025-06-17T12:40:19.324Z level=INFO source=main.go:1266 msg=\u0026#34;Starting TSDB ...\u0026#34; prometheus | time=2025-06-17T12:40:19.327Z level=INFO source=tls_config.go:347 msg=\u0026#34;Listening on\u0026#34; component=web address=[::]:9090 prometheus | time=2025-06-17T12:40:19.327Z level=INFO source=tls_config.go:350 msg=\u0026#34;TLS is disabled.\u0026#34; component=web http2=false address=[::]:9090 prometheus | time=2025-06-17T12:40:19.329Z level=INFO source=head.go:657 msg=\u0026#34;Replaying on-disk memory mappable chunks if any\u0026#34; component=tsdb prometheus | time=2025-06-17T12:40:19.329Z level=INFO source=head.go:744 msg=\u0026#34;On-disk memory mappable chunks replay completed\u0026#34; component=tsdb duration=972ns prometheus | time=2025-06-17T12:40:19.329Z level=INFO source=head.go:752 msg=\u0026#34;Replaying WAL, this may take a while\u0026#34; component=tsdb prometheus | time=2025-06-17T12:40:19.329Z level=INFO source=head.go:825 msg=\u0026#34;WAL segment loaded\u0026#34; component=tsdb segment=0 maxSegment=0 duration=156.935µs prometheus | time=2025-06-17T12:40:19.329Z level=INFO source=head.go:862 msg=\u0026#34;WAL replay completed\u0026#34; component=tsdb checkpoint_replay_duration=28.265µs wal_replay_duration=187.212µs wbl_replay_duration=133ns chunk_snapshot_load_duration=0s mmap_chunk_replay_duration=972ns total_replay_duration=249.607µs prometheus | time=2025-06-17T12:40:19.330Z level=INFO source=main.go:1287 msg=\u0026#34;filesystem information\u0026#34; fs_type=EXT4_SUPER_MAGIC prometheus | time=2025-06-17T12:40:19.330Z level=INFO source=main.go:1290 msg=\u0026#34;TSDB started\u0026#34; prometheus | time=2025-06-17T12:40:19.330Z level=INFO source=main.go:1475 msg=\u0026#34;Loading configuration file\u0026#34; filename=/etc/prometheus/prometheus.yml prometheus | time=2025-06-17T12:40:19.331Z level=INFO source=main.go:1514 msg=\u0026#34;updated GOGC\u0026#34; old=100 new=75 prometheus | time=2025-06-17T12:40:19.331Z level=INFO source=main.go:1524 msg=\u0026#34;Completed loading of configuration file\u0026#34; db_storage=820ns remote_storage=800ns web_handler=248ns query_engine=572ns scrape=325.871µs scrape_sd=23.08µs notify=68.448µs notify_sd=7.122µs rules=1.01µs tracing=2.927µs filename=/etc/prometheus/prometheus.yml totalDuration=641.79µs prometheus | time=2025-06-17T12:40:19.331Z level=INFO source=main.go:1251 msg=\u0026#34;Server is ready to receive web requests.\u0026#34; prometheus | time=2025-06-17T12:40:19.331Z level=INFO source=manager.go:175 msg=\u0026#34;Starting rule manager...\u0026#34; component=\u0026#34;rule manager\u0026#34; grafana | logger=settings t=2025-06-17T12:40:19.580946399Z level=info msg=\u0026#34;Starting Grafana\u0026#34; version=12.0.1+security-01 commit=ff20b06681749873999bb0a8e365f24fddaee33f branch=HEAD compiled=2025-06-17T12:40:19Z grafana | logger=settings t=2025-06-17T12:40:19.581187455Z level=info msg=\u0026#34;Config loaded from\u0026#34; file=/usr/share/grafana/conf/defaults.ini grafana | logger=settings t=2025-06-17T12:40:19.581202298Z level=info msg=\u0026#34;Config loaded from\u0026#34; file=/etc/grafana/grafana.ini grafana | logger=settings t=2025-06-17T12:40:19.581205854Z level=info msg=\u0026#34;Config overridden from command line\u0026#34; arg=\u0026#34;default.paths.data=/var/lib/grafana\u0026#34; grafana | logger=settings t=2025-06-17T12:40:19.581208666Z level=info msg=\u0026#34;Config overridden from command line\u0026#34; arg=\u0026#34;default.paths.logs=/var/log/grafana\u0026#34; grafana | logger=settings t=2025-06-17T12:40:19.581212098Z level=info msg=\u0026#34;Config overridden from command line\u0026#34; arg=\u0026#34;default.paths.plugins=/var/lib/grafana/plugins\u0026#34; grafana | logger=settings t=2025-06-17T12:40:19.581215368Z level=info msg=\u0026#34;Config overridden from command line\u0026#34; arg=\u0026#34;default.paths.provisioning=/etc/grafana/provisioning\u0026#34; grafana | logger=settings t=2025-06-17T12:40:19.5812188Z level=info msg=\u0026#34;Config overridden from command line\u0026#34; arg=\u0026#34;default.log.mode=console\u0026#34; grafana | logger=settings t=2025-06-17T12:40:19.581222384Z level=info msg=\u0026#34;Config overridden from Environment variable\u0026#34; var=\u0026#34;GF_PATHS_DATA=/var/lib/grafana\u0026#34; grafana | logger=settings t=2025-06-17T12:40:19.58122533Z level=info msg=\u0026#34;Config overridden from Environment variable\u0026#34; var=\u0026#34;GF_PATHS_LOGS=/var/log/grafana\u0026#34; grafana | logger=settings t=2025-06-17T12:40:19.581228361Z level=info msg=\u0026#34;Config overridden from Environment variable\u0026#34; var=\u0026#34;GF_PATHS_PLUGINS=/var/lib/grafana/plugins\u0026#34; grafana | logger=settings t=2025-06-17T12:40:19.581230583Z level=info msg=\u0026#34;Config overridden from Environment variable\u0026#34; var=\u0026#34;GF_PATHS_PROVISIONING=/etc/grafana/provisioning\u0026#34; ... Prometheus UI http://localhost:9090으로 접속했다면 Status \u0026gt; Targets의 타겟 상태가 UP 상태로 잘 올라와 있는지 확인한다.\nGrafana UI 설정해둔 주소 localhost:3000 접속 후 별다른 설정을 하지 않았다면 기본 로그인 정보인 (id: admin, pw: admin)으로 로그인한다.\nDATA SOURCES Add your first data source 클릭 이동 Prometheus 클릭 설정했던 http://prometheus:9090 입력 후 저장 주의할 것은 localhost 아니고 docker-compose.yml에서 설정한 컨테이너 이름이자 내부 DNS를 prometheus로 사용 대시보드 생성 대시보드 추가 과정 yml? yaml? yaml 확장자로 했을 때 Prometheus Status 잘 올라오지 않았고 별다른 설정 문제가 아니라면 기본적으로 찾게되는 확장자가 yml인가 싶어 변경했더니 잘 올라왔다.\nGemini YAML과 YML은 ==파일 확장자만 다를 뿐, 본질적으로 같은 것을 가리킵니다==. YAML은 \u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026quot;의 약자로, 사람이 읽기 쉬운 데이터 직렬화 언어입니다.\nYaml Ain\u0026rsquo;t Markup Language 또는 Yet Another Markup Language\n.html 대신 .htm으로도 사용된 것과 같이 옛날 옛날 한 옛날 Windows에서 Extensions는 파일 확장자가 3자로 제한되는 특성이 있었기 때문이라고 한다.\n🎯결론 시스템의 가시성과 안정성을 높이기 위한 가장 빠르고 효과적인 방법은 Prometheus와 Grafana를 Docker로 연동해 메트릭을 시각화하는 것이다.\n이 방식은 별도 설치 없이 빠르게 시작할 수 있으며 Spring Boot의 Actuator와 Micrometer를 활용하면 애플리케이션의 상태와 예외 흐름까지도 실시간으로 모니터링할 수 있다.\n운영 환경 이전에 개발 및 테스트 단계부터 시각화 환경을 구축해두는 것이 예외 대응력과 시스템 품질을 높이는 핵심 전략이 된다.\n","date":"2025-06-17T18:49:24+09:00","permalink":"https://blog.b9f1.com/p/2025-06-17-springboot-metrics-monitoring-with-prometheus-and-grafana/","title":"봄 모니터링 지표"},{"content":"📌개요 AWS에 배포하려면 반드시 알아야 할 기초적인 개념과 구성 요소들을 간단히 알아보자.\n📌내용 AWS 인프라 기초 개념 항목 설명 EC2 AWS의 가상 서버. Spring Boot 애플리케이션을 직접 배포할 수 있음. RDS AWS에서 제공하는 관리형 관계형 데이터베이스 서비스. MySQL, PostgreSQL 등 선택 가능. S3 파일(정적 리소스, 로그 등) 저장용 버킷. VPC Virtual Private Cloud. 네트워크 설정 단위. EC2, RDS는 VPC 내에 배치됨. IAM 권한 관리. EC2, RDS에 접근 가능한 사용자/서비스 권한 설정. Security Group AWS 방화벽 설정. EC2, RDS 접근을 허용하는 IP/포트 지정. 배포 순서별 필수 지식 1.Spring Boot 앱 배포 준비 빌드: ./gradlew bootJar 또는 mvn package 환경 분리: application.yml 혹은 application-prod.yml로 외부 DB 설정 로깅/에러 처리: EC2 상에서 로그를 확인할 수 있어야 함 (/var/log, nohup.out, etc.) 2.EC2 인스턴스 생성 Amazon Linux 2 또는 Ubuntu 22.04 선택 포트 22(SSH), 8080(Spring 앱 포트) 열기 인바운드 규칙에 내 PC의 IP만 허용 (보안상 중요) 3.EC2에 접속 및 배포 scp로 JAR 파일 전송하거나, git clone Java 설치 (sudo yum install java-17 등) 앱 실행: java -jar your-app.jar 실행 확인: curl localhost:8080, 또는 브라우저에서 접속 4.RDS 인스턴스 생성 및 연결 DB 엔진 선택: PostgreSQL, MySQL 등 VPC/Subnet, 보안 그룹 설정 인스턴스 생성 후 엔드포인트 + 포트 + 유저/비밀번호를 Spring Boot application.yml에 입력 DB 연결 확인: psql, mysql 등 CLI 도구 사용 가능 5.배포 안정화 및 운영 EC2 재부팅 시 앱 자동 실행: systemd 설정 또는 cron, nohup 로그 파일 분리: logback, log4j2 설정 AWS CloudWatch 연동하면 서버 상태 모니터링 가능 반드시 체크해야 할 보안 요소 항목 설명 보안 그룹 포트 제한 (22, 8080, 5432 등 최소화) DB 접근 제한 EC2의 private IP만 RDS 접근 허용 .pem 키 관리 Git 업로드 절대 금지. 로컬에 안전하게 보관 환경변수 관리 DB 비밀번호는 코드에 하드코딩하지 말고 .env 또는 AWS Parameter Store 사용 실무적으로 꼭 알아야 할 추가 지식 주제 이유 Elastic IP EC2 재시작 시 IP가 바뀌는 문제 방지 Route53 + 도메인 연결 도메인 연결 시 설정 S3 + CloudFront 정적 리소스 캐싱/배포 CodeDeploy / CodePipeline CI/CD 자동 배포 구성 Parameter Store / Secrets Manager 민감 정보 안전하게 관리 🎯결론 직접 생성해보며 확인해야 할 부분이 있지만 일단 간략한 개념부터 정리하고 알아봤다.\n기초 체크리스트 EC2 생성 및 SSH 접속 Spring Boot JAR 빌드 및 실행 RDS 생성 및 연결 포트 및 보안그룹 설정 로그 관리 및 서버 실행 유지 보안 설정 (환경변수, IP 제한 등) ⚙️EndNote 사전 지식 리눅스 CLI 명령어 (scp, chmod, java, systemctl 등) Spring Boot 프로파일, 환경변수 구조 RDB 기본 설정 및 JDBC 연결 구조 더 알아보기 Overview of Deployment Options on AWS (공식 백서) AWS App Runner Developer Guide Well-Architected Framework Spring Boot + RDS 가이드 (Spring 공식) ","date":"2025-06-16T17:00:28+09:00","permalink":"https://blog.b9f1.com/p/2025-06-16-aws-to-deploy-springboot-and-rdb/","title":"AWS 배포 기초"},{"content":"📌개요 트랜잭션의 ACID 속성 중 Isolcation(격리성) 은 다수의 트랜잭션이 동시에 실행될 때 각 트랜잭션이 서로에게 미치는 영향을 통제하기 위한 핵심 요소이며 가장 복잡하고 이해하기 어려운 속성이다.\n격리 수준이 보장하는 것은 단순히 일관성이 아니라 데이터 무결성과 비즈니스의 신뢰성 이다. 특히 동시성 환경에서 격리 수준이 부족하면 데이터는 꼬이고, 그 피해는 실시간으로 사용자에게 전달된다.\n이번 글에서 정리하는 건 모두 이해하고 작성할 수 없어서 관련 자료를 정리한 후 상세한 내용에 대해 더 깊게 파고들 예정이다.\n격리성이 무너지면서 발생하는 문제들을 예제로 설명하고, 이를 막기 위해 존재하는 격리 수준들에 대해 단계적으로 정리해본다.\n📌내용 격리성은 기본값으로 보장되고 있을까? 대부분의 RDBMS의 기본 격리 수준은 READ COMMITED 또는 REPEATABLE READ이지만 이름은 같아도 구현 방식과 보장 수준은 다르다. 예를 들어 Oracle의 SERIALIZABLE은 실제론 SNAPSHOT ISOLATION MySQL의 REPEATABLE READ는 기본적으로 Phantom Read를 방지하지 않으며 PostgreSQL은 MVCC 기반이지만 Serializable을 명시해야만 실제 직렬화 보장을 한다. MVCC: Multi-Version Concurrency Control, 다중 버전 동시성 제어 격리 수준은 이름이 같다고 같은 게 아니다. 반드시 DB별 구현 방식을 확인해야 한다.\n격리 수준이 깨질 때 벌어지는 현상들 유형 설명 예시 Dirty Read 커밋되지 않은 데이터를 읽음 A가 송금 처리 중인데, B가 중간 상태의 금액을 읽고 합산하여 중복 송금 발생 Non-Repeatable Read 동일 조건으로 두 번 조회했는데 결과가 다름 A가 상품 재고를 두 번 조회하는 사이, B가 재고를 변경함 Phantom Read 같은 쿼리인데 행 개수가 달라짐 A가 \u0026quot;나이 \u0026gt; 30\u0026quot; 조건으로 조회 → B가 35세 사용자를 삽입 → A가 다시 조회하면 결과 달라짐 실무에서 실제 발생한 사례 동시 송금 시스템에서의 경쟁 조건 사용자가 잔액이 100인 상태에서 거의 동시에 두 번 송금 요청을 보냄 둘 다 잔액 조회 결과 100을 읽고, 각각 80을 송금 → 총 160 송금됨 이유: READ COMMITTED 상태에서는 두 트랜잭션이 서로를 고려하지 못함 해결책은? SERIALIZABLE 수준의 격리 또는 애플리케이션 차원의 락 적용 ANSI SQL 격리 수준 비교표 Warning 격리 수준을 올리면 오류는 줄지만, 성능에 영향을 미친다.\n격리 수준 4가지와 이에 따른 현상 3가지가 있다. 이 3가지 현상을 읽기 현상(Read Phenomena)라고 표현한다.\nDirty Read: 커밋되지 않은(trash) 데이터를 읽는 현상 Non-Repeatable Read: 같은 쿼리를 두 번 실행했을 때 결과가 달라지는 현상 Phantom Read: 동일 조건의 쿼리에서 행 개수가 달라지는 현상 (삽입/삭제로 인해) 격리 수준 Dirty Read Non-Repeatable Read Phantom Read 비고 READ UNCOMMITTED 허용 허용 허용 최저 성능, 실사용 거의 없음 READ COMMITTED 방지 허용 허용 대부분 DB의 기본값 REPEATABLE READ 방지 방지 허용 MySQL 기본값 (하지만 Phantom Read 방지는 불완전함) SERIALIZABLE 방지 방지 방지 가장 강력하나 성능 저하 있음 Spring에서의 설정 방법 1 2 3 4 @Transactional(isolation = Isolation.SERIALIZABLE) public void processTransaction() { ... } Isolation.DEFAULT: DBMS 기본 설정 따름 프로젝트에 따라 .yml, .yaml 또는 DB 설정에서 전역 기본값을 조정할 수도 있음 트랜잭션을 사용할 때 @Transactional만 선언하고 끝이 아니라 어떤 isolation level이 적용되는지 알아본다면 좋을 것이다. 🎯결론 트랜잭션은 선언만으로 안전하지 않다.\n격리 수준은 코드의 동작 방식과 데이터 신뢰성을 결정짓는 중요한 요소다.\n문제를 겪고 나서야 \u0026ldquo;왜 데이터가 꼬였지?\u0026ldquo;를 고민하기보다, 미리 격리 수준을 설정하고 이해하는 것이 훨씬 값지다.\n⚙️EndNote 사전 지식 트랜잭션과 커밋/롤백의 개념 DB 락: Shared Lock vs Exclusive Lock MVCC(Multi-Version Concurrency Control) Read Phenomena in Transactions by oim_ 더 알아보기 PostgreSQL 공식 문서: Isolation Levels MySQL InnoDB 트랜잭션 격리 수준 Spring Declarative Transaction Management Hermitage: Testing the “I” in ACID I-9. 표준 SQL 이란? Isolation (database systems) ","date":"2025-06-15T17:31:13+09:00","permalink":"https://blog.b9f1.com/p/2025-06-15-db-what-happens-when-transaction-isolation-collapses/","title":"트랜잭션 격리성 - Isolation"},{"content":"📌개요 데이터베이스나 분산 시스템에서 트랜잭션은 안정성을 보장하는 핵심 개념이다. 특히 ACID(원자성, 일관성, 고립성, 지속성) 속성은 신뢰할 수 있는 데이터 처리를 위한 기본 토대다.\n이번 글에서는 ACID 각각의 속성이 보장되지 않을 때 어떤 문제들이 발생할 수 있는지 예시와 일반적인 해결책을 간단히 알아본다.\n📌내용 Atomicity - 원자성 모든 작업이 수행되거나, 아무것도 수행되지 않아야 한다.\n원자성이 깨지면, 하나의 트랜잭션 안에서 일부 작업만 처리되고 나머지가 실패한 상태가 될 수 있다.\n예시: A → B로 계좌 이체 중 A 계좌에서 출금은 됐지만 B 계좌로 입금이 되기 전 장애가 발생한 경우 → 금액 손실. 문제점: 시스템 상태가 불완전하게 갱신되어 데이터 손실 가능. 해결 방안: DBMS가 제공하는 트랜잭션 롤백(Rollback) 메커니즘 사용. try-catch를 통해 예외 발생 시 rollback() 호출하도록 처리. 1 2 3 4 5 6 7 8 try { connection.setAutoCommit(false); withdraw(fromAccount); deposit(toAccount); connection.commit(); } catch (Exception e) { connection.rollback(); // 전체 트랜잭션이 되돌아감 } Consistency - 일관성 트랜잭션 전후의 데이터 상태는 항상 시스템 규칙을 만족해야 한다.\n일관성은 데이터베이스에 정의된 제약 조건, 외래키, 트리거, 비즈니스 규칙 등이 항상 유지되어야 한다는 의미다.\n예시: 주문 시 존재하지 않는 상품 ID가 참조되거나, 재고가 음수가 되는 경우. 문제점: 무결성이 깨진 데이터가 DB에 저장될 수 있음. 해결 방안: DB 레벨: 외래 키, 유일성 제약, CHECK, NOT NULL 등의 제약 조건 설정 애플리케이션 레벨: 추가적인 비즈니스 유효성 검사 코드 구현 Warning 단, 일관성은 트랜잭션이 성공적으로 끝났을 경우에만 보장된다. 실패한 트랜잭션은 원자성과 함께 롤백되어 무결성을 해치지 않음.\nIsolation - 고립성 각 트랜잭션은 서로 독립적으로 실행돼야 한다.\n고립성이 깨지면 동시에 실행 중인 트랜잭션들이 서로 영향을 주며 비정상적인 데이터가 조회되거나 저장될 수 있다.\n주요 문제 유형: 문제 설명 예시 Dirty Read 다른 트랜잭션이 아직 커밋하지 않은 데이터를 읽음 B가 업데이트 중인 데이터를 A가 미리 읽음 Non-repeatable Read 같은 조건으로 두 번 조회했는데 결과가 달라짐 A가 한 사용자의 나이를 두 번 조회하는 사이 B가 그 값을 수정 Phantom Read 같은 조건으로 조회했는데 행 개수가 달라짐 A가 “나이 ≥ 30” 조건으로 조회한 후 B가 새로운 30세 사용자를 삽입 격리 수준 비교 (ANSI SQL 기준): 수준 보장 범위 발생 가능한 문제 READ UNCOMMITTED 아무것도 보장 안 됨 Dirty Read, Non-repeatable Read, Phantom Read READ COMMITTED 커밋된 데이터만 읽음 Non-repeatable Read, Phantom Read REPEATABLE READ 동일 쿼리 결과 동일 Phantom Read SERIALIZABLE 가장 강력, 완전한 고립 성능 저하 가능성 해결 방안: DB 격리 수준을 적절히 설정 (성능 vs 고립성 트레이드오프 고려) 비즈니스 중요도에 따라 적용 수준을 유연하게 구성 Durability - 지속성 트랜잭션이 커밋되면, 그 결과는 영구히 저장되어야 한다.\n지속성이 보장되지 않으면, 트랜잭션이 커밋되었더라도 시스템 장애 발생 시 데이터가 유실될 수 있다.\n예시: 사용자가 결제를 완료했는데, 직후 서버 다운으로 주문 내역이 저장되지 않음 → 신뢰성 손상 문제점: 커밋 이후에도 데이터 유실 가능 → 사용자 신뢰도 하락 해결 방안: DBMS의 WAL(Write-Ahead Logging): 로그를 먼저 디스크에 기록하고 나서 커밋 처리 RAID, 이중화 스토리지, 디스크 플러시(sync) 전략 분산 시스템에서는 Replication, Quorum 기반의 커밋 방식도 사용됨 🎯결론 트랜잭션의 ACID 속성은 단순한 개념이 아니라, 실제 시스템에서 데이터 무결성과 안정성의 최후 보루다.\n속성이 하나라도 무너지면 “커밋됐지만 저장되지 않음”, “이체했는데 금액이 사라짐”, “동시 접속 시 데이터 꼬임” 같은 치명적 문제가 발생할 수 있다.\n결국 이는 비즈니스 신뢰성과 사용자 경험에 직결된다.\n⚙️EndNote 사전 지식 트랜잭션 정의와 트랜잭션 경계 설정 데이터베이스의 커밋/롤백 메커니즘 SQL 격리 수준과 락(lock) 종류 (공유 락, 배타 락 등) 더 알아보기 PostgreSQL Isolation Levels 공식 문서 MySQL 트랜잭션 격리 수준 비교 Martin Kleppmann ","date":"2025-06-15T16:50:13+09:00","permalink":"https://blog.b9f1.com/p/2025-06-15-db-transaction-four-principles-acid/","title":"ACID"},{"content":"📌개요 JPA를 사용하면서 가장 자주 마주치는 성능 문제 중 하나가 바로 N+1 문제다. \u0026ldquo;나는 하나의 쿼리만 호출했는데 왜 수십 개의 쿼리가 날아가지?\u0026rdquo;\n이번엔 N+1 문제가 발생하는 원인과 이를 해결하는 방법을 Spring Boot 환경 중심으로 알아보자.\n📌내용 N+1 문제란? N+1 문제는 1개의 쿼리로 N개의 결과를 가져온 후 각 결과에 대해 N번 추가 쿼리를 실행하는 현상을 의미한다.\n1 2 3 4 5 6 ... List\u0026lt;Order\u0026gt; orders = orderRepository.findAll(); // 1번 for(Order order : orders) { System.out.println(order.getMember().getName()); // N번 } ... orders를 조회하는 findAll() 쿼리 1회 각 order의 연관된 member를 지연 로딩(LAZY)하면서 order 수만큼 N회 추가 쿼리 결과적으로 1+N회의 쿼리가 발생하게 된다.\n왜 이런 문제가 발생할까? 기본적으로 JPA는 연관 관계를 지연 로딩으로 설정하기 때문이다.\n1 2 3 4 5 @Entity public class Order { @ManyToOne(fetch = FetchType.LAZY) private Member member; } 지연 로딩의 목적은 불필요한 데이터를 미리 조회하지 않기 위한 것이지만 반복문처럼 연관 객체를 순차적으로 접근할 때 N+1 문제가 발생한다.\n해결 방법 1. Fetch Join 가장 직관적인 해결책이다. 연관된 엔티티를 함께 조회하는 조인 쿼리를 사용한다.\n1 2 3 4 5 6 @Query(\u0026#34;\u0026#34;\u0026#34; Select o FROM Order o JOIN FETCH o.member \u0026#34;\u0026#34;\u0026#34;) List\u0026lt;Order\u0026gt; findAllWithMember(); JPA는 이 쿼리 결과를 기반으로 Order와 Member를 한 번에 메모리에 올린다. Hibernate는 더 이상 각 member에 대해 추가 쿼리를 실행하지 않는다. 가장 강력하고 빠른 방법이지만 Fetch Join은 컬렉션(@OneToMany 등)에 사용할 경우 페이징이 불가능하다는 단점이 있다.\n2. @EntityGraph 엔티티 수준에서 Fetch Join과 유사한 효과를 얻을 수 있는 선언적 방법이다.\n1 2 3 @EntityGraph(attributePaths = {\u0026#34;member\u0026#34;}) @Query(\u0026#34;Select o FROM Order o\u0026#34;) List\u0026lt;Order\u0026gt; findAllWithMember(); 코드 가독성이 좋아지고 재사용 가능한 설정을 만들 수 있다. Fetch Join을 간결하게 사용하기 위한 어노테이션이고 내부적으로 기능이 비슷해서 비슷한 단점을 가진다. 실제로는 left outer join을 사용한다는 점을 주의해야 한다.\n3. Batch Size 설정 컬렉션에 대해 Lazy 로딩을 유지하면서 성능을 개선하는 방법이다.\n컬렉션이나 LAZY 로딩 관계의 객체들을 100개씩 in 절로 묶어서 한 번에 조회한다. 페이징 + 성능 최적화가 동시에 필요한 경우 유용하다. 1 2 3 4 5 spring: jpa: properties: hibernate: default_batch_fetch_size: 100 1 2 3 4 5 @Entity public class Order { @OneToMany(mappedBy = \u0026#34;order\u0026#34;) private List\u0026lt;OrderItem\u0026gt; items; } items 조회 시 in 절로 묶여 일괄 조회된다. 페이징 가능한 쿼리에 적합한 방법이다. Fetch Join VS @EntityGraph Info @EntityGraph와 JPQL fetch join은 기능적으로 유사하지만 내부 동작 방식과 조인 타입에서 차이를 가진다.\n핵심 차이 정리 항목 fetch join (JPQL/QueryDSL) @EntityGraph (Spring Data JPA) 선언 위치 JPQL/QueryDSL 내부 Repository 메서드 어노테이션 기본 조인 방식 INNER JOIN LEFT OUTER JOIN 연관 관계 없어도 조회됨? ❌ (자식 없으면 부모도 제외) ✅ (자식 없어도 부모 조회됨) 런타임 FetchType 전환 ❌ ✅ (LAZY → EAGER) 페이징 가능 여부 ❌ (컬렉션과 함께 사용 불가) ❌ (컬렉션 시 동일) 조건 활용 유연성 자유롭다 정적 메서드에 한정 중복 row 처리 distinct 필요 일부 자동 처리됨 예제 비교 1 2 3 4 5 6 7 8 9 10 11 // fetch join: inner join → 자식 없으면 조회되지 않음 @Query(\u0026#34;\u0026#34;\u0026#34; SELECT o FROM Order o JOIN FETCH o.member \u0026#34;\u0026#34;\u0026#34;) List\u0026lt;Order\u0026gt; findAllWithMember(); // entity graph: left outer join → 자식 없어도 조회됨 @EntityGraph(attributePaths = {\u0026#34;member\u0026#34;}) @Query(\u0026#34;SELECT o FROM Order o\u0026#34;) List\u0026lt;Order\u0026gt; findAllWithMember(); 상황에 따른 선택 가이드 상황 해결책 단일 객체 또는 ToOne 관계 조회 Fetch Join / EntityGraph 컬렉션 관계(@OneToMany 등) 페치 Batch Size 페이징 처리와 병행해야 하는 경우 Batch Size 또는 DTO Projection 🎯결론 JPA N+1 문제는 자동화된 지연 로딩의 그림자다. 하지만 문제를 정확히 이해하면 해결은 어렵지 않다.\n단 하나의 @ManyToOne 관계에서 시작된 N+1 문제도, 반복 루프나 리스트 응답에서는 큰 성능 이슈로 이어질 수 있다.\nFetch Join, EntityGraph, Batch Size 같은 다양한 전략을 상황에 맞게 적절히 조합하자.\n⚙️EndNote 사전 지식 JPA 기본 연관 관계(@OneToMany, @ManyToOne 등) LAZY vs EAGER 로딩 전략 JPQL 작성법 더 알아보기 [HIBERNATE] Batch Fetching [Spring Data JPA] JPA Query Methods MultipleBagFetchException 발생시 해결 방법 by 향로 ","date":"2025-06-15T15:34:08+09:00","permalink":"https://blog.b9f1.com/p/2025-06-15-springboot-jpa-causes-and-solutions-for-n-plus-1-problems/","title":"JPA의 대표적인 성능 병목"},{"content":"📌개요 공공데이터포털의 API 사용 시\n포털에서 발급 받은 Encoding 또는 Decoding 키를 이용해 포털에서 테스트 응답 확인이 가능하다. Postman 등 REST API를 테스트할 수 있는 툴에서 동일하게 응답 확인이 가능하다. 프로젝트에서 .http 테스트하거나 프로그램 실행 시 SERVICE_KEY_IS_NOT_REGISTERED_ERROR 오류를 만난다? 📌내용 URL 인코딩 퍼센트 인코딩 URL 인코딩에서는 퍼센트 인코딩(Percent encoding)이라는 방식을 사용한다. 퍼센트 인코딩은 RFC 3986에 정의되어 있다.\n\u0026ldquo;퍼센트 인코딩 URL 인코딩\u0026rdquo; 뿐만 아니라 URI, URN에도 사용될 수 있고 정확히는 \u0026lsquo;퍼센트 인코딩(Percent encoding)\u0026lsquo;이라는 용어가 더 적합하다고 한다.\n퍼센트 인코딩을 하는 이유는 인터넷에서 주고 받을 수 있는 문자는 ASCII 문자 뿐이기 때문이다. 따라서 ASCII가 아닌 문자는 전송 가능한 형태로 인코딩을 해야 한다.\n퍼센트 인코딩은 URL에서 URL로 사용할 수 없는 문자나 URL로 사용할 순 없지만 의미가 외곡될 수 있는 문자를 %XX (XX는 16진수)로 변환하는 방법이다. 예를 들어 한글은 ASCII가 아니기 때문에 UTF-8과 같은 방식으로 인코딩해야 한다. 예: 감자 -\u0026gt; %EA%B0%90%EC%9E%90 ASCII라도 예약된 의미를 가진 문자의 경우 그 문자 자체를 전달하고 싶다면 escape 처리가 필요하다. 예를 들어 / URL의 각 레벨을 구분, \u0026amp; 쿼리 파라미터를 구분, = 쿼리 파라미터 값 지정 A\u0026amp;B라는 글자를 보내고 싶을 땐 A%26B 이런 식으로 \u0026amp;을 이스케이프 처리 URL Encoding 사이트를 이용할 수도 있다. https://www.url-encode-decode.com/ 공공데이터포털 Open API 공공데이터포털에서 제공하는 apiKey는 W3C recommendations for URI addressing에 따라 +를 %2B로 변환한다.\n문제는 new URI(), URIComponents 등을 사용해도 발생하는 SERVICE_KEY_IS_NOT_REGISTERED_ERROR 오류였다.\nRestTemplate.getForEntity() 내부 코드를 좀 살펴보면 첫 번째 파라미터가 URI 타입인 것을 확인할 수 있고 new URI()로 맛있게 말아서 넘겨도 똑같은 오류를 만난다.\n1 2 3 4 5 6 @Override public \u0026lt;T\u0026gt; ResponseEntity\u0026lt;T\u0026gt; getForEntity(URI url, Class\u0026lt;T\u0026gt; responseType) throws RestClientException { RequestCallback requestCallback = acceptHeaderRequestCallback(responseType); ResponseExtractor\u0026lt;ResponseEntity\u0026lt;T\u0026gt;\u0026gt; responseExtractor = responseEntityExtractor(responseType); return nonNull(execute(url, HttpMethod.GET, requestCallback, responseExtractor)); } 그 원인은 + 기호가 인코딩 시 제외되는 문자이기 때문이었다. 따라서 아래와 같이 문제가 되는 기호를 먼저 정리하고 넘기는 방식으로 성공적인 응답을 받을 수 있었다.\n1 2 3 // + 기호는 인코딩에서 제외되기 때문에 미리 변환하고 // URI 클래스를 사용하면 URL 전송 할 때 문자열 그대로 날아가는 것이 아닌, 한 번 인코딩을 해서 보내준다 return new URI(base.replace(\u0026#34;+\u0026#34;, \u0026#34;%2B\u0026#34;)); 물론 이런 패턴 말고 다른 방법도 있을 수 있지만 인터넷에서 주고 받는 URL 인코딩에 대해 더 알아 볼 수 있었다.\n🎯결론 SERVICE_KEY_IS_NOT_REGISTERED_ERROR의 원인은 잘못된 URL 인코딩 처리였다.\n공공데이터포털에서 제공하는 Open API는 + 기호를 %2B로 인코딩해야 하며, 이를 올바르게 처리하지 않으면 인증 오류가 발생한다. RestTemplate이나 URI를 사용할 때도 인코딩 방식의 차이를 주의 깊게 살펴야 한다.\n퍼센트 인코딩의 정확한 이해와 사전 처리만으로도 오류를 쉽게 해결할 수 있었다.\n⚙️EndNote 사전 지식 퍼센트 인코딩(Percent Encoding) 또는 URL 인코딩의 개념 ASCII 문자 집합과 URL에서 허용되는 문자 Java의 URI, URLEncoder, RestTemplate의 동작 방식 차이 공공데이터포털의 OpenAPI 인증 구조 더 알아보기 RFC 3986 - URI Generic Syntax URLEncoder vs URI 차이점 Spring Framework RestTemplate 공식 문서 참고 자료 공공데이터포털 SERVICE_KEY_IS_NOT_REGISTERED_ERROR 원인 파헤치기 ","date":"2025-06-06T11:28:27+09:00","permalink":"https://blog.b9f1.com/p/2025-06-06-cs-network-open-api-url-service_key_is_not_registered_error/","title":"공공데이터포털 API"},{"content":"📌개요 데이터베이스 정규화는 이상 현상을 방지하고 데이터 일관성을 보장하기 위한 핵심 원칙이다.\n그러나 실전에서는 정규화된 모델이 항상 최고의 선택은 아니다. 자세한 내용은 다음에 다뤄보고 이번엔 역정규화(Denormalization) 가 필요하게 되는 상황과, 이를 적용할 때의 고려사항 및 장단점을 간단한 사례와 함께 살펴본다.\n📌내용 정규화와 현실의 간극 정규화는 테이블 간 중복을 줄이고 무결성을 유지하기 위한 훌륭한 이론이다. 하지만 현실에서는 다음과 같은 상황에서 정규화된 모델이 병목이 되기도 한다.\n역정규화가 필요한 대표적인 상황 복잡한 조인이 빈번하게 발생하는 경우 예: 게시판 목록을 조회할 때 게시글, 작성자, 댓글 수 등 여러 테이블을 조인해야 하는 경우 실시간 조회 성능이 중요한 경우 OLTP 시스템 또는 사용자 피드, 홈화면 로딩 등 수 ms 단위 응답이 필요한 상황 읽기 비율이 매우 높은 경우 쓰기보다 읽기가 훨씬 많고, 동일한 데이터를 반복 조회하는 경우 캐싱 효과를 극대화하기 위해 역정규화를 적용 집계 데이터가 자주 필요한 경우 주문 총액, 리뷰 수, 좋아요 수 등 매번 COUNT, SUM을 하지 않고 별도로 저장 역정규화 적용 시 고려사항 항목 고려 내용 데이터 정합성 중복된 컬럼이 여러 테이블에 있을 경우, 변경 시 일관성 유지가 어려움 유지보수 복잡도 역정규화된 필드는 직접 관리하거나 트리거, 애플리케이션 로직으로 동기화해야 함 성능 이점 조회 속도와 쿼리 단순화에는 확실한 효과가 있음 데이터 증가 중복 데이터로 인해 테이블 크기가 커질 수 있음 → 결국, ‘성능’과 ‘정합성’ 사이의 균형을 고려하여 설계해야 한다.\n장단점 비교 장점 단점 - 조인 최소화로 인한 속도 향상 - 쿼리 복잡도 감소 - 집계 데이터 미리 보관 가능 - 데이터 정합성 문제 가능성 - 유지보수 로직 증가 - 중복 데이터로 저장 공간 낭비 실무에서의 팁 정규화된 모델로 먼저 설계하고, 필요한 곳에만 역정규화한다. 집계용 컬럼은 write-through 방식으로 관리하거나, 이벤트 기반 비동기 처리도 고려한다. Redis, ElasticSearch와 같은 서브 시스템으로 조회 성능을 분산시키는 것도 대안이다. 🎯결론 “모든 데이터베이스는 처음엔 정규화로 시작하고, 결국엔 역정규화로 최적화된다.”\n정규화와 역정규화는 대립이 아닌 균형의 문제다. 목적에 맞는 데이터 구조 설계가 진짜 실력이다.\n⚙️EndNote 사전 지식 제1정규형~제3정규형 이해 기본 SQL 조인과 인덱스 작동 방식 데이터베이스 성능 튜닝 기초 더 알아보기 Martin Fowler - Refactoring Databases PostgreSQL Performance Tuning CQRS (Command Query Responsibility Segregation) 아키텍처 ","date":"2025-06-02T08:57:20+09:00","permalink":"https://blog.b9f1.com/p/2025-06-02-db-erd-reverse-normalization-when-and-why-do-you-use-it/","title":"역정규화, 언제 그리고 왜 사용하는가?"},{"content":"📌개요 SQL은 데이터베이스를 다루기 위한 언어지만, 그 목적에 따라 크게 두 가지로 나뉜다.\n바로 DDL(Data Definition Language) 과 DML(Data Manipulation Language)\n이 둘의 차이점을 명확히 정리하고, 각각의 대표적인 명령어와 그 용도를 알아보자.\n📌내용 DDL: 데이터 정의 언어 (Data Definition Language) DDL은 데이터베이스의 구조를 정의하는 데 사용된다. 즉, 테이블, 스키마, 인덱스, 뷰 등을 생성, 변경, 삭제할 때 사용하는 명령어들이다. 이러한 작업은 데이터의 틀을 잡는 작업이므로, 주로 개발 초기나 데이터 모델링 단계에서 많이 사용된다.\n대표 명령어 CREATE 테이블이나 데이터베이스 객체를 생성 1 2 3 4 CREATE TABLE users ( id INT PRIMARY KEY, name VARCHAR(50) ); ALTER 기존 테이블의 구조 변경 (컬럼 추가/수정/삭제 등) 1 ALTER TABLE users ADD email VARCHAR(100); DROP 테이블이나 객체를 삭제 1 DROP TABLE users; TRUNCATE 테이블을 비우되, 테이블 구조는 남김 1 TRUNCATE TABLE users; Warning DDL 명령은 대부분 자동 커밋(autocommit) 되어, 되돌릴 수 없는 경우가 많다.\nDML: 데이터 조작 언어 (Data Manipulation Language) DML은 이미 정의된 테이블 내의 데이터를 다루는 데 사용된다. 즉, 데이터를 조회하고, 추가하고, 수정하고, 삭제하는 등의 \u0026ldquo;행위\u0026quot;에 관련된 명령어들이다.\n대표 명령어 SELECT 데이터를 조회 1 SELECT * FROM users WHERE id = 1; INSERT 새로운 데이터를 추가 1 INSERT INTO users (id, name) VALUES (1, \u0026#39;Alice\u0026#39;); UPDATE 기존 데이터를 수정 1 UPDATE users SET name = \u0026#39;Bob\u0026#39; WHERE id = 1; DELETE 데이터를 삭제 1 DELETE FROM users WHERE id = 1; Tip DML 명령은 트랜잭션(transaction) 에 의해 제어되며, COMMIT 혹은 ROLLBACK이 가능하다.\nDDL vs DML 요약 비교 항목 DDL DML 목적 데이터 구조 정의 데이터 조작 대상 테이블, 뷰, 인덱스 등 테이블 내 행(row) 트랜잭션 자동 커밋(rollback 불가) 트랜잭션 제어 가능 사용 시점 설계, 구조 변경 운영, 조회/수정/삭제/추가 DDL, DML, DCL이란? 분류 명령어 설명 DML(Data Manipulation Language)데이터 조작어 SELECT 데이터베이스에 저장된 데이터를 조회하거나 검색하는 명령어.일명 RETRIEVE라고도 함. INSERT UPDATE DELETE 테이블에 데이터를 삽입, 수정, 삭제하는 명령어. DDL(Data Definition Language)데이터 정의어 CREATE ALTER DROP RENAME TRUNCATE 테이블, 뷰, 인덱스 등 데이터 구조를 정의(생성, 변경, 삭제, 이름변경 등)하는 명령어. DCL(Data Control Language)데이터 제어어 GRANT REVOKE 사용자에게 데이터베이스 객체에 대한 권한을 부여하거나 회수하는 명령어. TCL(Transaction Control Language)트랜잭션 제어어 COMMIT ROLLBACK SAVEPOINT DML 명령으로 변경된 데이터를 트랜잭션 단위로 확정하거나 취소하거나, 중간 저장점을 설정하는 명령어. 🎯결론 DDL은 구조를, DML은 내용을 다룬다. 데이터베이스를 이해하고 활용하려면 이 두 가지의 차이를 명확히 아는 것이 핵심이다. DDL로 틀을 만들고, DML로 그 틀 안을 채우는 구조로 SQL을 학습해 보자.\n⚙️EndNote 사전 지식 데이터베이스 기본 개념 (테이블, 행, 열) SQL 구문 기본 문법 더 알아보기 PostgreSQL Documentation – SQL Commands SQLBolt – Learn SQL the easy way W3Schools SQL Tutorial ","date":"2025-06-02T08:34:00+09:00","permalink":"https://blog.b9f1.com/p/2025-06-02-db-sql-differences-between-ddl-and-dml/","title":"DDL VS DML"},{"content":"📌개요 Controller의 Request 객체를 Service에 그대로 전달하면 처음엔 간편해 보일 수 있지만, 실제로는 여러 설계상의 문제가 생긴다.\n계층간 의존성이 강해진다. Service의 역할을 침해한다. 확장성과 재사용성이 저하된다. 유지보수 및 테스트가 어려워진다. 객체지향 원칙을 위배하게 된다. 이러한 문제를 방지하기 위해 Controller에서는 Service에 전달할 Command 객체를 별도로 정의하여 사용하는 것이 바람직하다.\nCommand는 DTO와 유사해 보이지만, 그 목적과 책임이 분명히 다르다.\nDTO와 Command 객체에는 어떤 차이가 있는지 코드와 함께 비교해보고 이 설계가 가지는 의미에 대해 알아보자.\n📌내용 전반적인 내용을 작성한다. 관련된 재미있는 일화가 있다면 함께 작성하여 기억에 오래 남는 글을 작성한다.\n이름만 다르고 구조는 같은 거 아닌가? 간단한 CRUD 프로젝트에선 그럴 수 있다. 실제로 구조가 동일한 경우도 있지만, 의미 단위로 객체를 분리하면 테스트, 유지보수, 의미 표현 측면에서 장점이 크다.\n1 2 3 4 // 간단한 구조에서는 굳이 Command 객체를 만들지 않고도 계층 분리가 가능하다. public UserResponse createUser(UserCreateRequest request) { return userService.create(request.name(), LocalDate.parse(request.birthDate())); } 하지만 애플리케이션이 성장하고 도메인 로직이 복잡해질수록, 구조적으로 명확한 역할 분리가 필요한 시점이 오게 된다. 이때 DTO와 Command는 단순히 구조가 비슷하더라도 책임과 의미가 다르기 때문에 분리되어야 한다.\n이처럼 작은 프로젝트에서는 구조 재사용도 가능하지만, 의미 단위로 객체를 분리해두면 도메인 개념 확장 또는 API 포맷 변경이 발생했을 때 유연하게 대처할 수 있다.\n목적과 책임이 어떻게 다를까? 이름만 다른 게 아니라 목적과 책임이 다르기 때문에 분리된 객체로 사용되는 것이다.\n구분 DTO (Request) Command 사용 계층 Controller (웹 계층) Service 또는 도메인 계층 책임 외부 요청 수집, JSON 매핑, 검증 포맷 제공 비즈니스 로직 수행을 위한 의미 있는 명령 표현 필드의 표현 방식 문자열 그대로 전달 (\u0026quot;1990-01-01\u0026quot;) 비즈니스에 맞는 타입으로 변환 (LocalDate) 검증 포인트 외부 형식 검증 (@Valid, @NotNull) 도메인 비즈니스 규칙 검증 (validate()) 구조 유연성 프론트 요구에 따라 자주 변경됨 서비스/도메인 로직에 맞춰 고정됨 테스트 용이성 Web 계층에 의존 (Spring Test 등 필요) 순수 Java 객체로 유닛 테스트 가능 유효성 검증 책임 분리 Request에선 유효성 검증, Command에선 비즈니스 규칙을 검증할 수 있다.\n계층 검증 목적 사용하는 도구/패턴 예시 Request DTO (Controller) 외부 입력의 형식 검증 @Valid, @Pattern, @Email 이메일 형식, 날짜 포맷, 필수 여부 등 Command 객체 (Service/Domain) 비즈니스 규칙 검증 도메인 메서드, 자체 validate() 메서드 나이 제한, 상태 전이 유효성, 고유 값 중복 등 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Request // 외부에서 받은 형태: 문자열 기반 public record UserCreateRequest( @NotBlank String name, @Email String email, @Pattern(regexp = \u0026#34;\\\\d{4}-\\\\d{2}-\\\\d{2}\u0026#34;) String birthDate ) { public CreateUserCommand toCommand() { return new CreateUserCommand(name, email, LocalDate.parse(birthDate)); } } // Command // 내부에선 진짜 도메인 개념 기반 public record CreateUserCommand(String name, String email, LocalDate birthDate) { public void validate() { if (birthDate.isAfter(LocalDate.now())) { throw new BusinessException(\u0026#34;미래에 태어났니?\u0026#34;); } } } Command에서 Request를 참조하지 말라 Command 객체가 Request를 참조하게 되면 계층 간 결합이 생기고, 이는 곧 Service나 Domain 계층이 Controller에 의존하게 되는 구조를 의미한다.\n1 2 3 4 5 6 // 잘못된 예: 의존 방향이 역전됨 public class CreateUserCommand { public static CreateUserCommand from(UserCreateRequest request) { return new CreateUserCommand(request.name(), ...); } } 올바른 설계는 Request가 Command를 생성하는 방향(Request → Command) 이다.\n이는 \u0026ldquo;의존 방향이 위에서 아래로 흐르도록\u0026rdquo; 설계하는 객체지향 계층 구조의 기본 원칙이다.\n이처럼 계층 간 의존성은 반드시 위에서 아래 방향으로만 흘러야 한다. 도메인 계층이 프레젠테이션 계층의 존재를 인지하게 되는 순간, 전체 구조는 깨지기 시작한다.\n보완 전략 만약 Command 객체를 생성하기 위한 toCommand()등의 메서드가 너무 커지고 DTO에서 책임이 많아질 것 같으면 Mapper 클래스를 따로 만드는 방법을 고려해보자.\n1 2 3 4 5 public class UserMapper { public static CreateUserCommand toCommand(UserCreateRequest request) { return new CreateUserCommand(request.name(), ...); } } Anemic Domain Model DTO와 Command 객체의 역할 분리는, 도메인 계층에서의 책임을 명확히 분리하여 풍부한 도메인 모델(Rich Domain Model) 로 발전할 수 있는 기반을 마련한다는 측면에서 중요하다.\nDTO와 Command를 적절히 분리하는 작업은 Anemic Domain Model을 피하고, 도메인 모델이 비즈니스 로직을 스스로 갖는 Rich Domain Model로 성장하는 기반이 된다.\n참고: Anemic Domain Model Anemic Domain Model(빈약한 도메인 모델) 영어권 개발자들 사이에서 실제로 널리 사용되는 용어이며, DDD(Domain-Driven Design) 맥락에서 등장한 개념이다. 이 표현은 2003년 마틴 파울러(Martin Fowler)가 처음으로 정리해 소개했으며, 당시부터 비판적인 의미로 사용되어 왔다.\n🎯결론 \u0026ldquo;겉으로는 같은 구조처럼 보여도, 계층과 책임이 다르면 객체도 분리되어야 한다.\u0026rdquo;\nWeb 계층의 입력 형식과 도메인 계층의 명령은 다른 역할을 하며, 이 둘을 구분하는 설계는 유지보수성과 테스트 용이성, 도메인 주도 설계 모두를 위한 기초가 된다.\n⚙️EndNote 사전 지식 계층형 아키텍처 (Layered Architecture) DDD 기본 개념 Java의 record 문법 Bean Validation (@Valid, @NotNull 등) 더 알아보기 Anemic Domain Model What is the difference between DTO and Command in DDD? CQRS란 무엇인가? ","date":"2025-05-24T17:17:02+09:00","permalink":"https://blog.b9f1.com/p/2025-05-24-design-dto-vs-command/","title":"DTO VS Command"},{"content":"📌개요 Spring Boot를 활용한 REST API 개발에서 @RestController는 가장 많이 사용되는 어노테이션 중 하나다. 하지만 그 이면에서 실제 HTTP 요청이 어떻게 애플리케이션으로 전달되고, 어떤 과정을 거쳐 클라이언트에게 JSON 등으로 응답이 전달되는지는 자주 간과된다.\n이번 글에서는 Spring MVC의 요청 처리 흐름을 DispatcherServlet을 기점으로 전반적으로 살펴보고, 특히 HttpMessageConverter가 어떤 역할을 하며 언제 동작하는지 명확히 정리해본다.\n📌내용 요청 처리 흐름 요약 Spring Boot 애플리케이션에서 HTTP 요청이 들어오면 다음과 같은 순서로 처리된다.\n요청 수신 – DispatcherServlet\n모든 HTTP 요청은 먼저 DispatcherServlet에서 수신된다. 이 서블릿은 Front Controller로서 전체 요청의 진입점이며, Spring Boot에서는 자동으로 등록된다. 핸들러 탐색 – HandlerMapping\n어떤 컨트롤러 메서드가 이 요청을 처리할지 결정한다. @RequestMapping, @GetMapping 등으로 설정된 경로 정보를 기반으로 매핑이 일어난다. 핸들러 실행 – HandlerAdapter\n찾은 핸들러(즉, @RestController의 메서드)를 실행하는 어댑터. @RequestBody가 선언되어 있다면 이 시점에 HttpMessageConverter를 통해 JSON → 객체 변환이 이뤄진다. 응답 변환 – HttpMessageConverter\n컨트롤러 메서드가 반환한 Java 객체는 HttpMessageConverter를 통해 다시 JSON 등의 HTTP 응답 본문으로 변환된다. 이때 사용되는 대표적인 구현체로는 MappingJackson2HttpMessageConverter가 있으며, 내부적으로 Jackson을 사용한다. 응답 반환 – DispatcherServlet\n변환된 응답은 다시 DispatcherServlet을 통해 클라이언트에게 반환된다. HttpMessageConverter의 동작 시점과 역할 HttpMessageConverter는 요청과 응답의 바디(body)를 변환하는 컴포넌트로서 다음과 같은 역할을 한다:\n@RequestBody가 있는 경우: 요청 바디를 JSON → Java 객체로 역직렬화 (ex. DTO로 바인딩) @ResponseBody 또는 @RestController가 있는 경우: 반환되는 객체를 Java 객체 → JSON으로 직렬화 이 변환은 RequestMappingHandlerAdapter에 등록된 messageConverters 리스트를 순회하며 타입과 Content-Type 헤더를 기반으로 적절한 컨버터를 찾아 자동으로 수행된다.\nsequenceDiagram participant Client participant DispatcherServlet participant HandlerMapping participant HandlerAdapter participant Controller participant HttpMessageConverter Client-\u003e\u003eDispatcherServlet: HTTP 요청 전송 DispatcherServlet-\u003e\u003eHandlerMapping: 핸들러 조회 HandlerMapping--\u003e\u003eDispatcherServlet: 핸들러 반환 DispatcherServlet-\u003e\u003eHandlerAdapter: 핸들러 실행 요청 HandlerAdapter-\u003e\u003eHttpMessageConverter: 요청 바디 역직렬화 (JSON → 객체) HttpMessageConverter--\u003e\u003eHandlerAdapter: 객체 반환 HandlerAdapter-\u003e\u003eController: 메서드 호출 Controller--\u003e\u003eHandlerAdapter: 객체 반환 HandlerAdapter-\u003e\u003eHttpMessageConverter: 객체 직렬화 (객체 → JSON) HttpMessageConverter--\u003e\u003eHandlerAdapter: JSON 반환 HandlerAdapter--\u003e\u003eDispatcherServlet: 응답 반환 DispatcherServlet--\u003e\u003eClient: 응답 전송 MappingJackson2HttpMessageConverter MappingJackson2HttpMessageConverter는 Spring MVC에서 **Java 객체를 JSON으로 직렬화하거나 JSON을 Java 객체로 역직렬화할 때 사용되는 기본 HttpMessageConverter**이다. 내부적으로 Jackson 라이브러리를 활용하여 JSON 처리 기능을 수행한다.\nSpring Boot를 사용하면 기본적으로 Jackson이 의존성에 포함되며, 이로 인해 MappingJackson2HttpMessageConverter는 자동으로 등록되어 동작한다. 주로 다음과 같은 상황에서 사용된다:\n클라이언트가 Content-Type: application/json으로 JSON 요청을 보낼 경우, @RequestBody에 해당 JSON이 자동으로 매핑된다. 컨트롤러가 객체를 반환할 때 @ResponseBody 또는 @RestController가 붙어 있으면, 해당 객체는 JSON으로 변환되어 응답된다. 1 2 3 4 5 @PostMapping(\u0026#34;/users\u0026#34;) public ResponseEntity\u0026lt;UserResponse\u0026gt; createUser(@RequestBody UserCreateRequest request) { User user = userService.create(request); return ResponseEntity.ok(new UserResponse(user)); } 위의 예시에서 @RequestBody를 통해 들어온 JSON은 Jackson이 UserCreateRequest로 역직렬화하며, 반환되는 UserResponse는 JSON으로 직렬화되어 클라이언트로 전달된다.\n커스터마이징 Jackson 설정은 다양하게 커스터마이징할 수 있다. 예를 들어:\nObjectMapper의 설정 변경 (@JsonNaming, @JsonProperty, @JsonIgnore 등) 날짜 포맷 지정 null 필드 무시 필드 snake_case ↔ camelCase 자동 변환 1 2 3 4 5 6 7 @Bean public Jackson2ObjectMapperBuilderCustomizer customizer() { return builder -\u0026gt; builder .featuresToDisable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS) .propertyNamingStrategy(PropertyNamingStrategies.SNAKE_CASE) .serializationInclusion(JsonInclude.Include.NON_NULL); } 이렇게 하면 MappingJackson2HttpMessageConverter에 연결된 ObjectMapper가 위 설정을 따라 동작하게 된다.\n기타 팁 Spring Boot는 spring.jackson.* 속성을 통해 설정을 간편하게 지원한다. 필요에 따라 WebMvcConfigurer에서 직접 HttpMessageConverter를 설정해 특정 타입만 Jackson 대신 다른 컨버터로 처리하게 할 수 있다. 1 2 3 4 @Override public void configureMessageConverters(List\u0026lt;HttpMessageConverter\u0026lt;?\u0026gt;\u0026gt; converters) { converters.add(new MappingJackson2HttpMessageConverter(myCustomObjectMapper())); } 🎯결론 Spring MVC의 요청 처리 흐름은 DispatcherServlet에서 시작해 HandlerMapping, HandlerAdapter, HttpMessageConverter를 통해 RESTful 서비스를 완성한다.\n특히 HttpMessageConverter는 요청과 응답을 Java 객체 ↔ JSON 사이에서 자동 변환해주는 핵심적인 컴포넌트로, 동작 시점과 역할을 명확히 이해하는 것이 REST API 개발의 안정성과 유지보수성을 높이는 데 매우 중요하다.\n⚙️EndNote 사전 지식 서블릿 기반 웹 애플리케이션 구조 Spring MVC의 @Controller, @RestController 어노테이션 차이 Jackson의 직렬화/역직렬화 개념 더 알아보기 Spring Web MVC 공식 문서 Spring Boot의 HttpMessageConverters 설정 DispatcherServlet 내부 구조 ","date":"2025-05-21T21:28:02+09:00","permalink":"https://blog.b9f1.com/p/2025-05-21-springboot-until-restcontrollers-request-is-returned-in-response/","title":"@RestController 요청이 응답으로 반환되기까지"},{"content":"📌개요 과거 웹 서비스 API의 표준은 SOAP(Simple Object Access Protocol)였다.\n그러나 2010년대를 지나며 REST(Representational State Transfer)가 빠르게 주류로 자리 잡았고 요즘 대부분의 공개 웹 API에서 RESTful API가 사실상의 표준으로 자리 잡고 있다.\n웹 API의 발전 과정 속에서, SOAP에서 REST로의 전환이 일어난 배경과 그에 따른 장단점을 알아보자.\n📌내용 SOAP의 등장과 전성기 SOAP은 WSDL(Web Service Definition Language), XML 기반 메시지 포맷 그리고 HTTP 외에도 SMTP나 FTP를 사용할 수 있는 유연성 덕분에 초기에는 대형 엔터프라이즈 시스템에서 주로 채택되었다. 보안(SOAP Security), 트랜잭션, 메시지 무결성 등 강력한 스펙이 특징이었다.\n그러나 SOAP은 다음과 같은 문제를 가지고 있었다.\n메시지 포맷이 무겁고 복잡함 (XML 기반) 학습 비용이 높고 구현이 어려움 브라우저, 모바일 등 가벼운 클라이언트 환경과 맞지 않음 REST의 부상 2000년에 로이 필딩(Roy Fielding)이 논문에서 제시한 REST는 본래 HTTP의 아키텍처 스타일로 제안된 개념이었지만 시간이 지나며 \u0026ldquo;RESTful API\u0026quot;라는 개념으로 대중화되었다.\nREST의 핵심은 자원(Resource) 지향 아키텍처와 표준 HTTP 메서드(GET, POST, PUT, DELETE)를 활용한 통신이다.\nREST가 빠르게 채택된 이유는 다음과 같다.\nHTTP라는 웹의 표준을 그대로 활용 JSON 기반 경량 메시지 포맷 (브라우저/모바일 친화적) 상태 없는(stateless) 구조로 확장성 우수 클라이언트와 서버 간 결합도가 낮음 문서화가 간단하고 테스트/디버깅이 쉬움 SOAP VS REST 항목 SOAP REST 메시지 포맷 XML (무겁고 엄격) JSON, XML (가볍고 유연) 표준 WSDL 등 다양한 스펙 존재 명확한 표준 없음 (URI, HTTP 활용) 보안, 트랜잭션 WS-Security, WS-Atomic 등 내장 지원 HTTP 보안 활용, 트랜잭션 미지원 상태성 상태 유지(Stateful) 가능 Stateless 기반 학습 곡선 높음 (설정과 구현 복잡) 낮음 (간단한 HTTP 인터페이스) 확장성과 경량성 제한적 뛰어남 REST 이후의 대안이 있을까? REST는 단순하고 확장 가능한 아키텍처지만, 다음과 같은 한계가 존재한다.\n과도한 데이터 전송 (Over-fetching/Under-fetching) 정적인 엔드포인트 설계 실시간 양방향 통신 미지원 리소스 간 관계 표현의 어려움 이러한 문제를 해결하기 위한 대안으로 다음 기술이 떠오르고 있다.\nGraphQL: 클라이언트가 필요한 데이터만 요청할 수 있어 Over-fetching/Under-fetching 문제를 해소함. gRPC: HTTP/2 기반의 양방향 스트리밍과 낮은 대역폭, 빠른 응답속도를 제공하여 마이크로서비스 간 통신에 적합함. Async API, WebSocket 기반 API: 실시간 스트리밍 통신 및 이벤트 기반 시스템에 특화됨. 이렇듯 REST는 여전히 강력한 기본값이지만, 목적에 따라 더 나은 대안들이 상황별로 사용되고 있다.\n🎯결론 페이스북, 트위터, 구글, 아마존 등 주요 플랫폼 API는 거의 모두 REST 기반이다. 심지어 마이크로소프트도 SOAP에서 REST 기반 API로 점진적으로 이동하고 있다고 한다.\n⚙️EndNote 사전 지식 HTTP 메서드(GET, POST, PUT, DELETE) XML vs. JSON 포맷 클라이언트-서버 아키텍처 상태 유지(Stateful) vs 상태 없음(Stateless) 더 알아보기 Architectural Styles and the Design of Network-based Software Architectures - Roy Thomas Fielding Import a SOAP API to API Management and convert it to REST RESTful API 디자인 가이드 - Microsoft SOAP vs REST 비교 블로그 - Postman GraphQL 공식 문서 gRPC 공식 문서 ","date":"2025-05-20T13:23:57+09:00","permalink":"https://blog.b9f1.com/p/2025-05-20-soap-to-rest-key-transition-in-web-api-evolution/","title":"웹 API 진화의 핵심 전환"},{"content":"📌개요 Spring 서버를 실행하고 크롬에서 로컬 호스트를 접속한 뒤 개발자 모드를 활성화 할 때 계속 같은 오류가 발생했다.\n오류가 발생한 시점은 사용자 정의 예외를 공통으로 처리하기 위해 클래스를 추가한 시점에 발생했다.\n프로그램에는 영향이 없는 것 같았지만 오류니까 거슬린다. 프로덕션 환경에선 문제 없는 것 같은데 로컬 개발 환경에서 자주 발견되는 것 같다.\n📌내용 1 [http-nio-8080-exec-7] ERROR GlobalExceptionHandler - [시스템 오류] No static resource .well-known/appspecific/com.chrome.devtools.json. 로그 포맷을 변경해서 일반적이진 않겠지만 이 부분 .well-known/appspecific/com.chrome.devtools.json., 개발자 모드를 활성화할 때마다 정적 리소스를 요청한다.\n크로미움 기반 브라우저에서 로컬 호스트의 개발 서버를 열 때 재현할 수 있는 것 같은데 그래서 저 오류가 뭔지 궁금했다.\nChromium DevTools Ecosystem Guide문서에서 관련 내용을 확인할 수 있다.\n어떤 해결 방법이 있을까 크롬 개발자 도구가 자동으로 .well-known/appspecific/\u0026hellip; 파일을 요청하면서 생기는 현상. 서버에 실제로 해당 정적 리소스가 없어서 Spring이 오류를 던지는 것이며, 보안 위험은 없다.\n무시하거나 더미 리소스를 만들어주거나 요청 경로 필터링으로 처리한다. 코드 기반으로 살펴 보기 일단 내 경우는 에러 핸들러 추가까진 안 보이다가 로그를 남기는 시점에 보이기 시작했다.\n땜빵(?)을 하긴 했지만 별로 좋은 해결책은 아닌 것 같고.. 일단 코드 대충 보고 원인부터 알아보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @ControllerAdvice public class GlobalExceptionHandler { private static final Logger log = LogManager.getLogger(GlobalExceptionHandler.class); @ExceptionHandler(BusinessException.class) public ResponseEntity\u0026lt;ErrorResponse\u0026gt; handleBusinessException(BusinessException e) { log.error(\u0026#34;[서비스 오류] code: {}, messageKey: {}\u0026#34;, e.getErrorCode().getCode(), e.getErrorCode().getMessageKey(), e); ErrorCode errorCode = e.getErrorCode(); HttpStatus status = mapToHttpStatus(errorCode); ... return ResponseEntity.status(status).body(response); } @ExceptionHandler(Exception.class) public ResponseEntity\u0026lt;ErrorResponse\u0026gt; handleGeneralException(Exception e) { // 크롬 버전 때문에 출력되는 에러 if (e.getMessage().equals(\u0026#34;No static resource .well-known/appspecific/com.chrome.devtools.json.\u0026#34;)) { return ResponseEntity.status(HttpStatus.NOT_FOUND).build(); } log.error(\u0026#34;[시스템 오류] {}\u0026#34;, e.getMessage(), e); ... return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(response); } private HttpStatus mapToHttpStatus(ErrorCode errorCode) { ... } } Spring MVC는 기본적으로 다음과 같이 예외를 처리한다.\n정적 리소스를 찾지 못하면 NoResourceFoundException 또는 NoHandlerFoundException을 발생 시킨다. 이 예외는 기보적으로 Spring 내부 DispatcherSevlet이 처리하고 브라우저엔 404 응답만 보내지 로그로 남기지 않는다. 그런데\u0026hellip; 이렇게 명시적으로 로그를 박아 버리면\nSpring의 내부 예외 처리 메커니즘을 가로챈다 .well-known/... 같은 요청에서 발생하는 예외도 잡힌다. 핸들러는 알 수 없는 시스템 오류로 간주하고 log.error()를 남긴다. 그 결과 크롬 개발자 도구에서 자동으로 보내는 요청에서도 에러 로그가 쌓인다. 1 2 3 4 5 @ExceptionHandler(Exception.class) public ResponseEntity\u0026lt;ErrorResponse\u0026gt; handleGeneralException(Exception e) { log.error(\u0026#34;[시스템 오류] {}\u0026#34;, e.getMessage(), e); ... } 지금의 나로선.. .well-known 요청을 필터나 핸들러에서 따로 무시 처리 또는 @ExceptionHandler의 범위를 좁혀서 시스템 예외 외에는 무시 가장 깔끔한 방법은 필요 없는 경로의 예외는 로깅하지 않도록 분기 처리하는 것 Automatic Workspace Folders란? Chrome DevTools (버전 M-135 이상) 에서 도입된 기능으로, DevTools가 개발 서버(devserver)로부터 프로젝트 디렉토리 정보를 자동으로 인식해, 로컬 워크스페이스로 연결하는 기능이다.\n기존의 Workspaces 기능을 자동화한 것으로, 디버깅 중 DevTools에서 소스 코드를 수정할 때 해당 수정 사항이 로컬 파일에 직접 반영될 수 있도록 설정해준다고 한다.\n이런 기능이 생긴 배경 기존 DevTools Workspaces 기능(M-63 도입)은 다음과 같은 불편이 있었다.\n수동 설정이 필요함 (Workspace 탭에서 직접 폴더 지정) 프로젝트별 관리가 불편함 (매번 폴더 추가/삭제 필요) 작동 방식 Chrome DevTools는 다음 조건을 만족할 때 자동으로 .well-known/appspecific/com.chrome.devtools.json 파일을 요청한다.\n브라우저에서 여는 페이지의 origin이 localhost DevTools가 열려 있는 상태 이 파일이 존재하면 DevTools는 해당 정보를 기반으로 자동으로 워크스페이스로 연결한다.\n요청되는 JSON 파일 요청 경로: /.well-known/appspecific/com.chrome.devtools.json 이 JSON 파일은 다음과 같은 구조를 가진다. 1 2 3 4 5 6 { \u0026#34;workspace\u0026#34;: { \u0026#34;root\u0026#34;: \u0026#34;/Users/foobar/Projects/my-awesome-web-project\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;53b029bb-c989-4dca-969b-835fecec3717\u0026#34; } } 각 필드 설명 workspace.root: 프로젝트 디렉토리의 절대 경로 workspace.uuid: 프로젝트 고유 식별자 (v4 UUID 등 무작위로 생성) 사용 조건 M-135 이상에서 사용 가능 M-136에서는 기본적으로 기능이 활성화되어 있음 이전 버전에서는 다음 두 플래그를 활성화해야 함 1 2 chrome://flags#devtools-project-settings chrome://flags#devtools-automatic-workspace-folders 요약 정보 항목 내용 기능 이름 Automatic Workspace Folders 도입 버전 M-135 (기능 비활성화), M-136 (기본 활성화) 목적 DevTools에서 로컬 프로젝트와 자동 연결하여 코드 수정 반영 작동 조건 origin이 localhost일 때 DevTools에서 JSON 요청 경로 /.well-known/appspecific/com.chrome.devtools.json JSON 필드 workspace.root, workspace.uuid 부가 도구 Vite 플러그인 존재, npx serve 예시 제공됨 🎯결론 명확한 해결책은 없지만 불필요한 로그를 출력하지 않게 설정하는 것이 좋겠다. 크롬 기능에 대해 실제 사용 예시나 한계, 조건 등을 알아보는 것도 좋겠다.\n⚙️EndNote 참고 자료 https://github.com/withastro/astro/issues/13789 Chromium DevTools Ecosystem Guide Chrome 135의 DevTools의 새로운 기능 ","date":"2025-05-20T11:18:19+09:00","permalink":"https://blog.b9f1.com/p/2025-05-20-no-static-resource-well-known-appspecific-com.chrome.devtools.json/","title":"No static resource"},{"content":"📌개요 Spring MVC는 클라이언트의 요청을 처리하고 응답을 반환하는 웹 프레임워크다. 이때 핵심적인 역할을 하는 것이 @Controller, @RestController이다.\n두 어노테이션은 비슷해보이지만 응답 방식에서 중요한 차이를 가진다.\n📌내용 요청 처리 흐름 sequenceDiagram participant Client participant DispatcherServlet participant HandlerMapping participant HandlerAdapter participant Controller participant ViewResolver participant HttpMessageConverter Client-\u003e\u003eDispatcherServlet: HTTP 요청 DispatcherServlet-\u003e\u003eHandlerMapping: 핸들러 탐색 HandlerMapping--\u003e\u003eDispatcherServlet: 핸들러 반환 DispatcherServlet-\u003e\u003eHandlerAdapter: 핸들러 실행 위임 HandlerAdapter-\u003e\u003eController: 메서드 실행 alt @Controller Controller--\u003e\u003eHandlerAdapter: View 이름 반환 HandlerAdapter-\u003e\u003eViewResolver: View 이름으로 View 객체 생성 ViewResolver--\u003e\u003eDispatcherServlet: View 객체 (HTML 포함) else @RestController Controller--\u003e\u003eHandlerAdapter: 객체 반환 HandlerAdapter-\u003e\u003eHttpMessageConverter: 객체를 JSON으로 직렬화 HttpMessageConverter--\u003e\u003eDispatcherServlet: 직렬화된 JSON end DispatcherServlet--\u003e\u003eClient: 응답 전송 Client가 HTTP 요청을 하게 되면 DispatcherServlet이 가로챔 모든 HTTP 요청은 DispatcherServlet이 가로채며 시작된다. DispatcherServlet은 HandlerMapping을 통해 해당 요청을 처리할 Handler를 찾은 다음 HandlerAdapter에게 해당 핸들러를 실행하도록 위임한다. 실행을 위임한다. HandlerAdapter는 Controller의 메서드를 실행한다. 어떤 Handler가 됐든 HandlerAdapter가 실제 메서드를 실행한다. @Controller, @RestController 케이스의 Resolver 호출 @Controller 어노테이션으로 등록된 메서드의 경우 Controller가 반환한 View 이름과 Model 데이터를 기반으로 DispatcherServlet이 ViewResolver를 통해 View 객체를 생성한 뒤 이를 렌더링하여 응답을 생성한다. ViewResolver는 단순히 View를 찾아주는 역할 실제 렌더링은 View 객체가 수행한다. @RestController 어노테이션으로 등록된 메서드가 반환한 객체는 DispatcherServlet → HandlerAdapter → HttpMessageConverter를 통해 JSON을 생성한 뒤 DispatcherServlet에게 전달한다. 직렬화 처리를 위임한다. @Controller, @RestController 케이스의 응답 반환 @Controller의 경우 ViewResolver는 View 이름에 해당하는 View 객체를 찾는 역할만 하고 해당 View 객체가 DispatcherServlet의 호출에 의해 실제 HTML을 렌더링해 클라이언트에 전달한다. ViewResolver → View 객체 탐색 View → HTML 렌더링 @RestController의 경우 HttpMessageConverter는 객체를 JSON으로 직렬화만 하고 직렬화된 결과는 DispatcherServelet을 통해 클라이언트에게 반환된다. DispatcherServlet이 응답의 시작과 끝을 담당하며 HttpMessageConverter는 응답 본문 생성을 돕는다. @Controller 전통적인 MVC 구조에서 사용 Model에 데이터를 담고 View 이름을 반환 View Resolver가 View 템플릿을 렌더링하여 응답 1 2 3 4 5 6 7 8 9 @Controller public class HelloController { @GetMapping(\u0026#34;/hello\u0026#34;) public String hello(Model model) { model.addAttribute(\u0026#34;name\u0026#34;, \u0026#34;Nine\u0026#34;); return \u0026#34;hello\u0026#34;; // templates/hello.html } } JSON 응답을 위해 @ResponseBody와 함께 사용 1 2 3 4 5 6 7 8 9 @Controller public class ApiController { @ResponseBody @GetMapping(\u0026#34;/api/hello\u0026#34;) public String apiHello() { return \u0026#34;Hello from Controller!\u0026#34;; } } @RestController RESTful 웹 서비스에서 주로 사용 @Controller + @ResponseBody의 조합 리턴값을 그대로 JSON 등으로 직렬화하여 Response Body로 전달 1 2 3 4 5 6 7 8 @RestController public class HelloRestController { @GetMapping(\u0026#34;/hello\u0026#34;) public String hello() { return \u0026#34;Hello, Nine!\u0026#34;; } } 차이점 비교 구분 항목 @Controller @RestController 리턴 타입 View 이름 또는 ModelAndView 객체 / ResponseEntity 직접 직렬화한 데이터 객체 또는 문자열 (자동 JSON 직렬화됨) 사용 목적 템플릿 렌더링 기반 웹 앱 (HTML) / 필요시 JSON 응답도 가능 API 응답을 위한 JSON/데이터 전송용 응답 처리 ViewResolver를 통해 HTML 렌더링 HttpMessageConverter를 통해 JSON 직렬화 조합 어노테이션 @Controller, @ResponseBody 필요 @RestController 단독 사용 가능 🎯결론 @Controller는 View 기반 응답을 기본으로 하지만 @ResponseBody나 ResponseEntity를 사용하면 JSON 데이터도 응답할 수 있다. @RestController는 모든 메서드가 데이터(JSON 등) 응답을 기본으로 하며, API 서버 구현에 적합하다. Spring MVC에서 클라이언트 요청을 처리하는 흐름을 이해하면, 상황에 맞는 어노테이션 선택이 쉬워진다. View 기반 웹 페이지와 RESTful API를 구분하여 설계하는 것이 핵심이다.\n⚙️EndNote 사전 지식 MVC 디자인 패턴 Servlet과 DispatcherServlet의 역할 View Resolver, HttpMessageConverter 작동 원리 더 알아보기 Spring MVC Architecture @RestController Javadoc Baeldung - Controller vs RestController Spring 공식 가이드 - RESTful Web Service ","date":"2025-05-18T15:20:00+09:00","permalink":"https://blog.b9f1.com/p/2025-05-18-spring-mvc-controller-and-restcontroller/","title":"Spring MVC"},{"content":"📌개요 Spring에서 OOP(Object Oriented Programming) 만큼이나 강조되는 AOP(Aspect Oriented Programming)은 언제, 왜 필요할까?\n또한 AOP와 OOP는 대립되는 개념이 아니라는 것. 서로를 보완하며 강력한 설계를 가능케 하는지 알아보자.\n📌내용 왜 AOP가 필요한가? Spring Framework의 핵심 철학은 POJO와 OOP 기반의 설계다. 하지만 서비스가 복잡해질수록 로깅, 트랜잭션, 보안 등 공통 관심사가 여기저기 흩어져 코드를 어지럽힌다.\n예를 들어 다음과 같은 로직을 생각해보자.\n1 2 3 4 5 6 7 @Transactional public void createUser(UserRequest request) { log.info(\u0026#34;사용자 생성 시작\u0026#34;); validate(request); userRepository.save(request.toEntity()); log.info(\u0026#34;사용자 생성 완료\u0026#34;); } 이 메서드는 핵심 기능 외에도 트랜잭션 처리, 로깅이라는 횡단 관심사(cross-cutting concerns) 를 포함하고 있다.\n이러한 코드가 여러 메서드에 중복되면 유지보수와 확장성이 심각하게 저하된다. AOP는 이런 문제를 해결하기 위해 등장했다.\n로그인 감사 로깅 1 2 3 4 5 6 7 8 9 10 11 12 @Aspect @Component public class LoginLoggingAspect { @Around(\u0026#34;@annotation(com.example.annotation.Auditable)\u0026#34;) public Object logLogin(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); Object result = joinPoint.proceed(); log.info(\u0026#34;[AUDIT] {} executed in {}ms\u0026#34;, joinPoint.getSignature(), System.currentTimeMillis() - start); return result; } } 사용자 정의 어노테이션 @Auditable을 붙인 메서드에 감사 로깅이 적용된다. 핵심 로직과 분리되어 있기 때문에 유지보수가 쉬워지고 로직이 더 깔끔해진다.\nAOP VS OOP AOP와 OOP는 결코 대립하지 않는다. 오히려 상호 보완적이다.\nOOP는 모듈화를 통해 도메인 개념을 캡슐화한다. AOP는 횡단 관심사를 모듈화하여 코드 중복을 제거한다. AOP는 OOP만으로는 분리하기 어려운 공통 로직을 흩어지지 않게 모듈화하는 방식이다. 즉, AOP는 OOP의 약점을 보완한다.\n💡 재미있는 일화 Spring 초창기에는 AOP가 XML 설정 기반으로 매우 복잡했지만 지금은 @Aspect 어노테이션만으로 매우 간단하게 적용할 수 있다.\nJPA 트랜잭션 설정 없이 @Transactional 하나만 붙였는데도 실제로 프록시 기반으로 트랜잭션이 관리된다는 사실을 알게 되었을 때, 마치 \u0026ldquo;마법 같다\u0026quot;는 반응이 많다.\nSpring AOP의 초기 XML 설정의 복잡성 Spring AOP의 초기 버전에서는 XML을 통해 AOP 설정을 해야 했다. 예를 들어 DigitalOcean의 튜토리얼 - By Pankaj Kumar에서는 XML 기반 AOP 설정의 예시를 다음과 같이 보여준다.\n이러한 설정은 복잡하고 유지보수가 어려웠겠지.\n1 2 3 4 5 6 \u0026lt;aop:config\u0026gt; \u0026lt;aop:aspect ref=\u0026#34;employeeXMLConfigAspect\u0026#34; id=\u0026#34;employeeXMLConfigAspectID\u0026#34; order=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;aop:pointcut expression=\u0026#34;execution(* com.journaldev.spring.model.Employee.getName())\u0026#34; id=\u0026#34;getNamePointcut\u0026#34;/\u0026gt; \u0026lt;aop:around method=\u0026#34;employeeAroundAdvice\u0026#34; pointcut-ref=\u0026#34;getNamePointcut\u0026#34; arg-names=\u0026#34;proceedingJoinPoint\u0026#34;/\u0026gt; \u0026lt;/aop:aspect\u0026gt; \u0026lt;/aop:config\u0026gt; @Transactional 어노테이션과 프록시 기반 트랜잭션 관리의 \u0026ldquo;마법\u0026rdquo; @Transactional 어노테이션을 사용하면 개발자는 명시적으로 트랜잭션을 시작하거나 종료하는 코드를 작성하지 않아도 된다.\nSpring은 내부적으로 프록시를 생성하여 트랜잭션을 관리한다. 이러한 동작은 개발자에게는 마치 \u0026ldquo;마법\u0026quot;처럼 느껴질 수 있다.\nMarco Behler의 블로그에서는 이러한 동작을 다음과 같이 설명한다.\nQuote \u0026ldquo;Now whenever you are using @Transactional on a bean, Spring uses a tiny trick. It does not just instantiate a UserService, but also a transactional proxy of that UserService.\u0026rdquo;\n\u0026ldquo;이제 빈에서 @Transactional을 사용할 때마다 Spring은 작은 트릭을 사용합니다. 이는 단순히 사용자 서비스를 인스턴스화하는 것뿐만 아니라 해당 사용자 서비스의 트랜잭션 프록시도 사용합니다.\u0026rdquo;\nStack Overflow의 질문: Spring - @Transactional - What happens in background?에서도 이와 유사한 설명을 확인할 수 있었다. 무려 15년 전 질문이다.\nQuote \u0026ldquo;But at a very high level, Spring creates proxies for classes that declare @Transactional on the class itself or on members. The proxy is mostly invisible at runtime. It provides a way for Spring to inject behaviors before, after, or around method calls into the object being proxied.\u0026rdquo;\n\u0026ldquo;매우 높은 수준에서 Spring은 클래스 자체 또는 멤버에 @Transactional을 선언하는 클래스에 대한 프록시를 생성합니다. 프록시는 런타임에 대부분 보이지 않습니다. 이 프록시는 Spring이 메서드 호출 전후 또는 주변의 동작을 프록시 대상 객체에 주입할 수 있는 방법을 제공합니다.\u0026rdquo;\n🎯결론 OOP는 설계의 뼈대, AOP는 설계의 윤활유다. AOP는 복잡한 애플리케이션에서 반드시 필요한 설계 전략이다.\n⚙️EndNote 사전 지식 Java 언어 및 Spring Framework 기초 DI(의존성 주입)과 Proxy 패턴 개념 어노테이션 기반 설정 이해 더 알아보기 Spring AOP 공식 문서 AspectJ 소개 핵심 키워드: @Aspect, @Around, JoinPoint, cross-cutting concern, Proxy, Advice, Pointcut ","date":"2025-05-18T15:20:00+09:00","permalink":"https://blog.b9f1.com/p/2025-05-18-spring-aop-and-oop/","title":"Spring에서 AOP란"},{"content":"📌개요 Jekyll은 간단하게 정적 블로그를 운영할 수 있는 훌륭한 도구다. 하지만 블로그 글이 많아질 수록 플러그인이 늘어날 수록 빌드 시간이 길어지는 문제가 생긴다.\n배포 빌드에선 풀 빌드해야 하니 다른 전략에서 다루고 일단 로컬부터 빌드 최적화 방법을 정리해본다.\n📌내용 --incremental 옵션 활용 로컬에서 테스트할 때 가장 손쉽게 적용할 수 있는 옵션\n1 bundle exec jekyll s --incremental 이 옵션은 마지막 빌드 후 변경된 파일만 다시 빌드해줘서 속도를 꽤 개선할 수 있다. 하지만 주의할 점은 새 파일 추가는 감지하지 못한다는 점이다. 그래서 새 포스트나 페이지를 추가한 경우엔 다시 풀 빌드를 돌려야 한다.\n--limit_posts 하나의 포스트만 빌드하기 parse \u0026amp; publish할 포스트의 개수를 제한해서 필요한 파일만 빠르게 빌드하고 확인한다. 최근 포스팅을 우선으로 빌드 되는 것 같다.\n1 2 # bundle exec jekyll s --limit_posts \u0026lt;포스팅 수\u0026gt; bundle exec jekyll s --limit_posts 1 --profile 옵션으로 병목 찾기 어떤 파일이나 플러그인이 빌드 속도를 잡아먹는지 알고 싶다면 이 옵션을 사용해본다.\n1 bundle exec jekyll s --profile 빌드 로그에 각각 파일의 처리 시간이 나오는데 유난히 시간이 많이 걸리는 파일이나 플러그인이 있으면 구조를 개선하거나 비활성화하는 방법을 찾을 수있다.\n--jekyll clean으로 캐시 초기화 증분 빌드를 계쏙 쓰다 보면 캐시 문제로 꼬이는 경우가 생긴다. 이때는 다음 명령어로 캐시를 초기화하면 깔끔하다.\n1 bundle exec jekyll clean 매번 서버를 새로 켜기 전에 이걸 실행하면 문제 해결에 도움이 된다.\nexclude와 keep_files로 불필요한 파일 제외 _config.yml 파일에 다음 옵션을 추가해 빌드에서 제외할 파일을 명확히 설정하는 것도 중요하다.\n1 2 3 4 5 6 7 8 9 exclude: - node_modules - README.md - Gemfile - vendor keep_files: - .git - .svn 특히 node_modules나 대용량 폴더는 반드시 제외해야 한다.\n디렉터리 구조 개선 컬렉션이나 커스텀 폴더를 많이 사용하고 있다면 구조를 간결하게 재정비하는 것도 도움이 된다. 복잡한 디렉터리 구조는 Jekyll의 내부 처리 속도를 저하시킬 수 있다.\n플러그인 최소화 및 최적화 jekyll-feed, jekyll-seo-tag 등 기본 플러그인은 큰 문제가 없지만 커스텀 플러그인이 많아질 수록 속도가 느려진다.\n불필요한 플러그인은 과감히 제거하고 꼭 필요하다면 코드를 최적화하거나 대안을 찾아보자.\n이미지 최적화 이미지가 많다면 이미지 크기 최적화만으로도 빌드 속도가 향상된다. 필요하다면 이미지 자체를 외부 CDN으로 분리하는 것도 한 방법이다.\n🎯결론 Jekyll은 심플함이 강점인 만큼 대규모 데이터엔 한계가 있다. 하지만 위 방법만 잘 적용해도 중형 블로그까지는 무리 없이 빠른 속도로 유지할 수 있다.\n규모가 커진다면 다른 대체제를 고려해보자.\n⚙️EndNote 참고 자료 Jekyll의 빌드 시간을 최적화하는 방법 Jekyll 블로그 빌드속도 개선하기 ","date":"2025-05-06T15:11:17+09:00","permalink":"https://blog.b9f1.com/p/2025-05-06-jekyll-improving-build-speed/","title":"Jekyll 블로그 빌드 속도 개선"},{"content":"📌개요 Spring Boot는 Spring에 비해 설정을 따로 해주지 않아도 자동으로 설정해주는 게 많아서 더 간편하다. 이전에 Spring과 Spring Boot을 비교하며 자동 설정 기능에 대해 간단히 알아봤다.\n이번엔 Spring Boot의 자동 설정(AutoConfiguration) 메커니즘을 이해하고 커스텀 설정을 구성할 수 있는지 알아보자.\n📌내용 Spring Boot의 @SpringBootApplication 어노테이션은 내부적으로 @EnableAutoConfiguration, @ComponentScan, @Configuration을 조합한 메타 어노테이션(복합 구성)이다.\ngraph TD A[\"@SpringBootApplication\"] A --\u003e B[\"@ComponentScan\"] A --\u003e C[\"@SpringBootConfiguration\"] C --\u003e C1[\"@Configuration\"] C1 --\u003e C2[\"@Component\"] A --\u003e D[\"@EnableAutoConfiguration\"] D --\u003e D1[\"@Import\"] D --\u003e D2[\"@AutoConfigurationPackage\"] D2 --\u003e D3[\"@Import\"] Auto-configuration 은 Spring Boot의 핵심 기능 중 하나로, 클래스패스에 있는 라이브러리와 환경 설정에 따라 적절한 Bean을 자동으로 구성해주는 메커니즘이다.\n자세히 알아보기 앞서 용어를 정리하고 내려가보자.\n자동 설정(Auto-configuration) Spring Boot가 클래스 패스와 설정에 따라 자동으로 Bean 등을 구성하는 기능 자동 설정 클래스(auto-configuration class) 자동 설정을 수행하는 실제 Java 클래스 내부적으로 조건부 어노테이션(@ConditionalOn, \u0026hellip;)으로 구성된다. @EnableAutoConfiguration 자동 설정 기능을 활성화하는 트리거 일반적으로 @SpringBootApplication 안에 포함되어 있다. @AutoConfiguration (Spring Boot 3.0 이상) 이전의 @Configuration + spring.factories 조합을 대체하는 새 방식. 이 어노테이션을 붙인 클래스는 자동 설정 후보로 간주된다. @Configuration Java 기반 설정을 정의하는 기본 Spring 어노테이션 @AutoConfiguration은 이를 확장한 것. Spring Boot 자동 설정 매커니즘 자세히 이해하기 자동 설정이란? 자동 설정(Auto-Configuration)은 Spring Boot가 클래스패스에 있는 라이브러리, 설정값, Bean 존재 여부 등을 기반으로 적절한 Bean 설정을 자동으로 구성해주는 기능이다.\n예를 들어 spring-boot-starter-data-jpa를 추가하면 자동으로 DataSource, EntityManagerFactory, TransactionManager 같은 Bean이 구성된다.\nSpring Boot가 내부에서 이를 위한 자동 설정 클래스를 탐지하고 적용하기 때문이다.\n@SpringBootApplication 실행 흐름 시각화 flowchart TD A[\"public static void main\"] --\u003e B[\"SpringApplication.run(...)\"] B --\u003e C[\"@SpringBootApplication\"] C --\u003e D_sub D_sub --\u003e E[\"ApplicationContext 초기화\"] E --\u003e F[\"내장 Tomcat 등 서버 구동\"] F --\u003e G[\"클라이언트 요청 대기 상태 진입\"] subgraph D_sub [구성 어노테이션] DA[\"@EnableAutoConfiguration\"] DB[\"@ComponentScan\"] DC[\"@SpringBootConfiguration\"] end main() : 애플리케이션의 단순한 진입점이 아니라 Spring Boot 전체 실행의 기준점 SpringApplication.run() : 실행 컨텍스트 구성, 리스너 등록, 설정 파일 로딩 등 수행 @EnableAutoConfiguration : 클래스패스를 기반으로 설정 후보 자동 로딩 및 적용 @ComponentScan : 지정된 패키지에서 컴포넌트(@Component, @Service 등) 자동 탐색 @SpringBootConfiguration : Java 기반 설정 클래스 등록. 수동 Bean 정의에 사용 ApplicationContext 초기화 : 의존성 주입 및 라이프사이클 콜백 수행 내장 서버 실행 : 기본 포트(8080)에서 HTTP 요청을 수신할 수 있는 상태로 전환됨 이 흐름은 단순히 외워야 할 절차가 아니라, 설정 누락, 컴포넌트 인식 실패, 자동 설정 충돌 등의 문제를 디버깅할 때 반드시 이해하고 있어야 하는 구조이다.\n자동 설정을 활성화하려면? 공식 문서 명시 You need to opt-in to auto-configuration by adding the @EnableAutoConfiguration or @SpringBootApplication annotations to one of your @Configuration classes.\n즉, 자동 설정은 opt-in 기반이다. 자동 설정은 \u0026ldquo;설정이 필요 없는 간편함\u0026quot;을 제공하지만, \u0026ldquo;조건\u0026rdquo; 기반이라는 점을 반드시 이해해야 한다.\nOpt-in/Opt-out Opt-in: 기본적으로 비활성화, 직접 명시적으로 활성화해야 동작하는 방식. Opt-out: 기본이 활성화, 원하면 명시적인 비활성화 처리 필요. 1 2 3 4 5 6 @SpringBootApplication // 내부에 @EnableAutoConfiguration 포함 public class MyApp { public static void main(String[] args) { SpringApplication.run(MyApp.class, args); } } 내부 구성 1 2 3 4 5 6 7 8 9 10 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { ... @EnableAutoConfiguration의 동작 원리 핵심은 @Import(AutoConfigurationImportSelector.class) 이 클래스가 자동 설정 클래스들을 로드해온다는 것이다.\n@EnableAutoConfiguration은 단순 트리거이고 실제로 자동 설정 클래스를 로드하고 필터링하는 핵심 로직은 바로 AutoConfigurationImportSelector 클래스에 들어 있다.\n이 클래스는 Spring-boot-autoconfigure.jar에 포함되어 있으므로 Spring Boot starter를 프로젝트에 추가하면 자동으로 로드 가능한 구조다.\n1 2 3 4 5 6 7 8 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import(AutoConfigurationImportSelector.class) public @interface EnableAutoConfiguration { ... ImportSelector가 내부적으로 다음 파일들을 스캔한다.\nSpring Boot 2.x META-INF/spring.factories Spring Boot 3.x 이상 META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports 자동 설정 클래스는 어떤 형태인가? 자동 설정 클래스는 @Configuration 또는 @AutoConfiguration으로 선언된 설정 클래스 형태를 갖는다.\n내부에 @ConditionalOnClass, @ConditionalOnMissingBean, @ConditionalOnProperty 등의 조건부 어노테이션을 사용하여 상황에 맞게 설정을 자동 적용한다.\nJacksonAutoConfiguration 예시 1 2 3 4 5 6 7 8 9 10 @AutoConfiguration @ConditionalOnClass(ObjectMapper.class) public class JacksonAutoConfiguration { @Bean @ConditionalOnMissingBean public ObjectMapper objectMapper() { return new ObjectMapper(); } } 자동 설정 클래스는 일반적으로 다음 조건부 어노테이션을 사용한다. 자동 설정 클래스는 조건에 따라 설정을 \u0026ldquo;선택적으로 적용\u0026quot;하기 위한 전략 집합이다.\n어노테이션 설명 @ConditionalOnClass 특정 클래스가 클래스패스에 존재할 때만 활성화 @ConditionalOnMissingBean 같은 타입의 Bean이 없을 때만 등록 @ConditionalOnProperty application.yml에 설정된 프로퍼티가 조건과 일치할 때만 활성화 @ConditionalOnWebApplication 웹 애플리케이션일 때만 활성화 등 자동 설정이 실제로 등록되었는지 확인하는 방법 Spring Boot Actuator + 조건 리포트\nbuild.gradle 또는 pom.xml에 actuator 추가 1 2 // build.gradle implementation \u0026#39;org.springframework.boot:spring-boot-starter-actuator\u0026#39; application.yaml에 아래와 같이 설정 1 2 3 4 5 management: endpoints: web: exposure: include: conditions 브라우저에서 http://localhost:8080/actuator/conditions로 접속 후 어떤 자동 설정 클래스가 적용(positiveMatches)됐고 어떤 조건이 왜 실패했는지(negativeMatches) 확인 가능. IntelliJ IDE에서 제공하는 패널을 확인하는 방법도 있다. Spring Boot Dashboard 또는 View \u0026gt; Tool Windows \u0026gt; Spring Beans, Mappings, Endpoints 등으로 전체 컨텍스트 확인 가능 커스텀 AutoConfiguration 테스트 작성한 HelloAutoConfiguration과 HelloService가 자동으로 빈 등록되는지 실제 확인하려면,\n다음과 같은 방식으로 별도 테스트 프로젝트를 만들어 사용하는 것이 가장 확실하다.\n/actuator/conditions에서 HelloAutoConfiguration 자동 설정 작동 여부 확인 context.containsBean(\u0026quot;helloService\u0026quot;) or @Autowired HelloService Bean 등록 확인 @ConditionalOnMissingBean, @ConditionalOnProperty 등 추가 후 조건 적용 확인 테스트 프로젝트 구성 방식 AutoConfiguration 모듈 (라이브러리 역할) Gradle 또는 Maven으로 별도 모듈로 분리하여 사용할 프로젝트.\nhello-autoconfig 프로젝트를 생성해서 모듈을 jar로 빌드하거나 settings.gradle에 includeBuild(...) 방식으로 연결하여 테스트할 예정이다.\nJava + Gradle로 빠르게 프로젝트 생성 모듈은 Spring Boot로 만들지 않아도 된다는 점 1 2 3 4 5 6 hello-autoconfig/ ├── src/ │ ├── main/java/com/example/autoconfig/HelloService.java │ └── main/java/com/example/autoconfig/HelloAutoConfiguration.java │ └── main/resources/META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports ├── build.gradle build.gradle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 plugins { id \u0026#39;java-library\u0026#39; } group = \u0026#39;com.example\u0026#39; version = \u0026#39;1.0-SNAPSHOT\u0026#39; repositories { mavenCentral() } dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-autoconfigure:3.1.5\u0026#39; // 버전은 호환을 고려해야 한다. } test { useJUnitPlatform() } HelloService.java 1 2 3 4 5 6 7 package com.example.autoconfig; public class HelloService { public String sayHello() { return \u0026#34;Hello from AutoConfiguration!\u0026#34;; } } HelloAutoConfiguration.java 1 2 3 4 5 6 7 8 9 10 11 12 13 package com.example.autoconfig; import org.springframework.boot.autoconfigure.AutoConfiguration; import org.springframework.context.annotation.Bean; @AutoConfiguration public class HelloAutoConfiguration { @Bean public HelloService helloService() { return new HelloService(); } } src/main/resources/META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports 1 com.example.autoconfig.HelloAutoConfiguration 프로젝트 생성과 클래스 작성이 완료됐다면 모듈로 사용하기 위해 jar 파일로 빌드한다.\n1 2 ./gradlew clean build # `hello-autoconfig-1.0-SNAPSHOT.jar` 생성됨 테스트용 실제 앱 프로젝트에서 사용 Spring Boot 프로젝트(initializr 등) 생성\nmy-app 프로젝트는 위에서 만든 모듈을 연결해 테스트하는 용도다.\nbuild.gradle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 plugins { id \u0026#39;java\u0026#39; id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;3.1.5\u0026#39; // 버전은 호환을 고려해야 한다. id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.1.3\u0026#39; } group = \u0026#39;com.example\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; java { toolchain { languageVersion = JavaLanguageVersion.of(17) } } repositories { mavenCentral() flatDir { dirs \u0026#39;../hello-autoconfig/build/libs\u0026#39; // jar 경로 지정 (지금은 테스트 용도로 같은 경로에 두 프젝을 만들었으니.) } } dependencies { implementation name: \u0026#39;hello-autoconfig-1.0-SNAPSHOT\u0026#39; // .jar 이름 그대로 implementation \u0026#39;org.springframework.boot:spring-boot-starter\u0026#39; } tasks.named(\u0026#39;test\u0026#39;) { useJUnitPlatform() } DemoApplication.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package com.example; import com.example.autoconfig.HelloService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import jakarta.annotation.PostConstruct; @SpringBootApplication public class DemoApplication { @Autowired private HelloService helloService; public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } @PostConstruct public void printHello() { System.out.println(helloService.sayHello()); } } 실행해보면 Hello from AutoConfiguration! 메시지를 잘 확인할 수 있다.\n🎯결론 자동 설정의 작동 원리, 조건부 어노테이션, 확인 방법 등을 알아봤다.\nSpring Boot의 자동 설정은 \u0026ldquo;있는지 보고 알아서 설정\u0026quot;하는 전략이다. 우리가 할 일은 조건과 우선 순위를 명확히 하여 Spring의 똑똑한 판단을 유도하는 것이다.\n자동 설정의 아키텍처와 커스텀 구성 전략까지 이해하고 설계 수준에서 응용이 가능한지 다음 기회에 더 알아보자.\n⚙️EndNote 사전 지식 Java 어노테이션 이해 Spring @Configuration 및 Bean 등록 방식 더 알아보기 Spring Boot 공식 문서 - 나만의 자동 구성 만들기 spring-boot-autoconfigure GitHub 소스코드 ","date":"2025-05-06T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-05-06-springboot-autoconfiguration-mechanism/","title":"Spring Boot 자동 설정"},{"content":"📌개요 VSCode에서 PR 보려는데 복잡시럽다 복잡시러. IntelliJ Ultimate/Community에선 GitHub 플러그인으로 손쉽게 확인이 가능하다.\n📌내용 IDE에서 PR을 확인하면 좋은 점 코드 리뷰에 대한 코멘트를 바로 파일의 라인에서 확인할 수 있다. 의견이 있어서 Reply를 남기거나 수정 완료를 곧바로 처리할 수 있다.\nGitHub Pull Requests 현재 기준 GitHub Pull Requests를 설치한다. VSCode marketplace - GitHub Pull Requests 예전 이름은 GitHub Pull Requests and Issues였나 보다. 기능은 같아서 PR, Issues를 확인할 수 있다.\n근데 이게 문제였다 확장 설치하면 열려 있는 PR만 확인이 됐다. 필요한 다른 항목은 쿼리를 수정해줘야 한다.\nCtrl + Shift + P 명령 팔레트\n1 2 3 4 \u0026gt;GitHub Pull Requests: Configure # 엔터 치면 두 개 나온다 # Configure Remotes... # Configure Queries... 나는 닫힌 PR을 확인해야 해서 추가했다.\n1 2 3 4 5 6 7 8 9 10 11 // settings.json { // ... others \u0026#34;githubPullRequests.queries\u0026#34;: [ // ... others { \u0026#34;label\u0026#34;: \u0026#34;My Closed PRs\u0026#34;, \u0026#34;query\u0026#34;: \u0026#34;is:closed assignee:${user}\u0026#34; } ] } 필요에 맞게 수정 후 새로고침하면 잘 확인된다.\n","date":"2025-05-06T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-05-06-vscode-github-pull-requests/","title":"VSCode에서 PR 보기"},{"content":"📌개요 Java 예외 처리는 프로그램의 안정성, 복원력, 유지보수성을 결정 짓는 핵심 설계 요소다.\n예외는 크게 다음 세 가지로 구분된다.\nChecked Exception: 외부 자원 접근 등 복구 가능한 예외 상황을 명시적으로 선언 (컴파일 타임에 처리) Unchecked Exception: 실행 중 발생하는 로직 오류, 주로 프로그래머의 실수에서 기인, 명시적 처리 강제가 없다. Error: OutOfMemory, StackOverflowError와 같은 시스템 레벨의 치명적 오류 특히 Checked, Unchecked Exception 간의 선택은 단순한 문법 차원이 아니라 설계 전략과 깊은 연관이 있다.\n📌내용 Java 예외 계층 구조 해당 계보도가 전부는 아니며 필요한 부분만 그렸다는 점을 참고하자.\nclassDiagram Object \u003c|-- Throwable Throwable \u003c|-- Exception Throwable \u003c|-- Error %% === Unchecked Exceptions Exception \u003c|-- RuntimeException RuntimeException \u003c|-- NullPointerException RuntimeException \u003c|-- IllegalArgumentException RuntimeException \u003c|-- ArithmeticException RuntimeException \u003c|-- IllegalStateException RuntimeException \u003c|-- UnsupportedOperationException %% === Checked Exceptions Exception ..\u003e IOException : checked Exception ..\u003e SQLException : checked IOException ..\u003e FileNotFoundException : checked %% === Errors Error \u003c|-- OutOfMemoryError Error \u003c|-- StackOverflowError Error \u003c|-- LinkageError Error \u003c|-- VirtualMachineError Checked Exception RuntimeException을 상속하지 않는 예외 컴파일 시점에 강제적으로 처리 필요(try/catch 또는 throws) 복구 가능성이 있는 외부 자원 오류 등 예시 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import java.io.*; import java.util.*; public class CheckedExceptionExample { public static void main(String[] args) { try { Scanner scanner = new Scanner(new File(\u0026#34;example.txt\u0026#34;)); while (scanner.hasNextLine()) { System.out.println(scanner.nextLine()); } scanner.close(); } catch (FileNotFoundException e) { System.out.println(\u0026#34;파일을 찾을 수 없습니다: \u0026#34; + e.getMessage()); } } } Unchecked Exception RuntimeException 또는 그 하위 클래스 컴파일러 검사 없음, 개발자 의도에 따라 예외 처리 선택 대부분 개발자의 실수 또는 버그로 발생 1 2 3 4 5 6 7 8 9 10 11 12 public class UncheckedExceptionExample { public static void main(String[] args) { int[] numbers = {1, 2, 3}; int index = 10; try { System.out.println(numbers[index]); } catch (ArrayIndexOutOfBoundsException e) { System.out.println(\u0026#34;잘못된 인덱스 접근: \u0026#34; + e.getMessage()); } } } 언제 어떤 예외를 사용할까? Effective Java \u0026gt; Item 70 복구 가능한 경우에만 Checked Exception을 사용하고 그렇지 않다면 RuntimeException을 사용하라.\n상황 추천 예외 이유 복구 가능한 외부 리소스 오류 Checked IO, DB 등은 사용자 안내 및 재시도 가능 프로그래머 실수나 버그 Unchecked NPE, IllegalArgument 등 빠른 실패 유도 공통된 예외 추상화 Custom Unchecked 도메인 중심 예외 설계에 유리 예외 처리의 진화 흐름 요약 시대/환경 트렌드 주요 배경 및 이유 초기 Java (JDK 1.0~1.4) Checked 중심 설계 - 강력한 컴파일 타임 검사\n- API 사용자의 실수 방지\n- \u0026ldquo;복구 가능한 예외는 명시적으로 처리하라\u0026quot;는 철학 현업 개발자들의 반발 증가 (2000년대 중반) Checked 회의론 - 반복적인 throws 선언으로 코드 노이즈 증가\n- 예외 처리 로직이 무분별해짐\n- 대부분의 클라이언트가 catch 후 무시하거나 로그만 남김 Effective Java (2008~2018) 선택적 사용 권장 - Joshua Bloch: \u0026ldquo;복구 가능한 경우에만 Checked 사용하라\u0026rdquo;\n- RuntimeException 기반 설계 권장\n- 예외는 API 설계의 계약이므로 의미 있게 사용 권장 Spring Framework 대중화 (2005~현재) Unchecked 기반 설계 - 대부분의 Spring 예외는 RuntimeException 기반\n- 선언적 트랜잭션, AOP 등과 결합해 Checked 예외가 불편함\n- 개발자에게 선택적 예외 처리 자유를 부여 모던 자바 개발 \u0026amp; DDD 확산 (2010s~) 도메인 기반 Unchecked 전략 - 예외는 “도메인 규칙 위반”을 표현\n- 예외를 catch하지 말고 전파 후 해석하는 방식 선호\n- API, 도메인, 인프라 계층별로 예외 책임 분리 지금의 흐름 (2020s~) 명확한 계층 전략 + RuntimeException 기반 커스터마이징 - 예외를 try-catch로 무조건 처리하는 시대는 끝남\n- Unchecked 중심이지만 무분별하지 않게 도메인화\n- 예외 메시지는 로깅, 사용자 응답, 추적 가능성까지 고려 예외를 설계 관점에서 본다는 것 예외는 \u0026ldquo;실행 흐름의 탈출권\u0026quot;이다. 예외는 단순한 코드 에러가 아니라 소프트웨어의 책임 구조를 드러내는 설계 도구다. 예외를 어디서 발생시키고 어디서 잡을지는 팀의 기술 철학과 도메인 전략이 반영되어야 한다. 결국 예외는 도메인의 경계를 지키는 마지막 방패 역할을 해야 한다. 누가, 언제, 어떤 책임을 갖고 이 탈출권을 행사해야 할까?\n사용자 입력 오류가 발생했을 때? DB 연결 실패가 발생했을 때? 도메인 규칙을 어겼을 때? ···사람들에게서 잊혀졌을 때다···!!!\n모든 예외를 try-catch로 처리할 수도 있지만 그건 제어 흐름의 책임을 흐트러뜨리고 도메인의 진실을 숨기는 위험한 선택일 수 있다.\n예외 설계의 3요소 요소 설명 책임 주체 예외 발생 (throw) 어디에서, 어떤 상황에서 예외를 발생시킬 것인가 도메인, 인프라 계층 예외 전파 (throws) 예외를 호출자에게 넘길 것인가, wrapping할 것인가 서비스 계층 예외 처리 (catch) 예외를 어디에서 처리하고 사용자에게 어떤 메시지를 줄 것인가 API 계층 (Controller, Filter) 예외는 무조건 처리하는 것이 아니라 올바른 계층에서 해석하는 것 예외를 너무 빨리 처리해버리면 문제를 숨기게 된다.\n예시 코드 1 2 3 4 5 try { orderService.placeOrder(...); } catch (Exception e) { log.warn(\u0026#34;에러 발생\u0026#34;, e); // 그리고 그냥 끗-⭐. } 이렇게 하면 실제로 어떤 문제가 발생했는지 알 수 없고 서비스 로직에서의 책임 분리가 흐려진다. 반대로 모든 예외를 끝까지 전파하면 Controller, API 계층이 도메인의 모든 세부사항을 알아야 한다.\n좋은 예외 설계의 기준 도메인 계층에서는 의미 있는 커스텀 예외를 발생 시킨다. 예: InsufficientBalanceException, InvalidCouponException 서비스 계층에서는 예외를 변환하거나 해석하지 않는다. 가능한 한 도메인 예외 그대로 전파 (또는 기술 예외를 도메인 예외로 래핑) 컨트롤러/외부 인터페이스 계층에서 예외를 해석하고 사용자 메시지로 변환한다. 예: @ControllerAdvice, @ExceptionHandler, HTTP 상태 코드 매핑 예외와 트랜잭션 롤백의 관계 Quote 예외는 단순한 오류가 아니라, 트랜잭션의 커밋 여부를 결정짓는 시그널이다.\nSpring에서는 @Transactional을 사용할 경우, 예외 발생 시 해당 트랜잭션의 롤백 여부는 예외의 종류에 따라 자동으로 결정된다.\n용어 정리부터 용어 설명 비즈니스 예외 사용자의 입력 실수, 제약 조건 위반, 정책 위반 등 \u0026ldquo;업무 규칙을 어긴 것\u0026quot;을 의미함 복구 가능한 예외 catch 후 재시도하거나 사용자 안내를 통해 정상 흐름으로 회복할 수 있는 예외 복구 불가능한 예외 시스템 오류, null 포인터, DB 연결 끊김 등 외부적이거나 근본적으로 해결 불가능한 예외 Spring의 기본 트랜잭션 롤백 규칙 예외 유형 롤백 여부 설명 RuntimeException 계열 롤백 기본적으로 트랜잭션을 롤백한다. Error 계열 롤백 시스템 오류로 간주하고 롤백한다. Checked Exception 커밋 기본적으로 롤백하지 않고 커밋된다. Quote In its default configuration, the Spring Framework’s transaction infrastructure code marks a transaction for rollback only in the case of runtime, unchecked exceptions. That is, when the thrown exception is an instance or subclass of RuntimeException. (Error instances also, by default, result in a rollback). Checked exceptions that are thrown from a transactional method do not result in a rollback in the default configuration. You can configure exactly which Exception types mark a transaction for rollback, including checked exceptions by specifying rollback rules.\n\u0026ldquo;기본 설정에서, Spring Framework의 트랜잭션 인프라 코드는 런타임, 언체크 예외의 경우에만 트랜잭션을 롤백하도록 표시합니다. 즉, 발생한 예외가 RuntimeException의 인스턴스이거나 하위 클래스인 경우입니다. (Error 인스턴스도 기본적으로 롤백을 유발합니다). 체크 예외가 트랜잭션 메서드에서 발생하면 기본 설정에서는 롤백되지 않습니다. 롤백 규칙을 지정하여 확인된 예외를 포함하여 어떤 예외 유형이 트랜잭션을 롤백할지 정확하게 구성할 수 있습니다.\u0026rdquo;\n예시 코드 1 2 3 4 5 6 7 @Transactional public void processPayment(Order order) throws PaymentException { // ... if (!paymentGateway.charge(order)) { throw new PaymentException(\u0026#34;결제 실패\u0026#34;); // Checked Exception } } 위 예시는 기본적으로 트랜잭션이 커밋된다. 하지만 결제 실패 시에도 롤백되길 원할 수 있다.\n롤백 대상 명시 방법 rollbackFor 속성 사용 1 2 3 4 @Transactional(rollbackFor = PaymentException.class) public void processPayment(Order order) throws PaymentException { // ... } noRollbackFor로 반대로 명시도 가능 1 @Transactional(noRollbackFor = ValidationException.class) 실무팁 비즈니스 로직에서 롤백을 유도하고 싶다면 Unchecked Exception을 사용하거나, rollbackFor를 반드시 명시하자. @Transactional은 프록시 기반이므로 public 메서드에서만 작동하며 내부 호출 시 적용되지 않는다는 점도 주의해야 한다. 🎯결론 Spring은 예외의 의미나 복구 가능성에 관계 없이 예외의 타입만을 기준으로 트랜잭션 롤백 여부를 결정한다. 기본적으로 RuntimeException 및 Error 만 롤백 대상이며, 체크 예외는 롤백되지 않는다. 체크 예외에 대해서도 롤백이 필요하다면, @Transactional 애너테이션의 rollbackFor 속성을 사용하여 명시적으로 지정하거나, 언체크 예외로 감싸서 던져야 한다. ⚙️EndNote 사전 지식 Java의 상속 구조 (Object, Throwable, Exception) try-catch-finally 구문 이해 throws와 throw의 차이 Java의 기본 예외 클래스 예시 (NullPointerException, IOException 등) 더 알아보기 Effective Java \u0026gt; Item 69~77: 예외 처리의 정석 Spring의 전역 예외 처리 방식: @ControllerAdvice, @ExceptionHandler 도메인 기반 예외 설계: BusinessException, ValidationException, PolicyViolationException DDD에서의 예외 처리 전략: 도메인 규칙 위반 표현, 계층 간 책임 분리 Exception vs Error: JVM에서의 메모리 관리와 예외 차이 로깅과 예외: log.warn, log.error 언제 쓰고, StackTrace는 어떻게 다룰 것인가 참고 자료 자바 예외 처리의 진화: Checked Exception에서 Unchecked Exception으로 [Java] Checked Exception과 Unchecked Exception Rolling Back a Declarative Transaction ","date":"2025-05-05T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-05-05-java-checked-exception-vs-unchecked-exception/","title":"Java 예외 처리의 구조와 진화"},{"content":"📌개요 Spring Framework는 지금껏 사용해야 하니 필요한 만큼만 알아보고 사용할 수 있다면 그만이었다. 그게 프레임워크의 장점이기도 하지만 자세히 알고 다루는 게 당연히 좋을 것이다.\nSpring의 기본 철학과 그게 왜 중요한지 실제 코드에선 어떻게 드러나는지 정리해보고 싶어졌다.\nSpring Framework는 POJO 기반 개발 철학을 중심으로 복잡한 엔터프라이즈 애플리케이션을 단순화 한다. 객체지향 설계 원칙(SRP, SoC, DRY 등) 을 자연스럽게 실현할 수 있도록 설계되어 클린 아키텍처의 이상을 구현한다. 테스트 주도 개발(TDD) 을 내재화하여 단위 테스트 및 통합 테스트가 손쉬운 구조를 지향한다. 전통적인 Java EE와의 차별점은 단순한 기술적 비교를 넘어 철학적 차별화로 설명할 수 있다. 📌내용 POJO 기반 개발 - 순수 자바의 진정한 의미 Spring은 POJO(Plain Old Java Object)라는 개념을 강조한다.\n그냥 평범한 자바 객체가 왜 중요한가.\n알고 보니 그 핵심은 프레임워크에 종속되지 않은 코드 작성이었다. 즉, Spring 없이도 돌아가는 순수 자바 객체를 만들어야 유지보수나 테스트가 더 쉬워진다는 것이다.\nPOJO의 핵심 특징 특별한 인터페이스/상속이 필요 없다. 어노테이션 없이도 돌아가는 단순한 구조 getter/setter만 있는 평범한 자바 객체 자주 강조되는 부분은 비침투성(Non-Invasiveness)인데, 이건 \u0026ldquo;내 비즈니스 로직에 프레임워크 종속적인 코드를 심지 않아도 된다\u0026quot;는 뜻이다.\n정말 중요한 개념 같아서 메모해놨던 문장이다.\nSpring Framework의 핵심 원칙 중 하나는 \u0026lsquo;비침투성\u0026rsquo;입니다. 비즈니스 로직이나 도메인 모델에 프레임워크 종속 클래스를 억지로 넣지 않아도 실행되어야 한다는 의미입니다.\n직접 작성한 간단한 예제도 다시 떠올려 봤다.\n1 2 3 4 5 6 7 8 9 10 11 public class OrderService { private final PaymentProcessor paymentProcessor; public OrderService(PaymentProcessor paymentProcessor) { this.paymentProcessor = paymentProcessor; } public void processOrder(Order order) { paymentProcessor.charge(order); } } Spring 없이도 동작하는 순수 자바코드. 이게 바로 Spring이 지향하는 방식이라는 걸 확실히 알게 됐다.\nPOJO의 재밌는 일화 마틴 파울러 형님의 POJO 소개\n**POJO (Plain Old Java Object)**는 \u0026ldquo;평범한 자바 객체\u0026quot;라는 의미의 약어입니다.\n이 용어는 2000년 9월, 저(마틴 파울러), 레베카 파슨스, 조쉬 매켄지가 한 컨퍼런스에서 발표를 준비하던 중에 만들어졌습니다. 그 발표에서는 비즈니스 로직을 Entity Bean 같은 복잡한 구조 대신, 일반 자바 객체에 담을 때 얻는 여러 이점에 대해 이야기하려고 했습니다. 그런데 우리는 사람들이 왜 그렇게까지 일반 자바 객체 사용을 꺼려하는지 의문이 들었고, 결론적으로 \u0026ldquo;그 객체들이 단지 이름이 멋지지 않기 때문\u0026quot;이라는 데 생각이 모였습니다. 그래서 우리가 이름을 하나 붙여줬고, 그 이름이 아주 잘 퍼져 나가게 된 것이죠.\n객체지향 설게 원칙 Spring의 또 다른 포인트는 객체지향 설계 원칙이었다. 사실 SOLID 원칙 같은 건 교과서적으로만 알고 있었는데 Spring 구조 안에서 그게 자연스럽게 드러난다는 걸 배웠다.\nDRY(Don\u0026rsquo;t Repeat Yourself) 중복 방지를 위해 AOP(관점지향 프로그래밍)를 활용하는 방식이 인상적이었다.\n단순히 기능적으로 쓸 수 있다는 게 아니라 반복되는 패턴을 추상화하고 코드의 관심사를 깔끔히 분리하려는 시도구나.\n예를 들어 트랜잭션 처리, 로깅 같은 건 매번 코드에 쓰지 않고 @Transactional, @Aspect로 공통화 하는 방식의 AOP를 도입하면 횡단 관심사를 코드 밖으로 꺼낼 수 있다.\nSRP(Single Responsibility Principle) 서비스, 컨트롤러, 레포지토리로 책임을 나누는 계층 구조 덕분에 코드가 깔끔하게 분리될 수 있다. \u0026ldquo;변화의 이유가 하나여야 한다\u0026quot;는 SRP 원칙을 확실히 이해하고 넘어간다.\nSoC(Separation of Concerns) DI(의존성 주입), AOP 같은 개념이 결국 각자의 역할에 집중할 수 있게 돕는 도구라는 점도 정리해볼 수 있었다. 예를 들어 @Aspect로 로깅은 별도로 떼어내고 서비스 로직은 비즈니스에만 집중할 수 있게 한다.\n예전에 소켓 통신 오류가 계속 발생하는 상황에 트러블 슈팅을 위해 어떤 방법이 있을까 고민해보고 찾아보다가 Aspect 클래스를 생성해서 메서드 실행은 건드리지 않고 내가 원하는 측정만 추가하는 방식이 필요했는데 그냥 쓰고 이게 되네 했던 순간보다 다시 알아보는 지금 \u0026ldquo;코드의 순수성\u0026quot;과 \u0026ldquo;변화 대응력\u0026quot;을 동시에 높이는 설계적인 해법이라서 신기하고 진짜 큰 가치구나 싶다.\nSpring은 테스트를 위한 구조다 이건 체감이 정말 컸던 부분이다. TDD는 테스트 환경 세팅이 어려운데 Spring은 POJO와 DI 덕분에 이게 정말 단순해진다.\nSpring에서 TDD 특징 컨테이너 없어도 단위 테스트 가능 (POJO 덕분!) 의존성 주입 덕분에 Mocking이 간단 @SpringBootTest로 통합 테스트도 손쉽게 가능 테스트했던 가장 단순 코드 @DisplayName 달아놨던.. 그래서 일관성 없이 어딘 달고 어딘 안 달고\u0026hellip; 메서드명을 한글로 사용해보자! 😊\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Test @DisplayName(\u0026#34;기본 정보로 사용자 생성시 필수 필드가 올바르게 설정되어야 한다\u0026#34;) void createWithBasicInfo() { User user = UserFixture.createValidUser(); assertAll( () -\u0026gt; assertThat(user.getId()).isNotNull(), () -\u0026gt; assertThat(user.getEmail()).isNotNull(), () -\u0026gt; assertThat(user.getName()).isNotNull(), () -\u0026gt; assertThat(user.getPassword()).isNotNull(), () -\u0026gt; assertThat(user.getCreatedAt()).isNotNull(), () -\u0026gt; assertThat(user.getUpdatedAt()).isNotNull(), () -\u0026gt; assertThat(user.getUpdatedAt()).isEqualTo(user.getCreatedAt()), () -\u0026gt; assertThat(user.getChannels()).isEmpty(), () -\u0026gt; assertThat(user.getProfileImageId()).isNull() ); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Test @DisplayName(\u0026#34;유효한 credentials로 로그인 시 UserResponse 반환\u0026#34;) void loginSuccess() { // given when(userRepository.findByNameWithPassword(user.getName(), user.getPassword())).thenReturn( Optional.ofNullable(user)); // when UserResponse response = authService.login(loginRequest); // Then assertThat(response).isNotNull(); assertThat(response) .usingRecursiveComparison() .isEqualTo(toUserResponse(user)); verify(userRepository).findByNameWithPassword(loginRequest.userName(), loginRequest.password()); } RED: 구현이 필요해? 테스트 작성하고 오류를 만나 빨간줄이 내가 뭘 만들어야 하는지 알려주는 게 돼 GREEN: 코드 작성 했어? 테스트를 돌려 로직을 수정하고 필요한 걸 또 만들어 REFACTOR: 개선할 수 있겠는데? 클린 코드로 개선해보자 변경하면서 다시 RED를 만나 TestContext Framework도 정말 유용하다. 트랜잭션 처리나 컨텍스트 초기화 같은 걸 알아서 해주니까 훨씬 간편하게 통합 테스트를 구성할 수 있다.\nTestContext Framework의 주요 기능 컨텍스트 캐싱: 테스트마다 매번 새 컨텍스트를 띄우면 느리니까, 같은 설정이면 재사용해서 속도를 최적화함. DI 지원: 테스트 클래스에 @Autowired 같은 걸 쓸 수 있게 해줌. 트랜잭션 관리: 테스트가 끝난 후 자동으로 DB 롤백 → 깨끗한 상태 유지. 이벤트 리스너/후크: 테스트 실행 전후에 더 복잡한 작업(예: 컨텍스트 초기화)도 가능. 테스트 중심 개발을 유도하는 설계 철학 Spring은 처음부터 테스트 가능한 코드(testable code)를 중심에 두고 설계됐다. 다음과 같은 구조적 특징이 TDD를 촉진시킨다.\nPOJO 기반 클래스 → 별도의 컨테이너 없이도 테스트 가능 DI를 통한 구성 → 객체간 결합도 감소 → 단위 테스트 용이 설정과 환경 분리 → 테스트 대상 코드의 독립성 확보 모듈화된 아키텍처 → 책임과 기능 분리 → 테스트 대상을 명확히 구분 가능 단순히 테스트가 가능한 수준을 넘어서 테스트 주도 개발을 실질적으로 적용하고 유지할 수 있는 환경을 제공한다는 점에서 의미가 크다.\n⚙️EndNote 사전 지식 객체지향 기본 개념 JUnit 기본 사용법 Mockito 사용 경험 더 알아보기 POJO란 무엇인가? 테스트 주도 개발 개념 Spring 공식 문서 ","date":"2025-04-29T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-29-spring-core-philosophy/","title":"Spring 핵심 철학"},{"content":"📌개요 웹 서버와 WAS의 차이점을 명확히 이해하고 Spring Boot의 내장 톰캣이 어떤 역할을 하는지 알아본다. 또한, Spring Boot에서 사용되는 다양한 Bean 등록 방법과 각각의 장단점을 비교 분석한다.\n📌내용 Web Server VS Web Application Server 웹 서버(Web Server) 정적 콘텐츠(HTML, CSS, JS, 이미지 등)을 제공하는 서버 클라이언트의 요청을 받아 파일 시스템의 리소스를 반환 동적 처리 불가능 WAS와 연동 필요 예: Nginx, Apache HTTP Server 등 웹 애플리케이션 서버(Web Application Server) 편하게 \u0026lsquo;와쓰\u0026rsquo;라고 발음하는 경우가 많다. 동적 콘텐츠(비즈니스 로직, DB 연동)를 처리하는 서버 서블릿 컨테이너를 포함해 애플리에킹션 실행 환경 제공 웹 서버의 기능도 일부 포함 가능 (단, 정적 리소스 처리 효율성이 떨어짐) 예: Tomcat, Jetty 등 주요 차이점 구분 웹 서버 WAS 역할 정적 콘텐츠 제공 동적 콘텐츠 처리 예시 Nginx, Apache Tomcat, Jetty 성능 정적 파일 처리 최적화 애플리케이션 로직 실행 보안 리버스 프록시, 로드 밸런싱 세션 관리, 트랜잭션 처리 Spring Boot의 내장 톰캣은 WAS Spring Boot는 내장형 톰캣을 기본으로 제공한다. 별도의 WAS 설치 없이 실행이 가능하단 소리다. spring-boot-starter-web 의존성 추가 시 자동 구성된다. WAS로 동작하지만 정적 리소스도 처리 가능하다. (개발용으로 충분) 프로덕션 환경에서는 Nginx + Spring Boot(Tomcat) 등 조합을 사용하는 것이 좋다. Spring Boot에서 Bean 등록 방법 비교 컴포넌트 스캔 @Component, @Service, Repository, @Controller 등 어노테이션으로 간단히 빈 등록이 가능하다. 명확한 케이스를 나타내는 어노테이션을 사용하는데, 대부분 @Component를 상속 받고 있다.\n장점: 간편한 등록 @Component 및 하위 어노테이션 자동 감지 의존성 자동 주입 @Autowired 단점: 명시적 제어 불가능 모든 빈이 스캔되므로 불필요한 빈이 등록될 수 있다. 커스터마이징이 어렵다. 자바 설정 클래스 @Configuration + @Bean 조합으로 명시적 등록 방법\n1 2 3 4 5 6 7 @Configuration public class AppConfig { @Bean public MyService myService() { return new MyServiceImpl(); } } 장점: 명시적 빈 등록 필요한 빈만 선택적으로 등록 가능 외부 라이브러리 빈 등록에 유용 단점: 수동 등록이 번거로울 수 있다. Import를 이용한 빈 등록 1 2 3 @Configuration @Import({DatabaseConfig.class, SecurityConfig.class}) public class AppConfig {} 장점: 모듈화된 설정 관리 가능 단점: 의존성 관계가 명확하지 않을 수 있다. XML 기반 빈 등록 legacy한 기술이라서 \u0026ldquo;이런 방법이 있구나\u0026rdquo; 정도만 이해하자.\n1 \u0026lt;bean id=\u0026#34;myService\u0026#34; class=\u0026#34;com.example.MyServiceImpl\u0026#34; /\u0026gt; 장점: 레거시 시스템과 호환성 단점: 가독성이 떨어진다. 유지보수가 어렵다. 최신 Spring에 권장되지 않는다. 최적의 방법은? 컴포넌트 스캔: 일반적인 애플리케이션 빈 등록 @Bean 수동 등록: 외부 라이브러리, 커스텀 설정 필요 시 @Import: 설정 분리 시 유용 ⚙️EndNote 사전 지식 Servlet Container: 웹 애플리케이션 실행 환경 예: Tomcat DI(Dependency Injection): Spring의 핵심 개념, 빈 간의 의존성 관리 더 알아보기 Spring 공식 문서 - Embedded Web Servers ","date":"2025-04-28T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-28-springboot-and-web-servers/","title":"Spring Boot와 웹서버"},{"content":"📌개요 DTO는 계층 간 데이터 전송을 위한 객체로 Java 개발에서 빈번히 사용된다.\n기존에는 Class로 DTO를 구현했지만 Java 14부터 등장한 Record를 활용해보면 불변성, 간결성, 명확성을 모두 확보할 수 있다.\n주문 시스템을 예시로 Record를 활용해 DTO를 효과적으로 구현하는 방법을 알아보자.\nDTO: Data Transfer Object 불변성(Immutability): 생성 후 수정 불가 간결성(Simplicity): 반복 코드 자동 생성 명확성(Clarity): 순수 데이터 객체임을 명시 📌내용 DTO란? Data Transfer Object는 다음과 같은 특징을 가진다.\n데이터 전송 전용 객체 비즈니스 로직 없이 순수 데이터만 포함한다. 계층 간 데이터 변환 예: Controller \u0026lt;-\u0026gt; Service 계층 연결 선택적 데이터 노출 민감 정보 필터링 또는 필요한 데이터만 전송 DTO 사용 Flow sequenceDiagram participant Client as 클라이언트 participant Controller as 컨트롤러 participant Service as 서비스 participant Repository as 리포지토리 Client-\u003e\u003eController: Request DTO 전송 Controller-\u003e\u003eService: Record DTO 변환 전달 Service-\u003e\u003eRepository: Entity 변환 저장 Repository-\u003e\u003eService: Entity 반환 Service-\u003e\u003eController: Record DTO 변환 Controller-\u003e\u003eClient: Response DTO 반환 Class DTO VS Record DTO Class DTO (기존 방식) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class ProductDTO { private final Long id; private final String name; private final double price; // 생성자 public ProductDTO(Long id, String name, double price) { this.id = id; this.name = name; this.price = price; } // 수동 작성 필요 public Long getId() { return id; } public String getName() { return name; } public double getPrice() { return price; } // getter, setter, toString(), equals(), hashCode() 등 ... } Record DTO (Java 16+) Record는 Constructor, getter, equals(), hashCode(), toString()을 자동 생성한다. 그 외에도 추가적인 기능을 제공하지만 직접 구현해야 하는 부분도 있다.\n1 public record ProductDTO(Long id, String name, double price) {} Record로 DTO 구현하기 간단히 어떻게 동작하는지 알아본다.\n테스트 프로젝트 구조 1 2 3 4 5 6 7 8 9 10 11 12 ├─main │ ├─java │ │ └─org │ │ └─b9f1 │ │ │ Main.java │ │ │ │ │ ├─dto │ │ │ ProductDTO.java │ │ │ UserDTO.java │ │ │ │ │ └─service │ │ UserService.java UserDTO.java 1 2 3 4 5 6 public record UserDTO( String id, String userName, String email, LocalDateTime createdAt ) {} ProductDTO.java 1 2 3 4 5 6 public record ProductDTO( String id, String name, int price, int stock ) {} UserService.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class UserService { private final Map\u0026lt;String, UserDTO\u0026gt; userStorage = new HashMap\u0026lt;\u0026gt;(); public UserDTO createUser(String userName, String email) { String id = UUID.randomUUID().toString(); UserDTO newUser = new UserDTO( id, userName, email, LocalDateTime.now() ); userStorage.put(id, newUser); return newUser; } public UserDTO getUser(String id) { return userStorage.get(id); } } Main.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class Main { public static void main(String[] args) { // 1. UserService 테스트 UserService userService = new UserService(); UserDTO user = userService.createUser(\u0026#34;길동쓰\u0026#34;, \u0026#34;길동@길동쓰.com\u0026#34;); System.out.println(\u0026#34;생성된 사용자: \u0026#34; + user); // Java Record의 getter는 전통적인 Java Beans 스타일(getXXX()) 과 다르게 동작한다. // user.getEmail() 이게 아니고 필드명과 동일한 메서드를 사용한다. System.out.println(\u0026#34;생성된 사용자 이메일: \u0026#34; + user.email()); // 2. ProductDTO 직접 사용 ProductDTO product = new ProductDTO(\u0026#34;LT1\u0026#34;, \u0026#34;Laptop\u0026#34;, 1000, 10); System.out.println(\u0026#34;상품 정보: \u0026#34; + product); } } Record DTO의 적절한 사용처 ✅ Record DTO를 사용하면 좋은 경우 성능 팁 Record는 일반 클래스보다 메모리 사용량이 20~30% 적고 생성 속도가 약 15% 빠른 것으로 측정된다고 한다. (JMH 벤치마크 기준)\n간단한 데이터 전송 객체 API 요청/응답 모델 계층 간 데이터 전달 데이터베이스 조회 결과 매핑 값 객체(Value Object) 좌표(Coordinate), 금액(Money) 등 도메인 원시값 래핑 임시 데이터 그룹핑 다중 반환값 처리 1 public record Pair\u0026lt;A, B\u0026gt;(A first, B second) {} ❌ RecordDTO가 부적합한 경우 가변 객체가 필요한 경우 상태 변경이 빈번한 도메인 모델 복잡한 비즈니스 로직 포함 유효성 검사 외 추가 로직이 필요한 경우 상속이 필요한 구조 주의사항 불변성 유지 값 변경이 필요하면 새 객체 생성 1 2 3 4 5 ProductResponse updated = new ProductResponse( original.id(), \u0026#34;새 이름\u0026#34;, // 변경 original.price() ); Jackson 직렬화 기본 지원되지만 커스텀 설정 필요시 @JsonCreator 사용 1 2 3 4 public record ProductRequest(String name, double price) { @JsonCreator public ProductRequest { /* ... */ } } 상속 불가 final 클래스이므로 상속할 수 없음 ⚙️EndNote 사전 지식 Java 16+ (Record 정식 기능) DTO 패턴 불변 객체(Immutable Object) 더 알아보기 Oracle - Record Classes Baeldung - The DTO Pattern (Data Transfer Object) ","date":"2025-04-23T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-23-java-implementing-dto-with-record/","title":"Java Record로 DTO 구현하기"},{"content":"📌개요 초기의 Java EE는 견고하고 강력한 구조를 제공했지만, 개발자들의 손과 발을 묶는 복잡성과 과도한 설정으로 악명 높았다.\n이러한 무거움에서 벗어나기 위해 등장한 것이 Spring Framework.\n\u0026ldquo;제어의 주체가 누구인가\u0026quot;라는 핵심 개념을 중심으로 알아본다.\nSpring Framework가 등장한 배경과 해결하고자 했던 문제점 프레임워크와 라이브러리의 구조적차이 Spring Framework와 일반 Java 라이브러리의 예시 📌내용 Java EE 시대의 불편함 2000년대 초 Java EE(Enterprise Edition)는 대규모 애플리케이션을 위한 표준으로 자리잡고 있었다. 하지만 실무에서 이를 사용하는 개발자들은 다음과 같은 고통을 겪고 있었다.\nEJB(Enterprise JavaBeans)의 과도한 복잡성 무거운 XML 설정과 보일러플레이트 코드 느린 배포 및 테스트 주기 객체 간 의존성 주입이 어렵고 테스트 불가능한 구조 즉, 코드보다 설정이 많고 유연성이 떨어지며 단위 테스트가 거의 불가능한 환경이었다.\nSpring의 등장 2003년 Rod Johnson은 저서 Expert One-on-One J2EE Design and Development에서 Spring Framework의 기반이 되는 아이디어를 처음 공개했다.\n핵심 철학은 간단했다.\n복잡한 EJB 대신 가볍고 유연한 구조로 Java 애플리케이션을 개발하자.\nSpring의 해결책 문제점 Spring의 접근 EJB의 무거움 Plain Old Java Object(POJO) 사용 XML 기반 설정 어노테이션 기반 설정, Java Config 결합도 높은 코드 DI(Dependency Injection)로 느슨한 결합 테스트 어려움 IoC 컨테이너로 유닛 테스트 용이성 확보 Spring은 이러한 철학을 바탕으로 다음 두 가지 설계 원칙을 채택한다.\nIoC(Inversion of Control) 객체의 생명주기와 의존성 관리를 프레임워크가 담당 AOP(Aspect Oriented Programming) 공통 관심사를 분리하여 핵심 로직에 집중 프레임워크와 라이브러리의 결정적 차이 누가 제어권을 갖는가 프레임 워크는 개발자의 코드를 호출하고 라이브러리는 개발자가 라이브러리를 호출한다.\nFramework 개발자가 프레임워크 규칙에 따라 코드를 작성 제어 흐름은 프레임워크가 주도 개발자는 Hook Point만 제공 예: Spring Framework는 개발자가 직접 인스턴스를 생성하지 않고, 스프링 컨테이너가 주도하여 객체 생성 및 주입 1 2 3 4 5 6 7 8 @Service public class UserService { private final UserRepository repository public UserService(UserRepository repository) { this.repository = repository; // Spring이 DI로 주입 } } Library 개발자가 직접 라이브러리를 호출하고 사용하는 방식 제어 흐름의 주도권은 개발자에게 있음 예: Apache Commons, Gson, Log4j 등은 필요할 때 개발자가 직접 호출 1 2 Gson gson = new Gson(); String json = gson.toJson(new User(\u0026#34;Alice\u0026#34;, 25)); Spring VS 일반 Java 라이브러리 항목 Spring Framework Java 라이브러리 예시 @Component, @Autowired, @Transactional Gson, Apache Commons IO, JDBC 제어 흐름 Spring이 주도 (IoC 컨테이너) 개발자가 직접 호출 의존성 관리 자동 주입 (DI) 수동 인스턴스 생성 단위 테스트 Mocking/Bean 주입으로 용이 의존성 분리 어려움 ⚙️EndNote 사전 지식 IoC (Inversion of Control): 객체 생성과 의존성 주입을 프레임워크에 위임 DI (Dependency Injection): 필요한 객체를 외부에서 주입받는 설계 방식 AOP (Aspect-Oriented Programming): 공통 관심사(예: 로깅, 트랜잭션)를 핵심 로직과 분리 더 알아보기 Expert One-on-One J2EE Design and Development - Rod Johnson Spring 공식 문서: https://spring.io/docs ","date":"2025-04-21T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-21-spring-why-was-the-spring-framework-born/","title":"Spring 왜 태어났을까?"},{"content":"📌개요 기본적으로 그냥 설치 후 환경 변수에 psql 등록돼서 버전 확인이 가능했다. 기본 옵션으로 설치해서 DBeaver 역시 어렵지 않게 연결할 수 있었다.\n근데 PostgreSQL이 동작하고 있는 서버의 IP라던가 포트를 확인하려고 하는데 오류가 있었다.\nWindows11, PostgreSQL 17.4 설치 SQL shell은 정상 동작 PostgreSQL 설치 후 Bash, CMD에서 psql 명령어 찾을 수 없음 오류 DBeaver 접속 정상 📌내용 PostgreSQL 설치 확인 설치가 잘 됐는지 Bash, CMD 등에서 버전을 확인해본다.\n1 2 3 4 psql --version # 예상 출력 psql (PostgreSQL) 17.4 PostgreSQL 접속하기 1 psql -U [username] -d [database_name] -h [hostname] -p [port_number] username: PostgreSQL 데이터베이스에 로그인할 사용자 이름이다. 기본적으로 \u0026lsquo;postgres\u0026rsquo;로 설정되어 있다. database_name: 접속할 데이터베이스의 이름이다. 기본값은 \u0026lsquo;postgres\u0026rsquo;이다. hostname: PostgreSQL 서버의 호스트 이름이다. 기본값은 \u0026rsquo;localhost\u0026rsquo;이다. port_number: PostgreSQL 서버의 포트 번호이다. 기본값은 \u0026lsquo;5432\u0026rsquo;이다. 기본 옵션으로 설치한 나의 경우 아래와 같다.\n1 2 3 4 5 6 7 8 9 10 11 psql -U postgres -d postgres -h localhost -p 5432 postgres 사용자의 암호: # 예시 출력 psql (17.4) 도움말을 보려면 \u0026#34;help\u0026#34;를 입력하십시오. # 프롬프트가 변경된다. postgres=# # \\q | Ctrl+C 등으로 빠져나올 수 있다. 사용자의 password 인증을 실패했습니다 명령창에서 psql 접속 후 시도하는 방법도 있지만 PostgreSQL 입문서에 보면 test 데이터베이스에 연결하여 간단한 SQL 쿼리를 실행하는 명령어가 있다.\n근데 시도하면 오류가 발생한다?\n1 2 3 4 psql -d test -c \u0026#34;select inet_server_addr()\u0026#34; \u0026lt;USER NAME\u0026gt; 사용자의 암호: psql: 오류: \u0026#34;localhost\u0026#34; (::1), 5432 포트로 서버 접속 할 수 없음: 치명적오류: 사용자 \u0026#34;\u0026lt;USER NAME\u0026gt;\u0026#34;의 password 인증을 실패했습니다 비밀번호는 틀리지 않았고 Bash, CMD 등에서 테스트 명령만 실패한다.\npg_hba.conf 파일의 인증 메서드를 수정하라는 정보가 많았는데 이건 보안을 바꾸는 거니까 지금 필요한 근본적인 해결책이 아니다.\npostgres 접속은 가능하니까 접속해서 USER 목록을 조회해본다.\n1 2 3 4 5 6 7 8 9 10 11 12 postgres=# SELECT usename FROM pg_user; usename ---------- postgres (1개 행) postgres=# SELECT usename, usesuper, usecreatedb FROM pg_user; usename | usesuper | usecreatedb ----------+----------+------------- postgres | t | t (1개 행) 존재하는 유저는 postgres 뿐인데 테스트 명령에 왜 엉뚱한 사용자명이 나올까?\n이 명령은 명시적으로 -U (user)를 지정하지 않았기 때문에 현재 로그인된 OS 사용자 이름을 PostgreSQL 클라이언트가 그대로 사용하려고 한다.\n확실히 존재하는 postgres 유저로 테스트 명령을 실행하기 위해 -U postgres를 붙여주면 명확하게 postgres 유저로 접속 시도하게 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 psql -U postgres -d test -c \u0026#34;select inet_server_addr()\u0026#34; postgres 사용자의 암호: inet_server_addr ------------------ ::1 (1개 행) psql -U postgres -d test -c \u0026#34;select inet_server_port()\u0026#34; postgres 사용자의 암호: inet_server_port ------------------ 5432 (1개 행) PostgreSQL 설치 PostgreSQL 공식 다운로드 Download 페이지에서 운영체제에 맞게 선택 후 다운로드\n나는 지금 시점 끈따끈따한 17.4 선택.\n0.Welcome 설치 파일을 실행하면 환영해준다. Click Next\n1 2 3 4 5 6 7 [Setup] # 설정 - PostgreSQL Setup - PostgreSQL # PostgreSQL 설치 마법사에 오신 것을 환영합니다. Welcome to the PostgreSQL Setup Wizard. 1.Installation Directory 설치할 경로를 선택한다. 특별히 다른 경로 지정할 필요가 없는 관계로 기본 경로 그대로 설치한다. Click Next\n1 2 3 4 5 6 7 [Setup] # PostgreSQL이 설치될 디렉토리를 지정해 주세요. Please specify the directory where PostgreSQL will be installed. # 설치 디렉토리 [경로 인풋] [경로 선택 버튼] Installation Directory [C:\\Program Files\\PostgreSQL\\17] [choose path button] 2.Select Components 설치 항목을 선택하라고 한다.\nPostgreSQL Server PostgreSQL database server PostgreSQL 데이터베이스 서버 pgAdmin4 pgAdmin4는 PostgreSQL 데이터베이스와 서버를 관리하고 사용하기 위한 그래픽 인터페이스입니다. pgAdmin4 is a graphical interface for managing and working with PostgreSQL databases and servers. Stack Builder Stack Builder는 PostgreSQL 설치를 보완하기 위해 추가 도구, 드라이버 및 애플리케이션을 다운로드하고 설치하는 데 사용할 수 있습니다. Stack Builder may be used to download and install additional tools, drivers and applications to complement your PostgreSQL installation Command Line tools 이 옵션은 libpq, ecpg, pg_basebackup, pg_dump, pg_estore, pg_bench 등과 같은 명령줄 도구와 클라이언트 라이브러리를 설치합니다. 명령줄 도구는 PostgreSQL 데이터베이스 서버 또는 pgAdmin4를 설치할 때 필요한 옵션입니다. This option installs command line tools and client libraries such as libpq, ecpg, pg_basebackup, pg_dump, pg_restore,pg_bench and more. The command line tools are a required option when installing the PostgreSQL Database Server or pgAdmin4. 간단히 필요한 것만 선택. 귀찮으면 다 선택된 상태로 Click Next\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [Setup] # 구성 요소 선택 Select Components # 설치할 구성 요소를 선택합니다: 설치하지 않을 구성 요소를 지웁니다. Select the components you want to install: clear the components you do not want to install. # 계속할 준비가 되면 다음을 클릭합니다 Click Next when you are ready to continue - [x] PostgreSQL Server - [ ] pgAdmin4 - [ ] Stack Builder - [x] Command Line Tools 3.Data Directory Data 경로를 설정한다. 특별히 다른 경로 지정할 필요가 없는 관계로 기본 경로 그대로 설치한다. Click Next\n1 2 3 4 5 6 7 8 9 10 [Setup] # 데이터 디렉토리 Data Directory # 데이터를 저장할 디렉토리를 선택해 주세요. Please select a directory under which to store your data. # 데이터 디렉토리 [경로 인풋] [경로 선택 버튼] Data Directory [C:\\Program Files\\PostgreSQL\\17\\data] [choose path button] 4.Password 데이터베이스 슈퍼사용자(postgres)의 비밀번호를 설정한다. Click Next\n1 2 3 4 5 6 7 8 9 10 11 12 [Setup] # 비밀번호 Password # 데이터베이스 슈퍼사용자(postgres)의 비밀번호를 입력해 주세요. Please provide a password for the database superuser (postgres). # 비밀번호 [비밀번호 입력 인풋] Password [type your password] # 비밀번호 재입력 [비밀번호 입력 인풋] Retype password [type your password] 5.Port 포트를 설정한다. 특별히 변경할 필요 없어서 기본 포트로 사용. Click Next\n1 2 3 4 5 6 7 8 9 10 [Setup] # 포트 Port # 서버가 수신해야 할 포트 번호를 선택해 주세요. Please select the port number the server should listen on. # 포트 [포트 입력 인풋] Port [5432] 6.Advanced Options Locale 설정 시 DEFAULT 값이 기본으로 설정되어 있을텐데 본인이 사용하고 있는 운영체제의 시스템 로케일을 그대로 사용하게 된다.\n한국어 Windows 기준으로는 Korean_Korea.949로 설정되고 한국어 macOS 기준으로는 ko_KR.UTF-8로 설정된다. 이는 PostgreSQL에서 한글 정렬이나 LIKE 검색 시 예기치 않은 동작을 유발할 수 있다.\nAWS와 같은 클라우드 플랫폼을 사용해 배포를 진행하는 경우 실제 서버 운영 환경 배제할 수는 없다. Linux 기반의 서버, AWS RDS, Docker 컨테이너 등에서는 en_US.UTF-8 Locale을 사용하는 점을 고려하자.\n나중에 서버 배포 하고 맞출 때 인코딩 불상사를 보고 싶지 않으면 미리 맞추는 게 좋고, 이미 만들어진 데이터베이스의 로케일은 변경할 수 없고, 새 클러스터를 만들거나 데이터베이스를 새로 생성해야 한다.\n잘 선택한 후 Click Next\n1 2 3 4 5 6 7 8 9 10 [Setup] # 고급 옵션 설정 Advanced Options # 새 데이터베이스 클러스터에서 사용할 지역을 선택합니다. Select the locale to be used by the new database cluster. # 지역 [지역 선택 드롭다운] Locale [en-US] 7.Pre Installation Summary 위 단계에서 선택한 설치 옵션의 요약 정보를 보여준다. Click Next\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [Setup] # 설치 전 요약 Pre Installation Summary # 다음 설정이 설치에 사용됩니다 The following settings will be used for the installation Installation Directory: C:\\Program Files\\PostgreSQL\\17 Server Installation Directory: C:\\Program Files\\PostgreSQL\\17 Data Directory: C:\\Program Files\\PostgreSQL\\17\\data Database Port: 5432 Database Superuser: postgres Operating System Account: NT AUTHORITY\\NetworkService Database Service: postgresql-x64-17 Command Line Tools Installation Directory: C:\\Program Files\\PostgreSQL\\17 Installation Log: C:\\Users\\\u0026lt;UserName\u0026gt;\\AppData\\Local\\Temp\\install-postgresql.log 8.Ready to Install 요약 정보를 확인하고 이상이 없는지 확인 한 번 더 해준다. Click Next\n1 2 3 4 5 6 7 [Setup] # 설치 준비 완료 Ready to Install # 이제 컴퓨터에 PostgreSQL 설치를 시작할 준비가 되었습니다. Setup is now ready to begin installing PostgreSQL on your computer. 9.Installing 설치 진행 바를 확인하며 기다리면 완료된다. 약 10초 내외 (성능 따라 다를 듯)\n1 2 3 4 5 6 7 8 9 10 11 [Setup] # 설치 Installing # 설치 프로그램이 컴퓨터에 PostgreSQL을 설치하는 동안 잠시 기다려 주세요 Please wait while Setup installs PostgreSQL on your computer # 설치 중... installing... [설치 진행 상태바] 10.Completing 10단계에서 완료 화면을 볼 수 있다. 만약 구성 요소 선택에서 다른 옵션을 더 체크했다면 별도의 추가 작업이 있다. 근데 이건 설치 이후에도 추가할 수 있는 부분이니까 별도로 추가하면 된다.\n1 2 3 4 5 6 7 [Setup] # PostgreSQL 설치 마법사 완료 Completing the PostgreSQL Setup Wizard # 컴퓨터에 PostgreSQL 설치가 완료되었습니다. Setup has finished installing PostgreSQL on your computer. PostgreSQL 삭제 제어판 \u0026gt; 프로그램 및 기능에서 설치한 PostgreSQL 버전 삭제를 누르면 Setup 팝업이 뜬다. Entire application 또는 individual components 중 선택하고 Click Next\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Uninstallation mode] # 전체 애플리케이션 또는 개별 구성 요소를 제거하시겠습니까? Do you want to uninstall entire application or individual components? # 전체 애플리케이션 - [ ] Entire application # 전체 애플리케이션과 애플리케이션에 설치된 모든 파일을 제거합니다 Removes entire application and all files installed by the application # 개별 구성 요소 - [ ] individual components # 개별 구성 요소를 제거하고 나머지 애플리케이션은 설치된 상태로 유지합니다 Removes individual components while leaving the rest of application installed 삭제 진행 프로그레스를 지나고 경고가 있을 수 있다. data\\log 파일을 보고 있었고 등등 다른 프로그램에서 사용 중이라서 발생한 것 같다.\n1 2 3 [Warning] The data directory (C:\\\\Program Files\\PostgreSQL\\\u0026lt;version\u0026gt;\\data) has not been removed 그래도 삭제 완료는 된다.\n1 2 3 [Info] Uninstallation completed PostgreSQL 설치 경로 C:\\\\Program Files\\PostgreSQL에서 폴더도 깔끔히 삭제해준다. PostgreSQL 관련 시스템 환경 변수를 등록했었다면 찾아서 삭제한다.\n재부팅 끗-⭐.\n⚙️EndNote 참고 자료 PostgreSQL을 여행하는 입문자를 위한 안내서 ","date":"2025-04-20T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-20-psql-getting-started/","title":"psql: 시작하기"},{"content":"📌개요 \u0026ldquo;Spring과 Spring Boot는 뭐가 다르지?\u0026rdquo; 백엔드 개발을 배우기 시작하면 누구나 한 번쯤은 이 질문을 던진다. 이 글에선 Spring과 Spring Boot의 개념적 차이와 각자의 장단점을 정리하고, 실제 개발에서 어떤 상황에 어떤 것을 쓰는 게 맞는지 판단할 수 있는 기준을 다룬다. Spring Boot는 Spring을 완전히 대체하는 것이 아니라, 더 쉽게 활용할 수 있게 돕는 프레임워크라는 점이 핵심이다.\n📌내용 Spring이란? Spring은 자바 기반의 대형 애플리케이션 프레임워크로, 주된 목표는 기업용 애플리케이션 개발에서의 복잡성을 줄이는 것이다. 다음과 같은 특징이 있다.\nDI (Dependency Injection): 객체 간의 의존성을 느슨하게 관리. AOP (Aspect-Oriented Programming): 횡단 관심사(로깅, 보안 등)를 분리. 트랜잭션 관리, 데이터 접근 (JDBC, ORM) 등 다양한 엔터프라이즈 기능 지원. 장점은 유연성과 확장성인데, 단점은 설정이 복잡하고 XML 또는 자바 기반의 방대한 설정 파일을 직접 작성해야 했다는 점이다.\nSpring Boot란? Spring Boot는 \u0026ldquo;Spring의 생산성을 혁신적으로 끌어올리기 위해 등장한 프레임워크\u0026rdquo; 다. 쉽게 말하면, Spring을 더 쉽게 쓰게 해주는 Spring의 확장판이다.\n자동 설정(Auto Configuration): 필요한 설정을 자동으로 감지. 내장 WAS 제공: 톰캣, 제티 등을 내장해서 배포가 간편. starter 의존성: spring-boot-starter-*로 대표되는 의존성 관리 패키지 제공. 운영툴 내장: 헬스체크, 메트릭 등 운영 편의성 향상. 어떤 관계일까? Spring Boot는 Spring 프레임워크 위에서 동작한다. Spring이 없으면 Spring Boot도 존재할 수 없다. 쉽게 말하면, Spring Boot는 Spring의 사용성을 높여주는 껍데기(Wrapper) 라고 보면 된다. 구분 Spring Spring Boot 초기 설정 수동 설정 (XML, Java Config) 자동 설정, Starter 의존성 웹 서버 별도 설치 필요 내장 톰캣 등 제공 개발 생산성 설정 주도 빠른 개발 \u0026amp; 운영 편의성 진입 장벽 높음 낮음 주요 용도 복잡한 엔터프라이즈 시스템 빠른 프로토타이핑, 마이크로서비스 등 왜 Spring Boot가 탄생했나? Spring의 유연성은 좋았지만 개발자들이 너무 많은 설정에 지쳐 갔다. 당시 루비 온 레일즈(Ruby on Rails) 같은 프레임워크가 \u0026ldquo;관습이 코드보다 우선한다(Convention over Configuration)\u0026ldquo;라는 철학으로 큰 인기를 끌면서, Spring 진영도 더 생산적인 개발 환경을 고민하게 됐다. 그 결과물이 바로 Spring Boot다.\n언제 어떤 걸 써야 할까? Spring만 사용하는 경우\n이미 사내 표준화된 Spring 환경이 있고, 커스텀한 설정이 중요한 레거시 시스템. 직접 세밀하게 설정을 제어해야 하는 경우. Spring Boot를 사용하는 경우\n빠르게 서비스를 런칭해야 하는 스타트업, 프로토타입 개발. 마이크로서비스 아키텍처처럼 독립적 서비스가 많은 구조. DevOps/운영 환경에서 헬스체크, 메트릭 등 운영 편의성을 중요하게 여기는 경우. Spring Boot의 한계 내부가 워낙 자동화돼 있어서 초보자일 땐 \u0026lsquo;왜 이렇게 동작하지?\u0026lsquo;라는 혼란이 올 수 있음. 필요 이상으로 불필요한 의존성이 포함될 때도 있어서, 프로젝트 규모가 커질수록 최적화와 경량화가 필요함. 내장 톰캣 등을 사용하는 경우 컨테이너 기반 환경에서는 커스텀 설정이 까다로울 수 있음. SpringBoot 살짝 뜯어보기 메인 클래스의 역할 Spring Boot 애플리케이션은 public static void main(String[] args) 메서드를 진입점으로 실행되며 이 메인 클래스가 Spring Boot 실행 구조의 출발점이자 전체 설정 및 컴포넌트 구성을 담당하는 핵심 위치를 차지한다.\nmain() 메서드의 역할 Java Application의 진입점은 main() 메서드이며 Spring Boot도 예외는 아니다. Spring Boot에서는 이 메서드 내부에서 다음과 같은 방식으로 애플리케이션을 시작한다.\n1 2 3 4 5 6 @SpringBootApplication public class MyApplication { public static void main(String[] args) { SpringApplication.run(MyApplication.class, args); } } SpringApplication.run(...)은 스프링 컨테이너를 초기화하고 내장 서버(Tomcat 등)을 구동하며 전체 애플리케이션을 실행하는 메서드 이 호출은 결국 ApplicationContext 생성, 설정파일 로딩, 자동 구성 처리, 컴포넌트 스캔 등의 일련의 과정을 시작한다. main() 메서드가 있는 클래스는 보통 구성의 기준점이 되는 루트 패키지에 위치시켜 이후 컴포넌트 탐색 및 의존성 주입의 범위를 결정하는데 사용된다. @SpringBootApplication Spring은 처음엔 너무 방대해서 DI, AOP 같은 핵심 개념만 뽑아서 작은 프로젝트에 적용해봤다. Spring Boot는 공식 문서의 Getting Started를 따라하면서 구조를 눈에 익히고, 실제로 @SpringBootApplication 어노테이션이 내부적으로 어떤 동작을 하는지 코드를 따라가 보면서 깊이 이해해 보는 것에 도전했다.\n처음 SpringBoot 프로젝트를 생성하면 @SpringBootApplication이라는 어노테이션이 보인다. 이 한 줄이 모든 걸 시작하게 만들지만 대부분은 단순히 \u0026ldquo;부트스트랩용 어노테이션\u0026rdquo; 정도로만 알고 넘어간다. 하지만 실제 동작 원리를 이해해야 Spring Boot의 진짜 힘을 느낄 수 있다.\n@SpringBootApplication은 뭘 감싸고 있나? 사실 이건 복합 구성(Meta-Annotation)으로 세 가지 주요 어노테이션을 묶고 있다.\ngraph TD A[\"@SpringBootApplication\"] A --\u003e B[\"@ComponentScan\"] A --\u003e C[\"@SpringBootConfiguration\"] C --\u003e C1[\"@Configuration\"] C1 --\u003e C2[\"@Component\"] A --\u003e D[\"@EnableAutoConfiguration\"] D --\u003e D1[\"@Import\"] D --\u003e D2[\"@AutoConfigurationPackage\"] D2 --\u003e D3[\"@Import\"] 1 2 3 4 5 6 7 8 9 10 11 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { ... } @SpringBootConfiguration 말 그대로 스프링 설정 파일 역할을 한다. 이건 @Configuration의 확장판이다. 즉, 이 클래스가 빈 설정의 진입점임을 알려준다.\n@EnableAutoConfiguration Spring Boot의 핵심 기능. 자동 설정을 활성화해서 클래스패스에 있는 라이브러리와 환경에 따라 필요한 설정을 자동으로 구성한다. 이건 내부적으로 spring.factories 파일을 통해 각종 AutoConfiguration 클래스를 로딩한다.\n@ComponentScan 현재 패키지를 기준으로 빈 검색을 시작한다. 컨트롤러, 서비스 등을 @Component 계열로 붙이면 이게 알아서 찾아 등록해준다.\n실행 흐름 시각화 위에서 살펴본 메인 클래스와 @SpringBootApplication의 세 가지 구성 요소는 애플리케이션 실행 시 정해진 순서와 구조에 따라 동작한다.\nflowchart TD A[\"public static void main\"] --\u003e B[\"SpringApplication.run(...)\"] B --\u003e C[\"@SpringBootApplication\"] C --\u003e D_sub D_sub --\u003e E[\"ApplicationContext 초기화\"] E --\u003e F[\"내장 Tomcat 등 서버 구동\"] F --\u003e G[\"클라이언트 요청 대기 상태 진입\"] subgraph D_sub [구성 어노테이션] DA[\"@EnableAutoConfiguration\"] DB[\"@ComponentScan\"] DC[\"@SpringBootConfiguration\"] end main() : 애플리케이션의 단순한 진입점이 아니라 Spring Boot 전체 실행의 기준점 SpringApplication.run() : 실행 컨텍스트 구성, 리스너 등록, 설정 파일 로딩 등 수행 @EnableAutoConfiguration : 클래스패스를 기반으로 설정 후보 자동 로딩 및 적용 @ComponentScan : 지정된 패키지에서 컴포넌트(@Component, @Service 등) 자동 탐색 @SpringBootConfiguration : Java 기반 설정 클래스 등록. 수동 Bean 정의에 사용 ApplicationContext 초기화 : 의존성 주입 및 라이프사이클 콜백 수행 내장 서버 실행 : 기본 포트(8080)에서 HTTP 요청을 수신할 수 있는 상태로 전환됨 이 흐름은 단순히 외워야 할 절차가 아니라, 설정 누락, 컴포넌트 인식 실패, 자동 설정 충돌 등의 문제를 디버깅할 때 반드시 이해하고 있어야 하는 구조이다.\n🎯결론 Spring Boot는 단순한 대체제가 아니라 Spring을 쉽고 빠르게 활용하도록 돕는 파트너다. 이번 포스팅을 통해 Spring과 Spring Boot의 관계를 명확히 이해하고, @SpringBootApplication 내부의 작동 방식을 직접 따라가 본 경험으로, 단순 사용자가 아니라 이해하고 제어할 줄 아는 개발자로 한 걸음 성장했다.\n다음 단계로는 Spring Boot의 자동 설정(AutoConfiguration) 메커니즘을 더 깊이 파고들어, 커스텀 자동 설정을 만드는 방법까지 알아보면 좋겠다.\n⚙️EndNote 사전 지식 Java 언어 기초 객체지향 프로그래밍 개념 (DI, AOP 등) 톰캣과 같은 WAS 개념 더 알아보기 Spring 공식 문서 Spring Boot 공식 문서 스프링 부트 Reference Docs 마이크로서비스 아키텍처 ","date":"2025-04-19T12:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-19-spring-vs-springboot/","title":"Spring VS Spring Boot"},{"content":"📌개요 네트워크 기초에서 가장 먼저 마주치는 질문 중 하나 \u0026ldquo;TCP와 UDP는 뭐가 다른가?\u0026rdquo; 이 두 전송 계층 프로토콜의 구조적 차이, 성능 트레이드오프, 사용 사례 등을 알아보자.\n📌내용 전송 계층의 두 얼굴 패킷이란? 정보 기술에서 패킷 방식의 컴퓨터 네트워크가 전달하는 데이터의 형식화된 블록이다. 즉, 컴퓨터 네트워크에서 데이터를 주고 받을 때 정해 놓은 규칙이다.\n인터넷은 데이터를 잘게 나눈 패킷을 여러 장비를 통해 전달하는 구조다. 이때 각 패킷이 어떤 방식으로 전송되는지를 결정짓는 핵심이 바로 전송 계층의 프로토콜이다. 대표적으로 TCP(Transmission Control Protocol)과 UDP(User Datagram Protocol)이 있다.\nTCP란? 연결 지향(Connection-oriented) 방식 데이터 전달의 신뢰성 보장: 손실, 순서 뒤바뀜, 중복 등 방지 3-way handshake로 연결 성립 흐름 제어 및 혼잡 제어 존재 대표 사례: HTTP, HTTPS, FTP, 이메일(SMTP/IMAP) UDP란? 비연결형(Connectionless) 방식 빠르지만 신뢰성 없음: 패킷 손실, 순서 바뀜 감수 handshake 없음, 오버헤드 낮고 속도가 빠름 대표 사례: 스트리밍, 게임, VoIP(인터넷 전화), 실시간 방송 빠르게 비교해보기 구분 TCP UDP 연결 방식 연결형 (3-way handshake) 비연결형 신뢰성 높음 (재전송, 순서 보장) 낮음 (유실 감수) 속도 느림 빠름 헤더 크기 20바이트 이상 8바이트 사용 사례 웹, 파일 전송, 이메일 게임, 영상/음성 스트리밍, DNS 택배와 엽서로 예를 들어보자.\nTCP = 택배 받는 사람 확인 배송 상태 추적 중간에 깨지면 다시 보내줌 도착 순서 지켜줌 UDP = 엽서 우체통에 넣으면 바로 간다 누가 받았는지 모름 중간에 없어지면 그냥 잃어버림 빨리 도착함 선택 기준 상황 추천 프로토콜 데이터 유실이 치명적인 경우 (예: 은행 송금, 로그인 등) TCP 속도가 중요하고, 약간의 유실은 괜찮은 경우 (예: 실시간 영상통화, 게임) UDP 🎯결론 TCP는 신뢰, UDP는 속도 전송 계층에서 무엇을 우선시하느냐에 따라 선택이 달라진다.\n정확성이 생명인 금융 거래, 로그인 등은 TCP 실시간성과 속도가 중요한 게임, 방송은 UDP ⚙️EndNote 사전 지식 OSI 7계층 모델 중 전송 계층의 역할 IP(인터넷 프로토콜)가 패킷을 어떻게 전달하는지 개념 이해 클라이언트-서버 모델에 대한 기초 더 알아보기 Wireshark로 TCP/UDP 패킷 캡쳐하기 RFC 793 (TCP) RFC 768 (UDP) 3-way handshake 개념과 흐름도 ","date":"2025-04-19T10:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-19-differences-between-tcp-and-udp/","title":"TCP vs UDP"},{"content":"📌개요 HashSet은 Java 컬렉션 프레임워크의 일부이며 Set 인터페이스를 구현한 클래스다. Set은 인터페이스(규약)이고 HashSet은 그 구현체다.\n📌내용 구현 계층 구조 graph TD A[Iterable] --\u003e B[Collection] B --\u003e C[Set] C --\u003e D[HashSet] C --\u003e E[SortedSet] E --\u003e F[TreeSet] 1 2 3 4 5 6 7 // 개념도 Set 인터페이스 (규약) ↑ 구현 HashSet 클래스 └── (내부적으로 HashMap 사용) ├── 키(key): 실제 저장할 객체 └── 값(value): PRESENT (Dummy Obejct) 내부 동작 방식 데이터 저장 구조 HashSet은 내부적으로 HashMap을 사용하여 요소들을 저장한다. HashSet에 추가하는 각 요소는 HashMap의 키(Key)로 저장되며 값(Value)에는 더미 객체(PRESENT)가 저장된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // java.util.HashSet 실제 Java 소스 코드 일부 public class HashSet\u0026lt;E\u0026gt; extends AbstractSet\u0026lt;E\u0026gt; implements Set\u0026lt;E\u0026gt;, Cloneable, java.io.Serializable { @java.io.Serial static final long serialVersionUID = -5024744406713321676L; transient HashMap\u0026lt;E,Object\u0026gt; map; // Dummy value to associate with an Object in the backing Map static final Object PRESENT = new Object(); /** * Constructs a new, empty set; the backing {@code HashMap} instance has * default initial capacity (16) and load factor (0.75). */ public HashSet() { map = new HashMap\u0026lt;\u0026gt;(); } } 중복 제거 매커니즘 HashSet이 중복을 확인하는 방법은 두 단계로 이루어진다.\n해시 코드 비교: 먼저 객체의 hashCode() 메서드를 호출하여 해시 코드를 비교한다. 해시 코드가 다르면 다른 객체로 판단한다. 해시 코드가 같으면 2단계 진행 equals 비교: equals() 메서드를 사용하여 실제로 같은 객체인지 확인한다. equals()가 true를 반환하면 같은 객체로 판단하여 추가를 허용하지 않음 equals()가 false를 반환하면 다른 객체로 판단하여 추가 허용 1 2 3 4 5 6 // 중복 확인 pseudo if(existingElement.hashCode() == newElement.hashCode() \u0026amp;\u0026amp; existingElement.equals(newElement)) { // 중복으로 판단하여 추가하지 않음 } else { // 새로운 요소로 추가 } 성능 특징 평균 시간 복잡도(add, remove, contains): O(1) 최악의 경우(모든 요소가 같은 해시 버킷에 있는 경우): O(n) 직접 구현해보는 간단한 HashSet 예제 값을 키로 사용하기 때문에 중복을 방지할 수 있고 키-값 쌍이어야 하는데 값을 키에 사용하니 값에는 넣을 필요 없이 더미 객체로 자리를 차지하게 두면 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 package com.myhashset; import java.util.HashMap; public class MyHashSet\u0026lt;E\u0026gt; { private static final Object PRESENT = new Object(); private HashMap\u0026lt;E, Object\u0026gt; map; public MyHashSet() { map = new HashMap\u0026lt;\u0026gt;(); } // 요소 추가 public boolean add(E e) { return map.put(e, PRESENT) == null; } // 요소 제거 public boolean remove(E e) { return map.remove(e) == PRESENT; } // 요소 포함 여부 확인 public boolean contains(E e) { return map.containsKey(e); } // 크기 반환 public int size() { return map.size(); } // 비어 있는지 확인 public boolean isEmpty() { return map.isEmpty(); } // 모든 요소 제거 public void clear() { map.clear(); } // 반복자 반환 public java.util.Iterator\u0026lt;E\u0026gt; iterator() { return map.keySet().iterator(); } public static void main(String[] args) { MyHashSet\u0026lt;String\u0026gt; set = new MyHashSet\u0026lt;\u0026gt;(); set.add(\u0026#34;Apple\u0026#34;); set.add(\u0026#34;Banana\u0026#34;); set.add(\u0026#34;Apple\u0026#34;); // 중복 추가 시도 System.out.println(set.contains(\u0026#34;Apple\u0026#34;)); // true System.out.println(set.contains(\u0026#34;Orange\u0026#34;)); // false System.out.println(set.size()); // 2 set.remove(\u0026#34;Banana\u0026#34;); System.out.println(set.size()); // 1 } } PRESENT의 역할 PRESENT는 HashSet이 내부적으로 사용하는 더미 객체(Dummy Object)로 HashMap에서 값 부분을 채우기 위한 목적으로 사용된다.\nHashSet은 내부적으로 HashMap을 사용하여 구현되어 있다. HashMap은 키-값 쌍으로 데이터를 저장하는데 HashSet은 오직 키만 저장하는 Set 인터페이스를 구현해야 한다. 따라서 값 부분에는 의미 없는 더미 객체를 저장한다. 핵심 개념 요약 키(Key)만 저장하는 Set의 특성 HashSet은 순수하게 Key들의 모음으로 동작해야 한다. 하지만 내부적으로는 HaspMap을 사용해 구현되어 있다. HashMap은 반드시 키-값 쌍으로 저장해야 한다. 값(Value)을 처리하는 방법 키에는 실제 저장할 값이 들어간다. 값에는 항상 동일한 PRESENT 객체를 저장한다. 모든 요소가 공유하는 더미 객체를 private static final 선언해서 메모리 낭비를 없앤다. HashMap의 구조를 유지하면서 Set처럼 동작한다. 중복 방지 매커니즘 HashMap은 키의 중복을 허용하지 않는다. HashSet.add()는 내부적으로 map.put(key, PRESENT)를 호출한다. 키가 처음 추가되면 → null 반환 → true 성공 키가 이미 존재하면 → 기존 PRESENT 반환 → false 실패 🎯결론 구현 트레이드 오프: HashSet은 메모리 효율성(단일 PRESENT 사용)과 코드 재사용성(HashMap 활용) 사이의 절충안으로 설계됐다. 중복 검사의 본질: hashCode()와 equals()의 적절한 오버라이딩이 필수적이며, 이는 모든 Map/Set 기반 컬렉션의 공통 요구사항이다. 성능 보장: 이상적인 경우 O(1)의 시간 복잡도로 동작하나 불균형 해시 분포 시 O(n)으로 저하될 수 있다. ⚙️EndNote 사전 지식 해시 테이블 원리: 버킷 구조, 해시 충돌 처리 방식(체이닝/개방 주소법) 불변성(Immutability): HashSet에 저장된 객체의 필드 변경 시 발생하는 문제 더 알아보기 다른 Set 구현체 비교 스레드 안전(Thread-safe): Tread-safe란 멀티 스레드 프로그래밍에서 일반적으로 어떤 함수나 변수, 혹은 객체가 여러 스레드로부터 동시에 접근이 이루어져도 프로그램의 실행에 문제가 없는 것을 말한다.\n하나의 함수가 한 스레드로부터 호출되어 실해 중일 때, 다른 스레드가 그 함수를 호출하여 동시에 함께 실행되더라도 각 스레드에서 함수의 수행 결과가 올바르게 나오는 것을 말한다.\n출처: 스레드 안전(Thread-Safety)란?\n구현체 내부 구조 순서 보장 null 허용 Thread-safe HashSet HashMap ❌ ✅ ❌ LinkedHashSet LinkedHashMap 삽입 순서 ✅ ❌ TreeSet Red-Black Tree 정렬 순서 ❌ ❌ Java 8+의 성능 개선 해시 충돌 시 연결 리스트 → Red-Black 트리로 전환 java.util.concurrent.ConcurrentSkipListSet 병렬 처리용 구현체 추천 참고 자료 HashSet의 HashMap 활용 설계 Java 공식 문서에서는 HashSet이 HashMap을 기반으로 구현됨을 명시하고 있다. 이는 메모리 효율성과 코드 재사용성을 위한 설계 선택이다. Oracle Java Docs: HashSet hashCode()와 equals()의 중요성 객체의 중복 검사 매커니즘은 Java 컬렉션 프레임워크의 핵심 원리로 모든 Map/Set 구현체에 적용된다. Java Tutorial: Object Methods ","date":"2025-04-18T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-18-java-hashset-internal-behavior-and-deduplication-mechanisms/","title":"Java HashSet"},{"content":"📌개요 개발자라면 \u0026lsquo;시간 복잡도\u0026rsquo;라는 용어를 자주 접한다. 하지만 막상 O(n)과 O(log n)의 차이가 얼마나 큰 영향력을 가지는지 체감하기 어려운 것 같다.\nO(n)과 O(log n)의 성능 차이를 실생활 예시로 알아보고 데이터의 크기가 1백만 개 이상일 때 각각 대략 몇 번의 연산이 필요한지 비교 분석해보자.\n📌내용 일상 속의 시간 복잡도 만만한 \u0026ldquo;전화번호부에서 이름 찾기\u0026quot;를 예로 들어보자.\nO(n) 순차 검색 처음부터 끝까지 하나씩 확인하는 방식 최악의 경우 모든 ㅔ이지를 뒤져야 한다. 데이터가 2배 증가하면 검색 시간도 2배 증가한다. O(log n) 이진 검색 가운데를 펴서 앞/뒤를 결정하는 방식 매 단계에서 검색 범위가 절반으로 줄어든다. 데이터가 2배 증가해도 검색 시간은 1단계만 더 필요하다. 수치로 보는 차이 1백만 개 이상의 데이터를 처리한다고 했을 때 무려 52,000배 이상의 효율 차이를 보인다. O(n)은 1,048,576회 연산 O(log n)은 단 20회 연산 데이터 크기 O(n) 연산 횟수 O(log₂n) 연산 횟수 차이 비율 16 16 4 4:1 1,024 1,024 10 102:1 1,048,576 1,048,576 20 52,429:1 알고리즘 선택의 기준 O(n)이 나은 경우 데이터 규모가 매우 작을 때 구현이 간단해야 할 때 한 번만 실행되는 스크립트 O(log n)이 필수인 경우 대규모 시스템의 핵심 기능 실시간 처리 필요한 서비스 빈번하게 호출되는 API 🎯결론 성능 분석 자료를 정리하기 위해 자료를 찾아보면서 이미 많은 사람들이 질문했던 \u0026ldquo;어떻게 하면 선형 시간을 로그 시간으로 끌어올릴 수 있을까?\u0026rdquo; 같은 고민들을 엿볼 수 있었다.\n왜 기업들 기술 인터뷰에서 O(log n) 알고리즘 구현을 강조하는지, 왜 데이터베이스에 인덱스를 생성하는지 같은 현업에서 마주하는 많은 질문들에 대한 근본적인 답을 얻을 수 있었다.\n⚙️EndNote 사전 지식 시간 복잡도: 알고리즘이 입력 크기에 따라 어떻게 수행 시간이 증가하는지 나타내는 척도 Big-O 표기법: 최악의 경우를 고려한 알고리즘의 상한 성능 표현 로그 함수: 지수 함수의 역함수, 알고리즘에서 문제 크기를 지속적으로 전반으로 나누는 과정에서 나타난다. 더 알아보기 인덱스 설계 전략 B-tree 인덱스는 기본적으로 O(log n) 복잡도 제공 Composite Index 설계 시 컬럼 순서가 로그 효율성에 영향 캐싱과의 조합 O(log n) 알고리즘 + LRU 캐시 = O(1)에 근접하는 성능 LRU: Least Recently Used LFU: Least Frequently Used 점근 표기법의 한계: 최악의 경우만 보는 Big-O의 단점 분할 정복 알고리즘의 공간 복잡도 트레이드 오프 참고 자료 Difference between O(n) and O(log(n)) - which is better and what exactly is O(log(n)) Big-O Cheat Sheet: https://www.bigocheatsheet.com/ ","date":"2025-04-18T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-18-big-o-differences-in-performance-between-on-and-ologn/","title":"O(n) VS O(log n)"},{"content":"📌개요 정적 팩토리 메서드는 객체 생성을 더 명확하고, 안전하며 유연하게 만들 수 있는 방법이다. 일반적인 생성자(new) 방식과 달리 이름을 자유롭게 부여할 수 있고, 객체 생성을 숨기거나 제어할 수 있다.\n보통 생성자를 private으로 숨기고 정적 팩토리 메서드를 통해서만 객체 생성을 허용하는 방식으로 활용한다.\n정적 팩토리 메서드의 장단점과 어떤 상황에서 효과적인지 알아보자.\n📌내용 왜 쓰는데? 명확한 의도 표현 생성자는 클래스명과 같아야 하므로 객체가 무엇을 위한 용도인지 코드만 보고 파악하기 어렵다.\n1 User user = new User(); // 이게 누구지? 회원? 관리자? 손님? 정적 팩토리 메서드는 이름을 자유롭게 지정할 수 있어 객체 생성의 의도와 맥락을 명확하게 전달할 수 있다.\n1 2 3 User guest = User.guest(); // 비회원 User fromJson = User.adminWith(\u0026#34;READ\u0026#34;, \u0026#34;WRITE\u0026#34;); // 권한 있는 관리자 User fromId = User.fromEmail(\u0026#34;test@test.com\u0026#34;); // 이메일 기반 생성 그냥 아무나 데려오는 것과 목적과 정체가 분명한 사람을 요청하는 것은 다르다. 생성자의 경우 의도를 파악하기 어렵지만 정적 팩토리 메서드는 의도와 맥락을 명확히 드러낸다.\n객체 생성 제어 정적 팩토리 메서드를 사용하면 객체 생성을 직접 통제할 수 있다. 예를 들어 같은 손님(guest)는 한 번만 생성되도록 만들 수 있다.\n1 2 3 4 5 6 7 8 9 public class User { private static final User GUEST = new User(\u0026#34;guest\u0026#34;, \u0026#34;GUEST\u0026#34;); private User(String name, String role) { ... } public static User guest() { return GUEST; // 항상 같은 객체를 반환 } } \u0026ldquo;공용 컴퓨터 계정\u0026quot;처럼 같은 계정을 여러명이 공유하는 개념이다. 매번 새로운 객체를 만들지 않고 미리 정의된 인스턴스를 공유할 수 있다.\n반환 타입의 유연성 생성자는 반환 타입이 고정되어 있어서 유연하게 타입을 다루기 어렵다.\n정적 팩토리 메서드는 상위 타입을 반환하고 내부적으로는 하위 클래스나 구현체를 사용할 수 있다. 덕분에 구현체를 숨기고 인터페이스 기반의 유연한 설계가 가능하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public interface User {} public class GuestUser implements User {} public class AdminUser implements User {} public class UserFactory { public static User guest() { return new GuestUser(); } public static User adminWith(String... permissions) { return new AdminUser(permissions); } } 웹서비스에서 \u0026ldquo;유저\u0026quot;라는 요청을 보내면 실제로는 \u0026ldquo;게스트\u0026rdquo; 혹은 \u0026ldquo;관리자\u0026quot;가 올 수 있다. 클라이언트는 \u0026ldquo;User\u0026quot;만 알고 어떤 타입인지는 서버가 알아서 판단한다.\n객체 생성 비용 절감 매번 DB나 외부 API를 통해 객체를 생성하는 건 성능에 부담이 된다. 정적 팩토리 메서드는 캐시를 활용하거나 지연 생성 전략을 적용할 수 있다.\n1 2 3 4 5 6 7 public class UserRepository { private static final Map\u0026lt;String, User\u0026gt; cache = new HashMap\u0026lt;\u0026gt;(); public static User fromEmail(String email) { return cache.computeIfAbsent(email, User::loadFromDb); } } 마치 이메일 계정으로 로그인할 때 이미 로그인한 적 있으면 바로 입장, 없으면 DB에서 불러와서 캐시에 저장 후 사용하는 전략이다.\n런타임 유연성 구현체가 없어도 설계 가능. 어떤 클래스가 어떤 구현을 가질지 작성 시점에 모를 수도 있다.\n정적 팩토리 메서드는 인터페이스만 먼저 정의하고 구현은 나중에 환경 설정이나 조건에 따라 동적으로 결정할 수 있다.\n1 2 3 4 5 6 7 public static User ofType(String type) { return switch (type) { case \u0026#34;guest\u0026#34; -\u0026gt; guest(); case \u0026#34;admin\u0026#34; -\u0026gt; adminWith(\u0026#34;ALL\u0026#34;); default -\u0026gt; throw new IllegalArgumentException(); } } \u0026ldquo;유저 타입: guest\u0026quot;라는 요청이 들어오면, 필요한 타입의 유저로 찾아서 제공하는 유연한 구조가 가능하다.\n생성자를 숨기면? 불변 클래스 보장 생성자를 외부에서 호출하지 못하게 방지하면 객체 상태를 변경할 수 없도록 안전하게 설계할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public final class User { private final String email; private final String role; private User(String email, String role) { this.email = email; this.role = role; } public static User fromEmail(String email) { return new User(email, \u0026#34;USER\u0026#34;); } // 상태를 바꿀 수 있는 setter가 존재하지 않음 } 한 번 발급된 여권처럼 이름이나 생년월일을 나중에 바꿀 수 없다. 다시 만들려면 새 여권을 발급해야 한다.\n정교한 인스턴스 생성 정적 팩토리 내부에서 입력 검증이나 비즈니스 로직을 넣을 수 있다.\n1 2 3 4 public static User fromEmail(String email) { if (!email.contains(\u0026#34;@\u0026#34;)) throw new IllegalArgumentException(); return new User(email, \u0026#34;USER\u0026#34;); } \u0026ldquo;@ 없는 이메일은 회원가입 불가\u0026quot;처럼 생성 조건을 내부 로직에서 엄격히 관리할 수 있다.\n추상화 및 캡슐화 강화 내부 구현체를 외부에 노출하지 않고 인터페이스만 제공해 유연한 구조를 만들 수 있다.\n1 User user = UserFactory.ofType(\u0026#34;admin\u0026#34;); 여행자에게 여권만 주고 그 사람이 한국인이든 외국인이든 내부적으로 판단하는 구조.\n팩토리 메서드 단점은 없나? 상속 제약 정적 팩토리 메서드만 제공하고 public 또는 protected 생성자가 없으면 하위 클래스를 만들 수 없다. 하지만 실무에서는 대부분 final, 불변 객체, 캡슐화를 목적으로 사용하기에 장점이 되기도 한다.\n개발자의 인지 정적 팩토리 메서드는 생성자와 달리 이름이 자유롭기 때문에 개발자가 어떤 메서드가 객체를 생성하는지 알기 어려울 수 있다. 따라서 명확한 네이밍 규칙을 정하고 문서화를 잘해야 한다.\n근데 생성자로도 되잖아? 정적 팩토리 메서드의 장점이라고 흔히 말하는 것들 중 상당수는 생성자만으로도 구현이 가능하다. 하지만 \u0026ldquo;가능하다\u0026quot;와 \u0026ldquo;적절하고 유연하게 된다\u0026quot;는 차원이 다른 이야기다.\n핵심은 기능 자체가 아니라 \u0026ldquo;표현력 + 유연성 + 제어력\u0026rdquo;\n의도를 드러내는 이름 부여 1 2 3 4 5 6 // 생성자: 의도 알기 힘듦 new User(\u0026#34;guest\u0026#34;, false); // 정적 팩토리 메서드 User guest = User.createGuest(); User member = User.createMember(); 생성자도 new User(\u0026quot;guest\u0026quot;, false) 처럼 사용해서 만들 수는 있다. 하지만 이게 어떤 의미인지 코드를 읽는 입장에서 바로 이해하기 어렵다.\n즉, 기능이 있냐 보단 의도를 읽을 수 있냐가 중요하다.\n같은 시그니처, 다른 로직 1 2 User user1 = User.from(\u0026#34;admin\u0026#34;); // 관리자 User user2 = User.from(\u0026#34;guest\u0026#34;); // 게스트 입력 값에 따라 내부적으로 완전 다른 구현체나 로직을 선택할 수 있다는 의미다. 생성자는 오버로딩을 해야 하는데, 시그니처가 같으면 구분할 수가 없다.\n생성자는 매번 객체를 새로 만들고 구분이 안 되는 반면, 정적 팩토리 메서드는 로직을 통합하고 선택적으로 처리 가능.\n하위 타입 반환 1 2 3 4 5 6 7 8 9 public interface User { ... } public class Guest implements User { ... } public class Admin implements User { ... } public static User from(String role) { if(\u0026#34;admin\u0026#34;.equals(role)) return new Admin(); return new Guest(); } 생성자는 무조건 해당 클래스의 인스턴스만 반환하지만 정적 팩토리 메서드는 상위 타입을 반환함으로써 내부 구현을 숨길 수 있다.\n이건 아예 설계 수준에서 다형성 추상화와 확장성이 달라지는 포인트다.\n객체 캐싱과 싱글톤 구현 1 2 3 4 5 private static final User INSTANCE = new USER(\u0026#34;singletone\u0026#34;); public static User getInstance() { return INSTANCE; } 생성자는 호출할 때마다 새로운 객체를 만드니까 객체 재사용을 하려면 외부에서 별도로 관리해줘야 한다. 정적 팩토리 메서드는 내부에서 인스턴스를 통제하니까 객체 풀, 캐시, 싱글톤 등을 자연스럽게 구현할 수 있다.\n🎯결론 정적 팩토리 메서드는 기능보다 표현력 + 제어력 + 추상화가 핵심이다. 생성자는 단순하고 직관적이지만 API 설계나 유지보수 측면에선 정적 팩토리 메서드가 더 유리하다.\n그러나 상속 제약과 개발자의 인지 문제 등 단점도 존재하므로 상황에 맞게 적절히 사용하는 것이 중요하다.\n기능 생성자 정적 팩토리 메서드 이름으로 생성 의도 표현 ❌ ✅ 같은 인자, 다른 로직 처리 ❌ ✅ 하위 타입 반환 ❌ ✅ 객체 재사용/캐싱 ❌ ✅ 다형성 추상화 설계 ❌ ✅ ⚙️EndNote 사전 지식 생성자: 클래스의 인스턴스를 생성하는 특별한 메서드 정적 메서드: 클래스의 인스턴스 없이 호출할 수 있는 메서드 싱글톤 패턴: 클래스의 인스턴스를 하나만 생성하도록 제한하는 디자인 패턴 불변 클래스: 객체 생성 후 내부 상태를 변경할 수 없는 클래스 더 알아보기 Effective Java 3rd Edition, Item 1: Consider static factory methods instead of constructors Java Constructors vs Static Factory Methods ","date":"2025-04-15T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-15-java-consider-a-static-factory-method-instead-of-a-constructors/","title":"Java 정적 팩토리 메서드"},{"content":"📌개요 싱글 브랜치 명령어를 사용해서 특정 브랜치만 클론한 저장소에서는 원격 저장소의 다른 브랜치를 확인할 수 없다.\n이런 상황에서 원격 브랜치를 추가, 삭제, 추적하는 방법을 알아보자.\n📌내용 어떤 상황이냐면? 1 git clone -b \u0026lt;브랜치_이름\u0026gt; --single-branch \u0026lt;원격_저장소_URL\u0026gt; 위 명령으로 특정 브랜치만 클론하면 .git/config 파일에 해당 브랜치에 대한 정보만 기록된다.\n특정 브랜치만 추적하기 위해 의도적으로 싱글 브랜치 클론을 받은 거라서 문제라고 하긴 뭐하지만, 추가적으로 다른 브랜치를 확인하고 싶은 경우를 예로 든다.\ngit remote show \u0026lt;저장소\u0026gt; 또는 git branch -a 명령어를 실행해도 원격 저장소의 다른 브랜치 목록을 확인할 수 없다.\ngit checkout 또는 git switch 명령어를 사용하여 원격의 다른 브랜치로 전환할 수 없다. 특정 브랜치만 로컬로 가져와 추적하고 원격 저장소에 또 다른 어떤 브랜치가 있는지 모르기 때문이다.\n어떡하지? 원격 브랜치 확인 1 2 3 4 # 로컬, 원격 모두 보기 git branch -a # 또는 원격 브랜치 보기 git branch -r 하지만 싱글 브랜치 옵션으로 로컬에 클론했기 때문에 특정 브랜치만 확인되고 나머지 원격 저장소에 존재하는 브랜치를 확인할 수 없다.\n원격 브랜치 추가 추적 1개 또는 그 이상의 브랜치를 추적할 수 있게 추가한 뒤 fetch 명령으로 브랜치에 대한 업데이트를 가져온다.\ngit remote set-branches 명령 사용 시 옵션을 함께 사용하지 않으면 덮어쓰기 되는 걸 주의하자.\n1 2 3 4 5 6 7 8 # 옵션 없이 사용하면 .git/config 덮어쓰기 된다. git remote set-branches origin \u0026lt;브랜치_이름1\u0026gt; \u0026lt;브랜치_이름2\u0026gt; ... # --add 옵션과 함께 git remote set-branches origin --add \u0026lt;브랜치_이름1\u0026gt; \u0026lt;브랜치_이름2\u0026gt; ... # 이후 조회 \u0026lt;저장소\u0026gt; = origin 또는 upstream git remote show \u0026lt;저장소\u0026gt; # 조회 후 업데이트 가져오기 git fetch 원격 브랜치 정보를 가져왔고 전환할 수 있게 됐다면, 직접 전환하거나 브랜치 목록을 추가할 수 있다.\n1 2 3 4 # 추적하며 전환 git checkout -t \u0026lt;브랜치_이름\u0026gt; # 또는 로컬 브랜치를 생성하며 원격 브랜치를 추적하도록 설정 git checkout -b \u0026lt;브랜치_이름\u0026gt; origin/\u0026lt;브랜치_이름\u0026gt; 브랜치 전환이 정상적으로 됐다면 원격 브랜치를 제대로 추적하고 있는지 확인한다.\n1 2 3 4 5 git branch -vv # 출력 결과 # 브랜치명1 1q2w3e4r [origin/브랜치명1] Merge pull request #* 브랜치명2 4r3e2w1q [origin/브랜치명2] feat: shomething 추가가 된다면 삭제는? git remote set-branches 명령어에는 --add 옵션이 있어서 추가할 수 있지만 삭제 옵션은 없다. 덮어쓰기가 가능하다는 점을 이용해서 추적 중지하고 싶은 브랜치를 제외하고 등록한다.\n1 2 3 4 # 옵션 없이 사용하면 .git/config 덮어쓰기 된다. git remote set-branches origin \u0026lt;브랜치_이름1\u0026gt; \u0026lt;브랜치_이름2\u0026gt; ... # 이후 조회 \u0026lt;저장소\u0026gt; = origin 또는 upstream git remote show \u0026lt;저장소\u0026gt; 🎯결론 추적 브랜치를 조정하는 작업은 fetch 명령 시 업데이트 정보를 확인하거나 브랜치를 전환하기 위함이다.\n원격 저장소에 브랜치가 많은데 모두 받긴 싫고 특정 브랜치로만 작업하고자 한다면 싱글 브랜치로 특정 브랜치만 가진 클론을 생성할 수 있다. 이후 추가적으로 다른 브랜치의 정보도 확인이 필요하다면 조정할 수 있다. 추적 브랜치를 추가하면 브랜치 목록에서 확인할 수 있다. 하지만 한 번이라도 추적이 된 브랜치라면 git remote set-branches 명령을 통해 제외하더라도 git branch -r 또는 git branch -a 옵션을 통한 명령어의 결과에선 계속 확인된다.\nfetch 설정과 원격 브랜치 목록 조회 명령어가 서로 다른 정보를 사용하기 때문이다.\n⚙️EndNote 최후의 수단 .git/config 파일을 직접 수정한다.\n.git/config 파일을 텍스트 편집기로 연다. [remote \u0026quot;origin\u0026quot;]과 같이 원하는 섹션에서 삭제하려는 브랜치에 대한 fetch 항목을 찾아 삭제하고 파일을 저장한다. 파일 정보를 예를 들면 아래와 같다.\n1 2 3 4 5 [remote \u0026#34;origin\u0026#34;] url = \u0026lt;원격_저장소_URL\u0026gt; fetch = +refs/heads/브랜치명2:refs/remotes/origin/브랜치명2 fetch = +refs/heads/브랜치명3:refs/remotes/origin/브랜치명3 fetch = +refs/heads/브랜치명4:refs/remotes/origin/브랜치명4 ","date":"2025-04-12T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-12-git-remote-branch-settings-in-a-single-branch-clone-environment/","title":"Git 원격 브랜치 설정"},{"content":"📌개요 버퍼(Buffer)는 컴퓨터 과학에서 핵심적인 개념으로, 데이터가 시스템의 서로 다른 구성 요소 간에 처리되거나 전송되는 동안 임시로 저장되는 메모리 영역이다.\n버퍼는 데이터 흐름을 관리하고 하드웨어나 소프트웨어 간 속도 차이로 인한 성능 병목 현상을 완화하며 효율성을 보장하는 데 필수적이다.\n특정 프로그래밍 언어에 국한되지 않는 개념인 CS에서 버퍼의 개념, 종류, 사용 사례 등을 알아보자.\n📌내용 버퍼란 무엇인가? 버퍼는 데이터를 한 곳에서 다른 곳으로 이동하거나 처리할 때 임시로 저장하는 메모리 공간이다.\n예를 들어, 빠른 처리 장치(예: CPU)와 느린 장치(예: 하드디스크) 간의 데이터 전송 속도 차이를 조정하거나, 네트워크를 통해 패킷이 전송될 때 데이터 손실을 방지하기 위해 버퍼가 사용된다.\n쉽게 말해, 버퍼는 데이터를 \u0026ldquo;중간에 잠시 보관\u0026quot;하여 시스템이 원활하게 작동하도록 돕는 역할을 한다.\n주요 기능: 속도 차이 완화: 서로 다른 속도로 작동하는 장치 간 데이터 흐름을 조정. 효율성 증대: 데이터를 모아서 한 번에 처리해 처리 속도를 높임. 안정성 확보: 데이터 손실이나 끊김을 방지. 버퍼의 중요성 버퍼는 컴퓨터 과학의 여러 분야에서 필수적이다.\n운영체제, 네트워킹, 데이터베이스, 멀티미디어 처리 등 다양한 영역에서 버퍼를 활용해 성능을 최적화하고 사용자 경험을 개선한다.\n예를 들어:\n운영체제: 디스크 읽기/쓰기 작업에서 입출력(I/O) 버퍼를 사용. 네트워킹: TCP/UDP 프로토콜에서 송수신 버퍼로 패킷을 관리. 멀티미디어: 비디오 스트리밍에서 끊김 없는 재생을 위해 데이터를 미리 저장. 데이터베이스: 쿼리 결과를 빠르게 제공하기 위해 캐시 버퍼를 활용. 버퍼는 단순한 메모리 저장 공간 이상의 역할을 하며, 시스템 설계와 성능 최적화에서 중요한 요소이다.\n버퍼의 종류 버퍼는 사용 목적, 구현 방식, 위치 등에 따라 다양한 방식으로 분류할 수 있다.\n1. 사용 목적에 따른 분류 입출력 버퍼: 파일 읽기/쓰기, 디스크 I/O에서 데이터를 임시 저장. 예: 하드디스크에서 데이터를 읽을 때 사용. 네트워크 버퍼: 네트워크 패킷을 저장해 전송 지연을 줄임. 예: 라우터의 송수신 버퍼. 캐시 버퍼: 자주 사용하는 데이터를 빠르게 접근하도록 저장. 예: 데이터베이스 쿼리 캐시. 스트리밍 버퍼: 비디오/오디오 스트리밍에서 끊김 없는 재생을 위해 데이터를 미리 저장. 예: 유튜브의 버퍼링. 그래픽 버퍼: 화면 렌더링에서 프레임 데이터를 저장. 예: GPU의 프레임 버퍼. 2. 구현 방식에 따른 분류 소프트웨어 버퍼: 응용 프로그램에서 메모리에 할당된 버퍼. 예: 파일 스트림 처리 시 메모리 내 버퍼. 하드웨어 버퍼: 물리적 장치에 내장된 버퍼. 예: 네트워크 카드의 패킷 버퍼. 3. 위치에 따른 분류 커널 버퍼: 운영체제 커널에서 관리. 예: 디스크 I/O 작업의 버퍼. 사용자 공간 버퍼: 응용 프로그램에서 관리. 예: 애플리케이션의 데이터 스트림 버퍼. 장치 버퍼: 하드웨어 장치에 위치. 예: 프린터의 출력 버퍼. 4. 크기 및 관리 방식에 따른 분류 고정 크기 버퍼: 크기가 고정된 버퍼. 예: 네트워크 패킷의 고정 크기 버퍼. 동적 크기 버퍼: 필요에 따라 크기가 조정되는 버퍼. 예: 스트리밍 앱의 적응형 버퍼. 순환 버퍼(Circular Buffer): 데이터를 덮어쓰며 재사용하는 방식. 예: 로그 시스템이나 실시간 데이터 처리. 버퍼의 사용 사례 버퍼는 다양한 상황에서 활용된다. 실세계와 컴퓨터 시스템에서의 대표적인 사용 사례를 알아본다.\n비디오 스트리밍: 유튜브나 넷플릭스 같은 플랫폼은 영상 데이터를 미리 버퍼에 저장해 끊김 없는 재생을 보장한다. 버퍼링이 완료되면 영상이 재생되며, 이는 네트워크 속도와 재생 속도 간 차이를 조정하는 과정이다. 파일 입출력: 파일을 읽거나 쓸 때, 데이터를 한 번에 처리하지 않고 버퍼에 모아 효율적으로 처리한다. 예를 들어, 텍스트 편집기에서 파일을 저장할 때 버퍼를 사용해 데이터를 디스크에 기록. 네트워크 통신: 라우터나 네트워크 스택에서 패킷을 임시로 저장해 네트워크 혼잡을 관리한다. TCP 프로토콜은 송수신 버퍼를 사용해 데이터 전송의 신뢰성을 높인다. 게임 개발: 게임 엔진에서 프레임 버퍼는 렌더링된 이미지를 저장해 부드러운 그래픽 출력을 보장한다. 버퍼 관리의 모범 사례 효과적인 버퍼 관리는 시스템 성능과 안정성에 큰 영향을 미친다. 버퍼를 사용할 때 고려해야 할 모범 사례다.\n적절한 버퍼 크기 선택:\n너무 큰 버퍼는 메모리 낭비를 초래하고, 너무 작은 버퍼는 성능 저하를 일으킨다. 사용 사례에 따라 최적의 크기를 실험적으로 결정해본다. 예: 스트리밍에서는 네트워크 속도를 고려해 동적으로 조정. 버퍼 오버플로우 방지:\n버퍼에 입력되는 데이터 크기를 철저히 검증해 오버플로우를 방지한다. 이는 보안 취약점(예: 버퍼 오버플로우 공격)을 예방하는 데 중요하다. 입력 데이터의 경계를 항상 확인한다. 동기화 관리:\n여러 프로세스나 스레드가 동일한 버퍼에 접근할 때는 동기화 메커니즘(예: 락, 세마포어)을 사용해 데이터 무결성을 유지한다. 메모리 효율성 고려:\n불필요한 버퍼 할당을 피하고, 사용이 끝난 버퍼는 즉시 해제한다. 순환 버퍼를 활용해 메모리 재사용을 최적화할 수 있다. 성능 모니터링:\n버퍼 사용이 시스템 성능에 미치는 영향을 주기적으로 분석한다. 예: 네트워크 버퍼가 과부하 상태라면 크기를 조정하거나 처리 속도를 최적화해야 한다. 버퍼 관련 주의사항 버퍼는 강력한 도구지만, 잘못 사용하면 문제를 일으킬 수 있다.\n버퍼 오버플로우(Buffer Overflow): 버퍼 크기를 초과하는 데이터 입력으로 인해 발생하며, 시스템 충돌이나 보안 취약점을 유발할 수 있다. 2000년대 초반 많은 소프트웨어가 이 문제로 해킹당한 사례가 있다고 한다. 메모리 누수: 버퍼를 해제하지 않으면 메모리 사용량이 증가해 시스템 성능이 저하될 수 있다. 지연 시간 증가: 과도한 버퍼링은 데이터 처리 지연을 초래할 수 있다. 예: 실시간 애플리케이션에서 큰 버퍼는 부적절하다. 비효율적 관리: 부적절한 버퍼 크기나 방식은 CPU 사용량 증가, 응답 시간 지연 등으로 이어질 수 있다. 🎯결론 버퍼는 컴퓨터 과학에서 데이터 흐름을 원활하게 하고 성능을 최적화하며, 시스템 안정성을 높이는 데 없어서는 안 될 요소이다.\n운영체제, 네트워킹, 데이터베이스, 멀티미디어 등 다양한 분야에서 버퍼는 필수적이며, 올바른 설계와 관리를 통해 최대한의 효율성을 끌어낼 수 있다.\n버퍼의 종류와 사용 사례를 이해하고, 모범 사례를 준수한다면 시스템 성능을 크게 향상 시킬 수 있다.\n⚙️EndNote 사전 지식 버퍼를 깊이 이해하기 위해 다음의 사전 지식과 추가 개념을 알아두면 도움이 된다.\n메모리 관리: 버퍼는 메모리 할당과 해제의 일부이므로, 메모리 관리 원리(스택, 힙, 가비지 컬렉션 등)를 이해하면 버퍼 설계가 쉬워진다. 동시성(Concurrency): 다중 스레드 환경에서 버퍼 접근 시 동기화 문제를 다루기 위해 락(Lock)이나 세마포어(Semaphore) 같은 개념을 알아야 한다. 네트워크 프로토콜: TCP/UDP와 같은 프로토콜에서 버퍼가 어떻게 작동하는지 이해하면 네트워크 버퍼링을 최적화할 수 있다. 보안: 버퍼 오버플로우 공격과 같은 보안 취약점을 예방하려면 안전한 코딩 기법(예: 입력 검증)을 익히는 것이 중요하다. 자료구조: 순환 버퍼(Circular Buffer)와 같은 특정 버퍼 구현은 큐(Queue)와 유사하므로, 기본 자료구조를 학습하면 도움이 된다. 성능 최적화: 캐싱, 지연 시간(Latency), 처리량(Throughput) 같은 개념을 이해하면 버퍼 크기와 관리 방식을 최적화할 수 있다. 더 알아보기 제로 카피(Zero-Copy): 버퍼 복사를 최소화해 성능을 높이는 기법. 비동기 I/O: 비동기 프로그래밍에서 버퍼가 어떻게 활용되는지. 실시간 시스템: 낮은 지연 시간을 요구하는 환경에서 버퍼 관리 방법. 버퍼 풀(Buffer Pool): 데이터베이스에서 다수의 버퍼를 관리하는 방식. ","date":"2025-04-11T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-11-cs-what-is-buffer/","title":"Buffer란 무엇인가?"},{"content":"📌개요 추상화는 복잡한 시스템에서 핵심적인 개념이나 기능을 추출하여 간단하게 표현하는 것을 말한다. 객체지향 프로그래밍에서는 불피룡한 세부 사항을 숨기고 필요한 부분만 표현하는 기법이다.\n📌내용 클래스가 이미 추상화된 개념인데 추상 클래스? 클래스 자체가 추상화된 개념이지만, 추상 클래스는 더 높은 수준의 추상화를 제공한다.\n일반 클래스 vs 추상 클래스 일반 클래스: 구체적인 구현을 포함하며 인스턴스화 가능 추상 클래스: 부분적으로 구현된 상태로, 하위 클래스에서 완성해야 하는 추상 메서드를 가짐 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 // 일반 클래스 - 구체적인 구현 class Dog { String name; void bark() { System.out.println(name + \u0026#34; says: Woof!\u0026#34;); } } // 추상 클래스 - 부분적 구현 + 추상화 abstract class Canine { String name; void breath() { System.out.println(name + \u0026#34; is breathing.\u0026#34;); } abstract void makeSound(); // 하위 클래스에서 구현 필요 } class Wolf extends Canine { @Override void makeSound() { System.out.println(name + \u0026#34; says: Howl!\u0026#34;); } } // 사용 예 public class Main { public static void main(String[] args) { Dog myDog = new Dog(); myDog.name = \u0026#34;Buddy\u0026#34;; myDog.bark(); // Canine myCanine = new Canine(); // 컴파일 에러 - 추상 클래스는 인스턴스화 불가 Wolf myWolf = new Wolf(); myWolf.name = \u0026#34;Ghost\u0026#34;; myWolf.breath(); myWolf.makeSound(); } } 추상화의 필요성 복잡성 감소: 시스템의 복잡도를 낮춰 이해하기 쉽게 만든다. 재사용성 증가: 공통적인 특성을 추출해 여러 곳에서 재사용할 수 있다. 유지보수 용이: 변경이 필요한 경우 추상화된 부분만 수정하면 된다. 포준화: 인터페이스를 통해 표준화된 접근 방식을 제공한다. 행위 중심 추상화 vs 데이터 중심 추상화 OOP에서 중요한 건 객체와 객체 간의 상호 작용을 설계하는 것. 따라서 필드(데이터) 보다는 메서드(행위)를 중점으로 추상화 기법을 적용하여 객체를 설계하는 것이 중요하다.\n그러나 필요에 따라 데이터를 중심으로 추상화 하여 객체 및 클래스를 설계하는 경우도 존재한다. 대표적으로 DTO(Data Transfer Object) 같은 클래스가 있다.\n구분 설명 예시 행위 중심 추상화 객체가 수행할 **기능(행동)**에 초점 interface Runnable { void run(); } 데이터 중심 추상화 객체가 가진 **속성(데이터)**에 초점 abstract class Shape { int x, y; } 추상 클래스와 인터페이스 공통점 인스턴스 생성 불가: 직접 객체를 생성할 수 없다. 추상 메서드 포함: 구현되지 않은 메서드를 가질 수 있다. 다형성 지원: 상속/구현을 통해 다형성을 제공한다. 계층 구조 형성: 클래스들의 관계를 구조화한다. 차이점 추상 클래스는 인스턴스를 생성할 수 없는데 왜 생성자를 가질 수 있을까?\n생성자의 존재는 \u0026ldquo;너는 이 클래스를 직접 쓰는 게 아니라, 하위 클래스를 만들 때 이 규칙으로 초기화해야 해\u0026quot;라는 의미로 해석할 수 있다. 특징 추상 클래스 인터페이스 키워드 abstract class interface 상속 단일 상속만 가능 다중 구현 가능 변수 인스턴스 변수, 상수 모두 가능 상수만 가능 (public static final) 메서드 추상/구현 메서드 모두 가능 Java 8 이전: 모두 추상 메서드\nJava 8+: default, static 메서드 가능 생성자 가질 수 있음 가질 수 없음 접근 제어자 다양한 접근 제어자 사용 가능 기본적으로 public 사용 목적 관련 있는 클래스들의 공통점 정의 다른 계층의 클래스들에 공통 기능 정의 잘못된 추상화의 예시와 개선 방법 불필요한 추상화 1 2 3 4 5 6 7 8 9 10 11 12 // 너무 구체적인 것을 추상화한 경우 abstract class DatabaseConnector { abstract void connectToMySQL(); abstract void connectToPostgreSQL(); abstract void connectToOracle(); } // 개선 방법: 공통적인 연결 개념으로 추상화 abstract class DatabaseConnector { abstract void connect(String url, String username, String password); abstract void disconnect(); } 언제 사용하지? 관련 클래스들 사이의 공통점을 추출할 때 일부 메서드는 구현하고, 일부는 하위 클래스에 위임할 때 템플릿 메서드 패턴 구현 시 계층 구조를 명확히 표현하고 싶을 때 🎯결론 Java의 추상화 개념, 추상 클래스와 인터페이스의 적절한 사용법, 그리고 잘못된 추상화의 예와 개선 방법을 알아보았다.\n인터페이스가 서비스의 최소 규약인 것처럼, 추상 클래스는 클래스의 골격이 되고 클래스는 생성될 객체의 청사진이라고 이해할 수 있겠다.\n⚙️EndNote 추가적으로 찾아보면 좋을 것들을 더 적어본다..\n디자인 패턴에서의 추상화 활용 팩토리 메서드 패턴 템플릿 메서드 패턴 브리지 패턴 SOLID 원칙과의 연관성 단일 책임 원칙(SRP) 추상화를 통해 책임 분리 유도 개방-폐쇄 원칙(OCP) 확장에는 열려 있고 변경에는 닫힌 설계 의존 역전 원칙(DIP) 고수준 모듈이 저수준 모듈에 의존하지 않도록 추상화 계층 도입 실무 적용 시 고려사항 추상화의 적정선 YAGNI 원칙 (You Ain\u0026rsquo;t Gonna Need It): 과도한 추상화는 복잡도 증가 계층 깊이 vs 유연성 트레이드오프 성능 영향 가상 메서드 테이블(VMT) 오버헤드 (극미량이지만 고성능 시스템에서 고려) 테스트 용이성 추상 클래스는 Mocking이 어려울 수 있음 @Spy 사용 사례 ","date":"2025-04-10T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-10-java-the-concept-and-necessity-of-abstraction/","title":"Java 객체지향: 추상 클래스"},{"content":"📌개요 Java의 Primitive Type(기본형)과 Wrapper Class(래퍼 클래스)는 본질적으로 같은 데이터를 다루지만, 존재 목적과 사용 방식에서 근본적인 차이가 있다.\n📌내용 기본 개념 Primitive Type 기본형 Java에서 제공하는 가장 기본적인 데이터 타입 byte, short, int, long, float, double, char, boolean 8가지 존재 스택(Stack) 메모리에 직접 값 저장 null 값을 가질 수 없음 산술 연산 가능 기본 값 존재 (예: int는 0, boolean은 false) Wrapper Class 래퍼 클래스 Primitive type을 객체로 감싸는 클래스 Byte, Short, Integer, Long, Float, Double, Character, Boolean 힙(Heap) 메모리에 저장 null 값 허용 다양한 유틸리티 메서드 제공 주요 차이점 특징 Primitive Type Wrapper Class 저장 위치 Stack Heap null 허용 불가 가능 메모리 사용량 적음 많음 접근 속도 빠름 상대적으로 느림 기본값 존재 있음 없음(null) 유틸리티 메서드 없음 있음 Collection 요소로 사용 불가 가능 Boxing \u0026amp; Unboxing 오토 박싱 Autoboxing 원시 타입 → Wrapper 클래스로 자동 변환 컴파일러가 내부적으로 Integer.valueOf()를 호출 1 2 int primitiveInt = 100; Integer autoBoxedInt = primitiveInt; // 오토박싱 (자동) 오토 언박싱 Autounboxing Wrapper 클래스 → 원시 타입으로 자동 변환 컴파일러가 내부적으로 intValue()를 호출 1 2 Integer wrapperInt = 200; int autoUnboxedInt = wrapperInt; // 오토언박싱 (자동) 명시적 박싱 (Explicit Boxing) 직접 Wrapper 클래스의 메서드(valueOf())를 호출 1 2 int primitiveInt = 300; Integer explicitBoxedInt = Integer.valueOf(primitiveInt); // 명시적 박싱 명시적 언박싱 (Explicit Unboxing) 직접 Wrapper 클래스의 메서드(intValue())를 호출 1 2 Integer wrapperInt = 400; int explicitUnboxedInt = wrapperInt.intValue(); // 명시적 언박싱 차이점 정리 구분 예제 변환 방식 오토박싱 Integer a = 100; 컴파일러가 자동 처리 명시적 박싱 Integer b = Integer.valueOf(100); 개발자가 직접 메서드 호출 오토언박싱 int c = wrapperInt; 컴파일러가 자동 처리 명시적 언박싱 int d = wrapperInt.intValue(); 개발자가 직접 메서드 호출 언제 사용하지? Primitive Type 성능이 중요한 경우 null 값이 필요하지 않은 경우 대량의 데이터를 다룰 때 (메모리 효율성) 단순한 산술 연산이 필요한 경우 Wrapper Class null 값이 필요할 때 데이터의 부재를 표현해야 하는 경우 컬렉션(Collection)에 저장해야 할 때 Java 컬렉션 프레임워크는 객체만 저장 가능 예: List\u0026lt;int\u0026gt; 불가, List\u0026lt;Integer\u0026gt; 가능 객체의 메서드를 사용해야 할 때 Integer.parseInt(), Character.isLetter() 등 제네릭 타입으로 사용해야 할 때 \u0026lt;T\u0026gt;에는 객체만 사용 가능 성능 고려사항 Boxing/Unboxing은 추가적인 오버헤드를 발생 시킨다. 반복문 등에서 자주 발생하면 성능 저하 가능성이 있다. 최신 JVM에서는 일부 상황에서 최적화되지만, 불필요한 Boxing/Unboxing은 피해야 한다. 1 2 3 4 5 6 7 8 9 10 11 // 비효율적인 예 (반복적인 boxing/unboxing) Long sum = 0L; for(long i = 0; i \u0026lt; Integer.MAX_VALUE; i++) { sum += i; // 매번 iteration에서 unboxing \u0026amp; boxing 발생 } // 개선된 예 long sum = 0L; for(long i = 0; i \u0026lt; Integer.MAX_VALUE; i++) { sum += i; // primitive 연산만 발생 } 실무에선 언제 사용하지? 의도가 명확한 코드 작성 JPA/Hibernate 엔티티 필드 기본키(ID)는 Wrapper로 선언 (null 가능성) 다른 필드는 상황에 따라 선택 DTO 설계 API 응답에서 값이 없을 수 있는 필드는 Wrapper 사용 메서드 반환 타입 값이 없을 수 있는 경우 Optional\u0026lt;Primitive\u0026gt; 대신 Wrapper 고려 🎯결론 동일한 데이터를 다루지만 메모리 구조, 성능, 사용 목적에서 차이가 있으므로 상황에 맞게 선택해야 한다.\n핵심 원칙:\n성능이 중요하면 Primitive 객체 지향 기능이 필요하거나 null 표현이 필요하면 Wrapper 구분 Primitive Type (기본형) Wrapper Class (래퍼 클래스) 메모리/성능 저장 위치 스택(Stack) 메모리 힙(Heap) 메모리 메모리 사용량 적음 (값 직접 저장) 많음 (객체로 감싸서 저장) 연산 속도 빠름 상대적으로 느림 (객체 접근 오버헤드) 기능/유연성 null 허용 불가능 가능 메서드 지원 없음 다양한 유틸리티 메서드 제공 주요 사용처 적합한 경우 대량 연산, 성능이 중요한 로직\n로컬 변수 컬렉션(Collection) 사용 시\n제네릭 타입 필요 시\nnull 표현이 필요한 필드 ⚙️EndNote 더 알아보기 AutoBoxing 최적화 Integer.valueOf()의 캐싱 매커니즘 Long VS long 반복문 성능 비교 실습 Java 메모리 모델 Stack VS Heap 메모리 동장 방식 Primitive가 스택에 저장되는 이유 Java 컬렉션과 제네릭 왜 컬렉션은 Primitive를 허용하지 않는가? IntStream, Eclipse Collections 등 대안 라이브러리 JPA/Hibernate 매핑 엔티티 필드 타입 선택 가이드 @Column(nullable = false)와 Primitive/Wrapper 관계 Java 8 이후 변화 OptionalInt VS Optional\u0026lt;Integer\u0026gt; 람다식에서의 자동 형변환 동작 ","date":"2025-04-10T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-10-java-primitive-vs-wrapper/","title":"Java 문법: Primitive VS Wrapper"},{"content":"📌개요 Java의 enum(열거형)은 타입 안전성(Type-Safety)과 코드 가독성을 제공하는 고정된 상수 집합 관리 도구다.\n기존의 public static final 상수보다 발전된 형태로, 제한된 선택지를 표현할 때 주로 사용된다.\n핵심 특징 Java 5부터 공식 지원 (J2SE 5.0) 클래스의 확장 기능 제공 (필드, 메서드 추가 가능) Comparable, Serializable 자동 구현 📌내용 enum은 왜 만들어졌는가? 기존 방식의 한계 1 2 3 4 5 public class Status { public static final int ORDERED = 1; public static final int PAID = 2; public static final int DELIVERED = 3; } 문제점:\n타입 안정성 부족: int status = 100; 처럼 유효하지 않은 값 할당 가능 가독성 낮음: if (status == 1) vs if(status == Status.ORDERED) 확장성 부족: 상수에 메서드나 속성을 추가할 수 없음 enum의 해결 방안 1 2 3 public enum Status { ORDERED, PAID, DELIVERED } 컴파일 타입 검증 Status status = Status.ORDERED만 허용 (잘못된 값 컴파일 오류) 명시적 표현으로 코드 가독성 향상 객체처럼 메서드/속성 추가 가능 내부 동작 원리 enum은 컴파일러에 의해 클래스로 변환된다.\njava.lang.Enum 상속 compareTo(), name(), ordinal() 기본 제공 생성자 private 외부 인스턴스화 불가 values(), valueOf() 자동생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public final class Status extends java.lang.Enum\u0026lt;Status\u0026gt; { // Enum 상수 = static final 인스턴스 public static final Status ORDERED = new Status(\u0026#34;ORDERED\u0026#34;, 0); public static final Status PAID = new Status(\u0026#34;PAID\u0026#34;, 1); // ... private static final Status[] $VALUES = { ORDERED, PAID }; private Status(String name, int ordinal) { // 생성자는 private super(name, ordinal); } public static Status[] values() { return $VALUES.clone(); } public static Status valueOf(String name) { /*...*/ } } 언제부터 본격적으로 사용됐을까? Java 5부터 본격 도입 - J2SE 5.0 (Tiger)에서 공식적으로 추가됐다고 한다. 2000년대 중반부터 도메인 모델링에서 enum이 적극 활용되기 시작됐다고 한다. Spring, JPA와 같은 프레임워크에서 enum을 권장한다. (예: @Enumerated(EnumType.STRING)) 어떤 것을 대체할 수 있었나 public static final 상수 interface에 상수 정의(interface Status { int ORDERED = 1; }) Map이나 List로 관리하던 것들 enum을 사용하게 된 결정적 이유? 비교 대상 문제점 enum의 장점 public static final 타입 안전성 X, 가독성 낮음 타입 체크 O, 명시적 이름 interface 상수 구현 클래스가 강제됨 독립적인 타입 Map/List 관리 런타임 오류 가능성 컴파일 타임 검증 🎯결론 enum은 기본 타입이 아니라 Java 컴파일러에 의해 특별히 처리되는 클래스다. public static final 상수의 문제점인 타입 안정성과 가독성을 해결 제한된 선택지를 안전하게 관리하기 위한 특수한 클래스 ⚙️EndNote 성능 최적화 == vs equals()\n1 2 3 Status status = Status.PAID; if (status == Status.PAID) {} // 권장 (빠름) if (status.equals(Status.PAID)) {} // 동작하지만 불필요 더 알아보기 JPA 연동: @Enumerated(EnumType.String) Enum 단점: 새로운 상수 추가 시 모든 switch 검토 필요 Enum 과 싱글톤 관계 참고 자료 Oracle Docs - Enum Types Effective Java Item 34: \u0026ldquo;Use enums instead of int constants\u0026rdquo; ","date":"2025-04-09T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-09-java-when-and-how-to-use-the-enum-type/","title":"Java Enum"},{"content":"📌개요 Git 원격 저장소를 클론할 때 모든 브랜치를 클론하지 않고 필요한 특정 브랜치만 클론하는 방법. git fetch 명령 시 업데이트를 확인할 브랜치를 지정하는 방법. 추가로 Fork한 저장소의 브랜치명이 바뀐 경우를 반영 방법을 알아본다.\n📌내용 저장소 클론 시 특정 브랜치만 저장소 클론 시 특정 브랜치만 추적하고 싶다면 브랜치명과 --single-branch을 사용한다.\n1 git clone -b \u0026lt;브랜치_이름\u0026gt; --single-branch \u0026lt;원격_저장소_URL\u0026gt; 이렇게 하나의 브랜치만 내려 받으면 하나의 브랜치만 추적 관리되고 fetch 명령 시 다른 브랜치의 업데이트는 확인하지 않아 필요한 브랜치만 업데이트 되니 효율적인 것 같다.\n특정 브랜치의 업데이트만 확인 저장소에 브랜치가 너무 많거나 뭐 다른 이유가 있어서 git fetch 명령 실행 시 모든 브랜치를 업데이트하는 것이 아니라 특정 브랜치의 업데이트만 확인하고 싶을 때\n1 2 3 4 5 6 7 8 9 # git remote -v 명령으로 조회한 저장소 이름 origin, upstream 등등 # 해당 명령은 실행마다 덮어씌워지는 점 참고 git remote set-branches \u0026lt;저장소 이름\u0026gt; \u0026lt;브랜치_이름1\u0026gt; \u0026lt;브랜치_이름2\u0026gt; ... # 기존 목록은 유지한채로 특정 브랜치를 더 추가하고 싶다면 git remote set-branches \u0026lt;저장소 이름\u0026gt; \u0026lt;브랜치_이름1\u0026gt; \u0026lt;브랜치_이름2\u0026gt; --add \u0026lt;브랜치_이름3\u0026gt; \u0026lt;브랜치_이름4\u0026gt; ... # 이후 적용 확인 # git remote -v 명령으로 조회한 저장소 이름 origin, upstream 등등 git remote show \u0026lt;저장소 이름\u0026gt; git remote show 명령으로 확인하면 특정 브랜치만 추적 설정된 게 보인다. git remote -r 또는 git remote -a의 경우 이미 추적된 브랜치는 계속 목록이 표시된다. 하지만 git fetch 명령에 가져오는 건 git remote set-branches명령으로 설정한 브랜치만 최신화 된다. 만약 git remote set-branches 설정 후 다른 브랜치의 업데이트도 확인해야 한다면 git fetch --all을 사용한다. 모든 브랜치를 다시 추적하겠다면 와일드 카-드\n1 2 # git remote -v 명령으로 조회한 저장소 이름 origin, upstream 등등 git remote set-branches \u0026lt;저장소 이름\u0026gt; \u0026#34;*\u0026#34; origin, upstream 저장소에서 삭제된 브랜치 로컬에 반영해서 추적 브랜치 최신화\n1 2 3 git fetch --prune origin # 또는 git fetch --prune upstream upstream 원본 저장소의 변경 Github에서 Fork한 저장소를 로컬에 클론하면 upstream을 등록하게 된다. 그럼 origin, upstream 두 remote를 가진 저장소에서 변경 사항을 어떻게 반영할지 고민해본다.\nGithub에서 포크한 저장소의 원본 저장소에서 브랜치명 변경이 발생한 케이스에 Pull Request할 때 어떤 브랜치를 어떤 브랜치에 합칠 건지 선택하기 때문에 내가 포크한 저장소의 브랜치는 원본 브랜치와 꼭 동일하게 맞출 필요가 없긴 하다.\n근데 일단 변경 사항과 동일하게 맞춰 보고 싶었다.\n원하는 해결책 내 작업 사항은 유지하면서, 원본 저장소에서 변경된 브랜치를 고대~로 내가 포크한 저장소에 반영하고 싶음\n문제 Github에서 포크한 저장소의 경우 원본 저장소의 변경은 새롭게 받아올 수 있는데, 브랜치명만 바뀐 경우 반영할 수 있는 기능이 없음. Pull Request 걸려 있는 상태임 브랜치명이 변경되면 PR은 자동으로 닫힘 PR 걸린 브랜치는 살려놔도 됨 신속한 해결책 그냥 깔끔하고 신속한 방법으로 맞추겠다 하면\n깔끔하게 새롭게 포크 받기 upstream에 대한 미러 클론을 로컬에 받아서 .git 파일을 기존 저장소에 교체 로컬 브랜치와 충돌이 발생할 수 있음 로컬 커밋 이력이 사라지고 새롭게 포크하는 것과 차이가 없음 복잡한 해결책 Github에서 브랜치 이름 수정 정성스럽게 각 브랜치의 이름을 변경한다. 원본 저장소의 브랜치 변경을 일괄 적용 내가 작업한 브랜치만 남기고 모두 정리한 상태로 upstream에 변경된 브랜치들을 내려 받아 포크한 저장소에 반영할 예정\n로컬 저장소의 origin을 upstream 주소로 변경 또는 upstream의 브랜치를 로컬에 생성\n1 git branch -r | awk \u0026#39;{print $1}\u0026#39; | grep -v \u0026#39;^origin/HEAD$\u0026#39; | while read remote; do git branch --track \u0026#34;${remote#origin/}\u0026#34; \u0026#34;$remote\u0026#34;; done 모든 브랜치를 받아온 뒤 다시 origin 복구 이후 origin에 모든 브랜치 업로드\n1 2 3 git branch | grep -v \u0026#39;^*\u0026#39; | while read branch; do git push origin \u0026#34;$branch\u0026#34;; done 마지막으로 필요한 브랜치만 남기고 모두 삭제\n1 2 # main, develop, master, 특정브랜치이름 만 남기고 삭제 git branch | grep -v \u0026#39;main\\|develop\\|master\\|특정브랜치이름\u0026#39; | grep -v \u0026#39;^*\u0026#39; | xargs git branch -D ⚙️EndNote 물리 파일 수정 권장되지 않는 방법이지만 빠르게 반영되고 좋다. .git 폴더 아래 필요한 파일들을 삭제하거나 수정한다.\n","date":"2025-04-08T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-08-git-reflect-changes-to-fork-repository/","title":"Git 추적 브랜치 관리"},{"content":"📌개요 사용자가 브라우저에 URL을 입력하고 엔터를 누르면, 요청한 웹페이지를 가져와 화면에 표시하기까지 복잡한 일련의 과정이 진행된다.\n이 과정은 여러 시스템, 프로토콜, 그리고 인터넷 스택의 다양한 계층을 포함한다. 엔지니어에게 이 과정을 이해하는 것은 성능 최적화, 보안 강화, 문제 해결에 필수적이다.\n이 과정을 기술적으로 상세히 알아보자.\n📌내용 네트워킹 브라우저는 언제 URL을 분석해서 요청을 처리할 방법을 결정한다.\nURL의 구성 요소 (예: https://www.example.com:443/path?query=value#fragment) 스키마/프로토콜: https는 사용할 프로토콜(HTTP/HTTPS)을 나타낸다. 도메인: www.example.com은 서버를 지정한다. 포트: :443(선택 사항 HTTP는 기본 80, HTTPS는 443) 경로: /path는 요청할 리소스를 식별한다. 쿼리: ?query=value는 추가 매개변수를 제공한다. 프래그먼트: #fragment는 페이지 내 특정 섹션을 가리킨다. 유효성 검사: 브라우저는 URL의 문법이 올바른지 확인한다. 잘못된 경우 검색 쿼리로 처리할 수 있다. HSTS 확인 HTTPS 연결을 위해 브라우저는 HSTS(HTTP Strict Transport Security) 목록을 확인하여 도메인이 보안 연결을 요구하는지 확인한다. 인코딩 특수 문자는 URL 인코딩 (예: 공백을 %20으로 변환)을 통해 처리된다. DNS 조회 도메인 이름을 실제 서버의 IP 주소로 변환하는 과정.\n로컬 캐시 확인 브라우저와 OS는 먼저 로컬 DNS 캐시를 확인한다. 예: 브라우저 캐시, /etc/hosts 파일 재귀적 DNS 쿼리 캐시에 없으면 시스템은 ISP의 DNS 리졸버 또는 공용 DNS (예: 8.8.8.8)에 쿼리를 보낸다. ISP: Internet Service Provider, DNS: Domain Name System 리졸버는 루트 DNS 서버, TLD(최상위 도메인, .com) 서버, 권한 있는 네임 서버를 순차적으로 질의하여 IP 주소를 얻는다. TLD: Top-Level Domain DNS 레코드 A 레코드(IPv4 주소) 또는 AAAA 레코드(IPv6 주소)를 반환. CNAME 레코드가 있으면 추가 조회가 필요할 수 있다. DNS 캐싱 조회된 IP는 TTL에 따라 로컬에 캐싱되어 이후 요청을 가속화한다. TTL: Time To Live DNS 보안 DNSSEC을 사용하면 응답의 무결성을 보장한다. DNSSEC: DNS Security Extensions TCP 연결 브라우저는 서버와 안정적인 연결을 설정하기 위해 TCP 프로토콜을 사용한다. TCP: Transmission Control Protocol (전송 제어 프로토콜)\n3 Way 핸드셰이크 클라이언트가 SYN 패킷을 서버로 전송 서버가 SYN-ACK로 응답 클라이언트가 ACK로 보내 연결을 완료 SYN: SYNchronization (동기화), ACK: ACKnowledgement (확인) 소켓 생성 클라이언트와 서버는 각각 소켓을 열어 데이터를 주고 받을 준비를 한다. 지연 요소 네트워크 레이턴시와 패킷 손실은 연결 시간을 늘릴 수 있다. TCP Slow Start는 초기 전송 속도를 조절한다. Keep-Alive: HTTP/1.1부터는 연결을 재사용하여 오버헤드를 줄인다. TLS 핸드셰이크 (HTTPS) HTTPS 요청의 경우 데이터 보안을 위해 TLS(Transport Layer Security) 연결을 설정한다.\nTLS 협상 클라이언트는 ClientHello 메시지로 지원하는 암호화 스위트와 TLS 버전을 전송. 서버는 SeverHello로 선택한 암호화 방식과 인증서를 응답 인증서 검증 클라이언트는 서버의 인증서를 CA로 검증 CA: Certificate Authority 인증서의 도메인 일치 여부와 유효 기간을 확인 키 교환 Diffie-Hellman 또는 RSA를 사용해 세션 키를 생성 이후 데이터는 대칭 암호화(예: AES)로 보호 성능 고려사항 TLS 1.3은 핸드셰이크를 단순화하여 지연을 줄임 세션 재개는 이전 연결의 캐시를 활용 HTTP 통신 HTTP 요청 브라우저는 서버에 HTTP 요청을 보내 리소스를 요청한다.\n요청 구성 메서드: GET, POST 등. 헤더: Host, User-Agent, Accept, Cookie 등. 바디: POST 요청 시 데이터 포함 (예: JSON, Form 데이터). HTTP/2 및 HTTP/3 HTTP/2는 멀티플렉싱과 헤더 압축을 지원 HTTP/3는 UDP 기반 QUIC를 사용하여 성능을 개선 프록시 및 CDN 요청은 프록시 서버나 CDN(예: Cloudflare)을 거칠 수 있음 CDN은 캐싱된 콘텐츠를 제공해 지연을 줄임 서버 응답 서버는 요청을 처리하고 응답을 반환한다.\n응답 구성 상태 코드 200 OK, 404 Not Found, 301 Moved Permanently 등 헤더 Content-Type, Content-Length, Cache-Control 등 바디 HTML, JSON, 이미지 등의 콘텐츠 리다이렉션 301 또는 302 상태 코드는 브라우저를 다른 URL로 이동시킴 압축 gzip 또는 brotli로 콘텐츠를 압축해 전송 속도를 높임 렌더링 HTML 파싱 및 DOM 구축 브라우저는 서버에서 받은 HTML을 파싱하여 DOM을 생성한다. DOM: Document Object Model\n파싱 과정 HTML은 바이트 스트림에서 토큰으로 분해됨 토큰은 노드로 변환되어 DOM 트리로 조립 오류 처리 잘못된 HTML(예: 닫히지 않은 태그)도 최대한 파싱 비동기 로딩 \u0026lt;script\u0026gt; 태그는 기본적으로 파싱을 차단하나, async 또는 defer 속성으로 최적화 가능 CSS 파싱 및 렌더 트리 CSS는 스타일을 정의하고 렌더 트리를 생성해 화면에 표시할 요소를 결정한다.\nCSSOM 생성 CSS는 CSSOM으로 변환 CSSOM: CSS Object Model 렌더 트리 DOM과 CSSOM을 결합해 보이는 요소만 표함 리플로우 스타일 변경 시 레이아웃을 재계산 자바스크립트 실행 자바스크립트는 동적 콘텐츠를 생성하고 페이지를 조작한다.\n엔진 V8(Chrome), SpiderMonkey(Firefox) 등. 이벤트 루프 비동기 작업(예: setTimeout, AJAX)을 처리. 성능 병목 무거운 스크립트는 렌더링을 지연시킬 수 있음 화면 렌더링 브라우저는 렌더 트리를 기반으로 픽셀을 화면에 그린다.\n레이아웃: 요소의 위치와 크기를 계산 페인팅: 계산된 스타일을 픽셀로 변환 합성: GPU를 활용해 레이어를 합성 최적화: 하드웨어 가속과 캐싱으로 성능 개선 🎯결론 URL 입력부터 화면 표시까지의 과정은 네트워킹(DNS, TCP, TLS, HTTP)과 웹 기술(파싱, 렌더링)의 긴밀한 협력으로 이루어진다.\n각 단계는 성능, 보안, 사용자 경험에 직접적인 영향을 미치며 엔지니어는 이를 이해함으로써 최적화와 문제 해결의 기반을 마련할 수 있다.\n과정 요약 URL 파싱: 브라우저는 URL을 분석하여 프로토콜(예: HTTPS), 도메인, 경로 등을 식별하고 요청 준비를 한다. DNS 조회: 도메인 이름을 IP 주소로 변환하며, 로컬 캐시 또는 DNS 서버를 통해 빠르게 처리된다. TCP 연결: 클라이언트와 서버 간 안정적인 연결을 위해 3Way 핸드셰이크를 수행한다. TLS 핸드셰이크 (HTTPS): 보안 연결을 위해 인증서 검증과 세션 키 교환을 통해 데이터를 암호화 한다. HTTP 요청/응답: 브라우저가 서버에 리소스를 요청하고, 서버는 HTML, CSS, 이미지 등으로 응답한다. 렌더링 HTML을 파싱해 DOM을 구축하고 CSS를 적용해 렌더 트리를 생성 자바스크립트를 실행해 동적 콘텐츠를 처리 레이아웃 계산, 페인팅, 합성을 통해 화면에 페이지를 표시 ⚙️EndNote 사전 지식 OSI 7계층: DNS(TCP/UDP), HTTP, TLS는 응용/전송 계층에서 동작. TCP/IP 모델: 인터넷 프로토콜 스택의 기본 구조 이해. 브라우저 엔진: Webkit, Blink, Gecko의 렌더링 방식 차이. 더 알면 좋을 것들 웹 성능 최적화: Critical Rendering Path, Lazy Loading. 보안: CORS, CSRF, XSS 방지 기법. 모니터링: Lighthouse, Web Vitals로 성능 측정. 프로그레시브 웹 앱(PWA): 오프라인 캐싱과 빠른 로딩. 브라우저 개발자 도구: 네트워크 탭과 성능 분석 활용. 참조 자료 [네트워크] 주소창에 URL을 입력하면 일어나는 일 DNS cache and DNS lookup: What happens from typing in a URL to displaying a website? (Part 1) [네트워크] 브라우저 주소창에 URL을 입력 시 일어나는 일 정리 (DNS) ","date":"2025-04-08T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-08-what-happens-when-you-type-a-url-in-the-browser-and-press-enter/","title":"URL 입력 후 ENTER 키"},{"content":"📌개요 Java Stream API는 Java 8에서 도입된 강력한 데이터 처리 도구로, map()과 flatMap()은 스트림 요소를 변환하는 핵심 연산자이다.\n두 메서드의 차이점을 명확히 구분하고, 실제 코드 예제를 통해 활용 방법을 설명한다. 함수형 프로그래밍 패러다임을 이해하고 복잡한 데이터 구조를 효율적으로 처리하는 방법을 학습할 수 있다.\n📌내용 핵심 개념 비교 특성 map() flatMap() 변환 방식 1:1 변환 1:N 변환 + 평탄화 입/출력 관계 A → B A → [B1, B2, B3] → B1, B2, B3 반환 타입 Stream\u0026lt;R\u0026gt; Stream\u0026lt;R\u0026gt; (내부 스트림 풀어냄) 주요 사용 케이스 단순 값 변환 중첩 컬렉션 처리, 다중 값 생성 예시 문자열 길이 추출 문장을 단어로 분해 동작 방식 map() map()은 각 원소를 1:1로 변환하여 원본 배열과 동일한 길이의 새 배열을 반환한다.\n단일 속성 추출, 단순 타입 변환 등 단순 계산 1:1 매핑이 보장된 경우 1 2 3 4 // 대소문자 변환 List\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;java\u0026#34;, \u0026#34;stream\u0026#34;); List\u0026lt;String\u0026gt; upperNames = names.stream().map(String::toUpperCase).collect(Collectors.toList()); // 결과: [\u0026#34;JAVA\u0026#34;, \u0026#34;STREAM\u0026#34;] flatMap() flatMap()은 1:N 분해 후 평탄화로 길이를 가변적으로 만든다.\n컬렉션 풀기, 문자열 토큰화, Optional 체이닝 등 하나의 입력이 여러 출력을 생성해야 할 때 1 2 3 4 5 6 7 8 9 10 11 12 // 다중 리스트 평탄화 List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; numberLists = Arrays.asList( Arrays.asList(1, 2), Arrays.asList(3, 4, 5) ); List\u0026lt;Integer\u0026gt; allNumbers = numberLists.stream().flatMap(List::stream).collect(Collectors.toList()); // 결과: [1, 2, 3, 4, 5] // 문자열 토큰화 List\u0026lt;String\u0026gt; sentences = Arrays.asList(\u0026#34;Hello World\u0026#34;, \u0026#34;Java Stream\u0026#34;); List\u0026lt;String\u0026gt; words = sentences.stream().flatMap(s -\u0026gt; Arrays.stream(s.split(\u0026#34; \u0026#34;))).collect(Collectors.toList()); // 결과: [\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;, \u0026#34;Java\u0026#34;, \u0026#34;Stream\u0026#34;] 동작 방식 비교 연산자 입/출력 과정 길이 변화 map() [A, B] → [f(A), f(B)] 유지 (n → n) flatMap() [A, B] → [A1, A2, ...] + [B1, B2, ...] → [A1, A2, ..., B1, B2, ...] 가변 (n → m) 🎯결론 flatMap()이 더 강력하지만, map()으로 해결 가능한 작업에는 map()을 사용하자. 중첩 구조 분해나 1:N 변환이 필요할 때만 flatMap()을 선택하는 것이 성능과 가독성 면에서 유리할 것이다.\nmap() 단일 속성 추출, 단순 타입 변환 등 단순 계산 1:1 매핑이 보장된 경우 성능이 더 중요한 경우 flatMap() 컬렉션 풀기, 문자열 토큰화, Optional 체이닝 등 하나의 입력이 여러 출력을 생성해야 할 때 내부 스트림 생성 오버헤드 존재 가독성 vs 기능 단순 변환은 map()이 더 직관적 복잡한 분해 로직은 flatMap()이 유리 대용량 데이터 처리 시 map().flatMap() 분리 고려 ⚙️EndNote 필요한 사전 지식 람다 표현식: (x) -\u0026gt; x*2 같은 간결한 함수 정의 방식 함수형 인터페이스: Function\u0026lt;T, R\u0026gt;, Predicate\u0026lt;T\u0026gt; 등 더 알아보기 성능 고려사항 flatMap()은 중간 스트림 생성 오버헤드가 있을 수 있다. 병렬 스트림(parallelStream())에서의 동작 차이 Optional의 flatMap() Optional 클래스에서도 동일한 개념 적용 가능 1 Optional.of(\u0026#34;hello\u0026#34;).flatMap(s -\u0026gt; Optional.of(s.length())); Reactivestreams와의 연관성 RxJava/Project Reactor에서도 유사한 연산자 사용 ","date":"2025-04-07T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-07-java-streamapi-map-and-flatmap/","title":"Java Stream API"},{"content":"📌개요 한 언어에만 국한되는 내용이 아닌 객체지향 프로그래밍의 핵심 설계 방식이다. SOLID 원칙은 2000년대 초반 로버트 C. 마틴(Robert C. Martin) - \u0026lsquo;엉클 밥(Uncle Bob)\u0026rsquo; 미국의 소프트웨어 공학자가 정리했지만, 그 기원은 1980년대 후반부터 시작된 객체 지향 설계 연구에 뿌리를 두고 있다.\nSOLID 원칙이 핵심 원칙이라 불리는 이유? 변경 비용 감소: 기능 추가/수정 시 영향 범위 최소화 시스템 수명 연장: 유지보수성 향상으로 기술 부채 감소 팀 협업 효율화: 코드 가독성과 예측 가능성 증가 그 중 유지보수성과 확장성이 높은 소프트웨어를 설계하는 데 필수적인 SRP, OCP 두 가지 원칙을 알아본다.\n📌내용 SRP (Single Responsibility Principle) - 단일 책임 원칙 정의: 한 클래스는 하나의 책임만 가져야 한다. \u0026ldquo;한 우물만 파라!\u0026rdquo;\n하나의 클래스는 오직 하나의 이유로만 변경되어야 한다. 책임 = 변경 이유 응집도 ↑, 유지보수성 ↑ 잘못된 예 문제 상황: 뭐든 다 하는 만능 클래스\nUser 클래스가 두 가지 책임(사용자 정보 관리 + 데이터베이스 작업)을 동시에 가진다. 만약 데이터베이스 로직이 변경되면 User 클래스도 수정해야 한다. 사용자 정보의 필드 변경 시에도 User 클래스를 수정해야 하므로 변경의 이유가 두 가지가 된다. 1 2 3 4 5 6 7 8 9 10 11 12 class User { private String name; private String email; // 사용자 정보 관련 책임 public String getName() { return name; } public void setName(String name) { this.name = name; } // 데이터베이스 관련 책임 (SRP 위반) public void saveUserToDatabase(User user) { /* ... */ } public User getUserFromDatabase(String userId) { /* ... */ } } 올바른 예 해결책: 전문가를 따로 둡시다\nUser 클래스는 사용자 정보 관리만 담당한다. UserRepository 클래스는 데이터베이스 작업만 담당한다. 데이터베이스 로직이 변경되어도 User 클래스는 영향을 받지 않으며 반대의 경우도 마찬가지다. 1 2 3 4 5 6 7 8 9 10 11 12 class User { private String name; private String email; public String getName() { return name; } public void setName(String name) { this.name = name; } } class UserRepository { public void saveUserToDatabase(User user) { /* ... */ } public User getUserFromDatabase(String userId) { /* ... */ } } OCP (Open/Closed Principle) - 개방/폐쇄 원칙 정의: 소프트웨어 개체는 확장에는 열려 있어야 하고, 수정에는 닫혀 있어야 한다. \u0026ldquo;확장은 쉽게, 수정은 최소로!\u0026rdquo;\n기존 코드를 변경하지 않고도 시스템의 기능을 확장할 수 있어야 한다. 이는 추상화와 다형성을 통해 구현된다. 확장성 ↑, 기존 코드 안정성 ↑ 잘못된 예 문제 상황: 수정이 불가피한 코드\n새 보고서 형식(예: Excel) 추가 시 기존 코드 수정 불가피 한 형식의 버그 수정이 다른 형식에 영향 줄 수 있음 1 2 3 4 5 6 7 8 9 10 class ReportGenerator { public void generate(String type) { if (\u0026#34;PDF\u0026#34;.equals(type)) { // PDF 생성 100줄 코드... } else if (\u0026#34;CSV\u0026#34;.equals(type)) { // CSV 생성 80줄 코드... } // 새 형식 추가할 때마다 이 클래스를 수정해야 함 } } 올바른 예 해결책: 확장 가능한 구조\n새 보고서 형식(예: ExcelReport) 추가 시 기존 코드 변경 불필요 각 형식의 코드가 독립적, 유지보수 용이 실행 시점에 보고서 형식 결정 가능 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // 모든 보고서가 구현할 인터페이스 interface Reportable { byte[] generate(); } // PDF 전문 생성기 class PdfReport implements Reportable { public byte[] generate() { /* PDF 전용 로직 */ } } // CSV 전문 생성기 class CsvReport implements Reportable { public byte[] generate() { /* CSV 전용 로직 */ } } // 확장 가능한 보고서 생성기 class ReportGenerator { public byte[] createReport(Reportable reporter) { return reporter.generate(); // 어떤 형식이든 동일한 방식으로 처리 } } 🎯결론 SRP로 잘 분리된 코드는 OCP 적용이 자연스럽다.\nOCP를 구현하려면 SRP가 먼저 지켜져야 한다.\nSRP 적용 팁:\n클래스 설명 시 \u0026ldquo;~와 ~를 한다\u0026quot;고 말하게 된다면 경고 신호 하나의 기능 변경이 다른 기능의 테스트를 깨트린다면 리팩토링 필요 OCP 적용 팁:\nif-else/switch-case가 자주 등장하면 OCP 위반 의심 새로운 기능을 추가할 때 기존 코드를 건드리지 않아야 한다. \u0026ldquo;처음부터 완벽하게 만들려고 하지 마세요. 하지만 변경이 필요할 때마다 조금씩 개선해나가세요\u0026rdquo; - 마틴 파울러\n⚙️EndNote 단일 책임 원칙 원문: [The Single Responsibility Principle - by Robert C. Martin (Uncle Bob)](https://8thlight.com/blog/uncle-bob/2014/05/08/SingleReponsibilityPrinciple.html](https://8thlight.com/blog/uncle-bob/2014/05/08/SingleReponsibilityPrinciple.html)\n1972년에 데이비드 파나스가 내놓은 고전적인 논문 “Criteria To Be used in decomposing systems into modules”. 이 논문은 Communications of the ACM Vol 15 No 12에 게제되었습니다.\n이 논문에서 파나스는 간단한 알고리즘을 분해(Decompose)하고 분리(Seprate)하는 두가지 전략을 비교하고 있습니다. 이 논문은 매우 흥미롭게 읽을수 있기 때문에 읽어 보는 것을 강력 추천합니다.\n출처: narusas\u0026rsquo;s blog\n개방/폐쇄 원칙 이 승(Seung Lee)의 Ocp PPT\n","date":"2025-04-07T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-07-java-solid-srp-and-ocp/","title":"SOLID 원칙"},{"content":"📌개요 한 언어에만 국한되는 내용이 아닌 객체지향 프로그래밍의 핵심 설계 방식이다. SOLID 원칙은 2000년대 초반 로버트 C. 마틴(Robert C. Martin) - \u0026lsquo;엉클 밥(Uncle Bob)\u0026rsquo; 미국의 소프트웨어 공학자가 정리했지만, 그 기원은 1980년대 후반부터 시작된 객체 지향 설계 연구에 뿌리를 두고 있다.\nSOLID 원칙이 핵심 원칙이라 불리는 이유? 변경 비용 감소: 기능 추가/수정 시 영향 범위 최소화 시스템 수명 연장: 유지보수성 향상으로 기술 부채 감소 팀 협업 효율화: 코드 가독성과 예측 가능성 증가 각 원칙에 대한 정의와 그 의미를 알아보고 간단한 Java 코드로 예시시를 확인해본다.\n📌내용 SRP (Single Responsibility Principle) - 단일 책임 원칙 정의: 한 클래스는 하나의 책임만 가져야 한다. 클래스를 변경하는 이유는 단 하나여야 한다는 원칙으로, 여러 책임이 있는 클래스는 변경이 필요할 때마다 영향을 받을 가능성이 높다.\n하나의 책임을 갖는다는 건 변경의 이유도 하나를 갖는다는 의미가 된다.\n잘못된 예 User 클래스가 두 가지 책임(사용자 정보 관리 + 데이터베이스 작업)을 동시에 가진다. 만약 데이터베이스 로직이 변경되면 User 클래스도 수정해야 한다. 사용자 정보의 필드 변경 시에도 User 클래스를 수정해야 하므로 변경의 이유가 두 가지가 된다. 1 2 3 4 5 6 7 8 9 10 11 12 class User { private String name; private String email; // 사용자 정보 관련 책임 public String getName() { return name; } public void setName(String name) { this.name = name; } // 데이터베이스 관련 책임 (SRP 위반) public void saveUserToDatabase(User user) { /* ... */ } public User getUserFromDatabase(String userId) { /* ... */ } } 올바른 예 User 클래스는 사용자 정보 관리만 담당한다. UserRepository 클래스는 데이터베이스 작업만 담당한다. 데이터베이스 로직이 변경되어도 User 클래스는 영향을 받지 않으며 반대의 경우도 마찬가지다. 1 2 3 4 5 6 7 8 9 10 11 12 class User { private String name; private String email; public String getName() { return name; } public void setName(String name) { this.name = name; } } class UserRepository { public void saveUserToDatabase(User user) { /* ... */ } public User getUserFromDatabase(String userId) { /* ... */ } } OCP (Open/Closed Principle) - 개방/폐쇄 원칙 정의: 소프트웨어 개체는 확장에는 열려 있어야 하고, 수정에는 닫혀 있어야 한다. 기존 코드를 변경하지 않고도 시스템의 기능을 확장할 수 있어야 하며, 이는 추상화와 다형성을 통해 구현된다.\n잘못된 예 수정에 닫혀 있지 않음 새로운 도형(예: 삼각형)이 추가될 때마다 AreaCalcurator 클래스의 calculateArea() 메서드를 계속 수정해야 한다. 확장 시 if-else 블럭을 추가해야 되는 안티패턴은 유지보수성이 떨어지고, 기존 코드의 안정성이 위협 받는다. 확장에 열려 있지 않음 새로운 기능을 추가하려면 기존 클래스의 로직을 직접 변경해야 한다. 1 2 3 4 5 6 7 8 9 10 11 12 class AreaCalculator { public double calculateArea(Object shape) { if(shape instanceof Circle) { Circle circle = (Circle) shape; return Math.PI * circle.radius * circle.radius; } else if (shape instanceof Rectangle) { Rectangle rect = (Rectangle) shape; return rect.width * rect.height; } throw new IllegalArgumentException(\u0026#34;Unknown shape\u0026#34;); } } 올바른 예 추상화 도입 모든 도형이 calculateArea() 메서드를 구현하도록 강제한다. 새로운 도형이 추가되어도 AreaCalculator는 변경되지 않는다. 다형성 활용 AreaCalculator는 구체적인 도형 클래스를 알 필요 없이 인터페이스에 의존한다. 도형의 종류가 늘어나도 calculateArea() 메서드는 한 번만 구현하면 된다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 interface Shape { double calculateArea(); } class Circle implements Shape { public double radius; @Override public double calculateArea() { return Math.PI * radius * radius; } } class Rectangle implements Shape { public double width; public double height; @Override public double calculateArea() { return width * height; } } class AreaCalculator { public double calculateArea(Shape shape) { return shape.calculateArea(); } } LSP(Liskov Subtitution Principle) - 리스코프 치환 원칙 정의: 프로그램의 객체는 프로그램의 정확성을 깨뜨리지 않으면서 하위 타입의 인스턴스로 바꿀 수 있어야 한다. 부모 클래스가 사용되는 모든 곳에서 자식 클래스를 안전하게 사용할 수 있어야 하며 이때 프로그램의 정확성이 깨지지 않아야 한다.\n즉, 클라이언트가 상위 타입에 기대하는 동작을 하위 타입에서도 동일하게 제공해야 한다.\n계약적 설계 개념:\n사전 조건: 메서드 실행 전 만족해야 하는 조건 사후 조건: 메서드 실행 후 보장되는 조건 불변 조건: 객체 생명주기 동안 유지되는 조건 LSP의 3가지 핵심 조건:\n메서드 시그니처 호환성: 하위 클래스는 상위 클래스의 모든 메서드를 동일한 시그니처로 구현해야 한다. 사전 조건 약화: 하위 클래스의 메서드 사전 조건(입력 제약)은 상위 클래스보다 강하지 않아야 한다. 사후 조건 강화: 하위 클래스의 메서드 사후 조건(출력 보장)은 상위 클래스보다 약하지 않아야 한다. 잘못된 예 계약 위반: Bird의 fly()는 \u0026ldquo;날 수 있다\u0026quot;는 행동을 보장하지만, Penguin은 이를 부정하며 예외를 던진다. 클라이언트 충격: watchFlight()은 모든 Bird가 날 것이라 기대하지만, 펭귄 전달 시 시스템이 비정상 종료된다. 일상적 직관과 충돌: 생물학적으로 펭귄은 새이지만, 프로그래밍에서는 상속 관계가 적합하지 않다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // 부모 클래스: 새 class Bird { public void fly() { System.out.println(\u0026#34;날개짓 하며 날아간덩\u0026#34;); } public void eat() { System.out.println(\u0026#34;먹이를 먹는덩\u0026#34;); } } // 자식 클래스: 펭귄 (LSP 위반) class Penguin extends Bird { @Override public void fly() { throw new UnsupportedOperationException(\u0026#34;펭귄은 날 수 없덩\u0026#34;); } } // 클라이언트 코드 class BirdWatcher { public void watchFlight(Bird bird) { bird.fly(); // 펭귄 전달 시 예외 발생 } public static void main(String[] args) { Bird bird = new Penguin(); new BirdWatcher().watchFlight(bird); // 런타임 예외! } } 올바른 예 생물학적 분류 != 프로그래밍적 상속 \u0026ldquo;is-a\u0026rdquo; 관계가 아닌 경우 상속 금지 상속 대신 확장 포인트 제공 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 interface Bird { void eat(); } interface Flyable { void fly(); } class Sparrow implements Bird, Flyable { public void fly() { System.out.println(\u0026#34;날개짓 하며 날아간덩\u0026#34;); } public void eat() { System.out.println(\u0026#34;먹이를 먹는덩\u0026#34;); } } class Penguin implements Bird { public void eat() { System.out.println(\u0026#34;먹이를 먹는덩\u0026#34;); } // fly() 메서드 없음 } // 클라이언트 코드 class BirdWatcher { // 날 수 있는 새만 처리 public void watchFlight(FlyingBird bird) { bird.fly(); // 펭귄은 컴파일 타임에 전달 불가 } // 모든 새 처리 public void watchFeeding(Bird bird) { bird.eat(); // 펭귄도 안전하게 호출 } } ISP (Interface Segregation Principle) - 인터페이스 분리 원칙 정의: 특정 클라이언트를 위한 인터페이스 여러 개가 범용 인터페이스 하나보다 낫다. 클라이언트가 자신이 사용하지 않는 메서드에 의존하지 않아야 한다. ISP는 SRP의 인터페이스 버전이라고 볼 수 있다.\n잘못된 예 불필요한 의존성 RobotWorker는 eat(), sleep() 메서드를 전혀 사용하지 않지만 구현해야 함 더미 코드나 예외 발생으로 처리해야 하는 문제 계약 위반 Worker 인터페이스가 너무 많은 책임을 가진다. 새로운 기능(예: charge()) 추가 시 모든 클래스가 영향 받음 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 interface Worker { void work(); void eat(); void sleep(); } class HumanWorker implements Worker { public void work() { /* 일하기 */ } public void eat() { /* 먹기 */ } public void sleep() { /* 자기 */ } } class RobotWorker implements Worker { public void work() { /* 일하기 */ } public void eat() { /* 로봇은 먹지 않는데 구현해야 함 */ } public void sleep() { /* 로봇은 자지 않는데 구현해야 함 */ } } 올바른 예 명확한 계약 각 인터페이스는 단일 기능만 정의 RobotWorker는 work()만 구현하면 된다. 유연한 확장 새로운 기능(Rechargeable) 추가 시 기존 코드 수정 불필요 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 interface Workable { void work(); } interface Eatable { void eat(); } interface Sleepable { void sleep(); } class HumanWorker implements Workable, Eatable, Sleepable { public void work() { /* 일하기 */ } public void eat() { /* 먹기 */ } public void sleep() { /* 자기 */ } } class RobotWorker implements Workable { public void work() { /* 일하기 */ } } DIP (Dependency Inversion Principle) - 의존관계 역전 원칙 정의: 고수준 모듈은 저수준 모듈에 의존해서는 안 된다. 둘 다 추상화에 의존해야 한다. 구체적인 구현이 아닌 추상화에 의존해야 한다.\n잘못된 예 고수준 모듈이 저수준 모듈에 직접 의존 강한 결합도 Switch는 LightBulb에 강하게 결합되어 다른 기기 추가가 불가능 전구 구현 변경 시 Switch도 수정 필요 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class LightBulb { public void turnOn() { System.out.println(\u0026#34;전구 켜짐\u0026#34;); } public void turnOff() { System.out.println(\u0026#34;전구 꺼짐\u0026#34;); } } class Switch { private LightBulb bulb; // 문제 1: 구체 클래스에 직접 의존 public Switch(LightBulb bulb) { this.bulb = bulb; // 문제 2: 생성자 주입도 구체 클래스 사용 } public void operate() { if (Math.random() \u0026gt; 0.5) { bulb.turnOn(); // 문제 3: 고수준 모듈이 저수준 구현을 직접 호출 } else { bulb.turnOff(); } } } 올바른 예 결합도 감소 Switch는 이제 LightBulb, Fan 등 어떤 Switchable 기기와도 작동 계층 구조 역전 DIP 적용 전: Switch(고수준) → LightBulb(저수준) DIP 적용 후: Switch(고수준) ← Switchable(추상화) → LightBulb(저수준) 실제 적용 사례 Spring Framework의 @Autowired 로깅에서 LoggerInterface 사용 (실제 로거 구현과 분리) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // 추상화 인터페이스 정의 (고수준) interface Switchable { void turnOn(); void turnOff(); } // 저수준 모듈들이 추상화에 의존 class LightBulb implements Switchable { public void turnOn() { System.out.println(\u0026#34;LED 전구 켜짐\u0026#34;); } public void turnOff() { System.out.println(\u0026#34;LED 전구 꺼짐\u0026#34;); } } class Fan implements Switchable { public void turnOn() { System.out.println(\u0026#34;팬 작동 시작\u0026#34;); } public void turnOff() { System.out.println(\u0026#34;팬 정지\u0026#34;); } } // 고수준 모듈이 추상화에 의존 class Switch { private Switchable device; // 추상화에 의존 public Switch(Switchable device) { this.device = device; // DI - Dependency Injection(의존성 주입) } public void operate() { if (Math.random() \u0026gt; 0.5) { device.turnOn(); // 다형성 호출 } else { device.turnOff(); } } } 🎯결론 SOLID 원칙은 서로 연관되어 전체적인 설계 안정성을 이루며, 각 원칙을 종합적으로 적용해야 유연하고 견고한 시스템을 구축할 수 있다.\nSRP가 클래스를 단순하게 유지하면 OCP 적용이 쉬워지고 LSP가 상속 계층을 안정화하면 DIP로 확장하기 용이해지며 ISP는 SRP와 DIP를 자연스럽게 지원하는 선순환 구조를 만든다. SRP (단일 책임 원칙) 핵심 가치: 응집도 ↑, 유지보수성 ↑ 실천 방안: 클래스 설계 시 \u0026ldquo;이 클래스를 수정하는 이유는 단 하나인가?\u0026rdquo; 자문하기 책임이 복잡하면 분할하고, 관련성 높은 기능은 응집시키기 OCP (개방/폐쇄 원칙) 핵심 가치: 확장성 ↑, 기존 코드 안정성 ↑ 실천 방안: 변하는 부분은 추상화 (인터페이스/상속), 변하지 않는 부분은 고정 \u0026ldquo;인터페이스에 프로그래밍하라\u0026rdquo; → 예시처럼 Shape 인터페이스처럼 확장 포인트 제공 LSP (리스코프 치환 원칙) 핵심 가치: 다형성 안정성 ↑, 계약 준수성 ↑ 실천 방안: 하위 클래스는 상위 클래스의 행동 규약을 반드시 지켜야 함 생물학적 분류 != 프로그래밍적 상속 \u0026ldquo;is-a\u0026rdquo; 관계가 아닌 경우 상속 금지 → 예시처럼 Penguin은 Bird를 상속하면 안 됨 상속 대신 확장 포인트 제공 ISP (인터페이스 분리 원칙) 핵심 가치: 불필요한 의존성 ↓, 클라이언트 맞춤 설계 실천 방안: \u0026ldquo;클라이언트는 자신이 사용하지 않는 메서드에 의존하지 말아야 한다\u0026rdquo; 거대한 인터페이스는 작은 단위로 분할 (예: UserAPI → ReaderAPI + WriterAPI) DIP (의존 역전 원칙) 핵심 가치: 모듈 간 결합도 ↓, 유연성 ↑ 실천 방안: \u0026ldquo;추상화에 의존하라, 구체화에 의존하지 말라\u0026rdquo; \u0026ldquo;프로그램을 플러그인 아키텍처로 만든다\u0026rdquo; 모든 의존성이 추상화를 향하도록 설계하면, 시스템은 유연한 레고 블록처럼 조립 가능해진다. 핵심 질문 정리 원칙 키워드 핵심 질문 SRP 단일 책임 \u0026ldquo;이 클래스를 변경하는 이유는 하나인가?\u0026rdquo; OCP 확장 개방 \u0026ldquo;새 기능을 추가할 때 기존 코드를 수정하는가?\u0026rdquo; LSP 치환 가능 \u0026ldquo;하위 클래스를 상위 클래스로 대체해도 문제없는가?\u0026rdquo; ISP 인터페이스 분리 \u0026ldquo;클라이언트가 필요 없는 메서드를 구현하도록 강제하는가?\u0026rdquo; DIP 추상화 의존 \u0026ldquo;고수준 모듈이 저수준 모듈에 직접 의존하는가?\u0026rdquo; ⚙️EndNote 사전 지식 객체 지향 프로그래밍(OOP) 기본 개념 (클래스, 객체, 상속, 다형성 등) 인터페이스와 추상 클래스의 차이 의존성 주입(Dependency Injection) 개념 더 알면 좋은 것들? GRASP 원칙: SOLID 외에도 일반적인 책임 할당을 위한 소프트웨어 패턴 디자인 패턴: SOLID 원칙을 적용한 구체적인 설계 예시들 (팩토리, 전략, 옵저버 패턴 등) 리팩토링 기법: SOLID 원칙을 준수하도록 코드를 개선하는 방법 테스트 주도 개발(TDD): SOLID 원칙과 잘 어울리는 개발 방법론 의존성 주입 프레임워크: Spring, Guice 등 DIP를 쉽게 적용할 수 있게 도와주는 도구들 ","date":"2025-04-07T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-07-solid-object-oriented-principles/","title":"SOLID 원칙: 객체지향의 5가지 원칙"},{"content":"📌개요 IntelliJ IDEA에서 프로젝트를 열 때 \u0026ldquo;Microsoft Defender may affect IDE\u0026rdquo; 경고가 뜨는 건 Microsoft Devender(Windows Defender)의 실시간 보호 기능이 IDE 성능에 영향을 줄 수 있다는 의미다.\n📌내용 Windwos Defender의 실시간 보호 기능은 파일 시스템의 변경 사항을 실시간으로 검사하여 악성 코드나 의심스러운 활동을 탐지한다.\nIntelliJ와 같은 IDE는 빌드, 인덱싱, 파일 읽기/쓰기 작업이 빈번하게 발생하기 때문에 Defender의 스캔이 성능 저하를 유발할 수 있다.\n이 경고는 사용자가 이를 해결할 수 있도록 몇 가지 옵션을 제공한다.\nExclude folders 현재 열려 있는 프로젝트를 Defender가 특정 폴더를 실시간 스캔에서 제외하도록 설정한다. 해당 폴더 내 파일을 스캔하지 않게 되어 IDE 성능이 향상될 수 있다.\n예를 들어, IntelliJ는 .idea(프로젝트 설정), target(Maven/Gradle 빌드 결과물), 또는 소스 코드 폴더에서 빈번하게 파일을 생성하거나 수정한다.\n이런 폴더를 제외하면 Defender가 불필요한 스캔을 하지 않아 성능 저하를 줄일 수 있다.\nIgnore for this project 해당 프로젝트에 대해 경고를 다시 표시하지 않도록 설정한다. 단, 이 경우 Defender 설정이 변경되지는 않으며 단순히 경고 알림만 비활성화된다. 성능 문제는 여전히 남아 있을 수 있다.\nNever ask again 모든 프로젝트에 대해 이 경고를 비활성화한다. 역시 Defender 설정 자체에는 영향을 주지 않는다.\n제외하면 어디에 기록되지? Exclude folders를 선택하면 IntelliJ가 Windows Defender의 제외 목록에 해당 폴더를 추가한다. 이 설정은 IntelliJ 자체에 저장되는 것이 아니라 Windows Defender의 설정에 기록된다.\nWindows Defender 제외 설정 위치 Windows Defender의 제외 목록은 Windows 시스템 설정에 저장된다. 일반적으로 이 정보는 레지스트리나 Defender의 내부 데이터베이스에 기록되며, 사용자가 직접 편집할 수 있는 특정 파일로 존재하지 않는다.\n하지만 Windows Defender 설정을 통해 확인할 수 있다.\n설정 \u0026gt; Windows 보안 \u0026gt; 바이러스 및 위협 방지 \u0026gt; 바이러스 및 위협 방지 설정 관리에서 제외 목록을 확인할 수 있고 제거할 수 있다.\nIntelliJ의 역할 IntelliJ는 사용자가 Exclude folders를 선택하면 Windows Defender API를 호출하여 해당 프로젝트 폴더를 제외 목록에 추가한다고 한다.\nTrusted Location 작업 폴더를 모아두는 드라이브, 폴더 등이 일정하다면, IntelliJ 설정에서 신뢰할 수 있는 경로를 설정할 수 있다.\nSettings \u0026gt; Build, Execution, Deployment \u0026gt; Trusted Locations 해당 메뉴에서 본인이 원하는 프로젝트들을 감싸고 있는 상위 경로를 추가 후 저장한다.\n🎯결론 간단한 테스트를 위해 만든 프로젝트를 모두 계속 등록하게 되면 어딘가에 기록이 쌓이는지 궁금해서 찾아보게 됐다.\n폴더 또는 파일의 수정이 빈번하게 발생하는 프로젝트를 계속 Windows Defender가 스캔하며 성능이 저하될 수 있다는 경고다.\n경고를 무시하고 진행해도 되고, 성능을 위해 Defender의 제외 목록에 등록할 수도 있다.\nIntelliJ에서 프로젝트를 열 때 신뢰 관련 확인을 요구하는 게 귀찮다면 Trusted Location 설정을 하는 방법이 있다.\n","date":"2025-04-06T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-06-intellij-microsoft-defender-may-affect-ide/","title":"IntelliJ IDEA에서 프로젝트 경고"},{"content":"📌개요 IntelliJ IDEA에서 메모리 힙 사이즈에 대해 전역 설정과 애플리케이션 별로 설정하는 방법을 알아본다.\n📌내용 전역 메모리 설정 Change Memory Settings Help \u0026gt; Change Memory Settings 선택 → 팝업에서 힙 사이즈 변경\nEdit Custom VM Options IDE 메뉴에서 설정하는 방법도 있지만, 직접 설정 파일을 수정하는 방법도 있다.\nHelp \u0026gt; Edit Custom VM Options 선택, 또는 각 설정 파일 경로를 찾아가서 수정하는 방법이 있다.\nWindows: %USERPROFILE%\\AppData\\Roaming\\JetBrains\\\u0026lt;IntelliJ 버전\u0026gt;\\idea64.exe.vmoptions macOS: ~/Library/Application Support/JetBrains/\u0026lt;IntelliJ 버전\u0026gt;/idea.vmoptions Linux: ~/.config/JetBrains/\u0026lt;IntelliJ 버전\u0026gt;/idea64.vmoptions 1 2 3 4 5 # 초기 힙 사이즈(최소 메모리) -Xms512m # 최대 힙 사이즈 (권장: 시스템 메모리의 1/4 ~ 1/2) -Xms2048m 파일 수정 후 IntelliJ를 재시작하여 변경 사항을 적용한다.\n실행 프로필 별로 메모리 설정 Run/Debug Configurations에서 VM Options 설정으로 특정 프로젝트의 애플리케이션을 실행할 때 사용되는 JVM의 메모리를 설정할 수 있다.\n상단 툴바에서 Edit Configurations 선택, 또는 Shift + Alt + F10 → Edit Configurations\nVM options에 메모리 설정 추가\n1 -Xms256m -Xmx1024m // 최소 256MB, 최대 1GB 할당 메모리 사용량 표시 IDE 하단 우클릭 \u0026gt; Memory Indicator\n","date":"2025-04-06T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-06-intelij-changing-the-memory-size/","title":"IntelliJ 메모리 설정"},{"content":"📌개요 자바 소스 코드를 실행하는 JVM의 메모리 구조를 알아본다. JVM은 자바 프로그램을 실행하기 위한 가상 머신으로, 플랫폼 독립성을 제공하며 메모리 관리, 스레드 동기화, 가비지 컬렉션 등의 기능을 수행한다.\n📌내용 Java 소스 실행 자바 소스 코드는 .java 파일로 저장되며, 이를 실행하기 위해 자바 컴파일러 javac가 바이트 코드(Byte code) .class 파일로 변환하게 된다.\n바이트 코드는 JVM이 이해할 수 있는 중간 표현으로, 플랫폼 독립성을 제공한다. 바이트 코드로 변환하는 이유는 다음과 같다.\n보안성: 소스 코드를 직접 노출하지 않고 바이트 코드로 변환함으로써 코드의 내용을 보호할 수 있다. 효율성: 컴파일 시 문법 검사와 최적화를 거치므로, 실행 시 추가적인 문법 검사가 필요 없어 시간이 절약된다. 플랫폼 독립성: 바이트 코드는 JVM이 설치된 어떤 플랫폼에서든 실행 가능하다. 그러나 이 방식은 소스 코드가 변경될 때마다 컴파일을 하고 실행 시켜야 되기 때문에 초기 빌드 시간이 길어질 수 있다.\n컴파일된 .class 파일은 클래스 로더(Class Loader)에 의해 JVM 메모리 영역인 런타임 데이터 영역(Runtime Data Area)로 로드된다.\n이후 실행 엔진(Execution Engine)이 바이트 코드를 실행 가능한 기계어로 변환하여 프로그램을 실행한다.\nJVM 메모리 구조 JVM은 자바 프로그램을 실행할 때 메모리를 다음과 같은 주요 영역으로 나눠 관리한다. 각 영역은 특정 데이터를 저장하고 스레드 간 공유 여부나 가비지 컬렉션 대상 여부에 따라 역할이 나뉜다.\n메모리 영역 설명 특징 메소드 영역 (Method Area) 클래스 정보, 정적 변수, 상수, 메서드 데이터, 바이트 코드 등이 저장됨. 모든 스레드가 공유. JVM 시작 시 생성. 힙 (Heap) 객체와 배열 같은 동적 데이터가 저장됨. 가비지 컬렉션(GC)이 관리. 모든 스레드가 공유. 스택 (Stack) 메서드 호출 시 지역 변수, 매개변수, 호출 정보(리턴 주소 등)가 저장됨. 스레드별 생성. LIFO 구조. PC 레지스터 (PC Register) 현재 실행 중인 JVM 명령어의 주소를 저장. 스레드별 생성. 매우 빠른 접근. 네이티브 메소드 스택 (Native Method Stack) 네이티브 메서드 호출 시 사용되는 스택. C/C++ 코드 실행 시 활용. 스레드별 생성. Method Area JVM이 시작될 때 생성되며, 프로그램 종료 시까지 유지된다. 바이트 코드가 이 영역에 저장된다.\n저장 데이터: 클래스 정보: 클래스 구조, 메서드, 필드 등의 메타데이터 정적 변수(static): 클래스의 정적 멤버 변수 상수 풀: 문자열 리터럴, final 변수 등 메서드 바이트 코드: 컴파일된 메서드 코드. Java 8 이후 Metaspace로 대체되었으며, 이는 네이티브 메모리를 사용하여 동적으로 확장 가능하다. 모든 스레드가 공유하므로 동기화가 필요할 수 있다.\nHeap 동적으로 생성된 객체와 배열이 저장되는 영역이다. new 키워드로 생성된 모든 인스턴스는 힙에 할당된다.\n해당 객체가 소멸되기 전이나 GC(Garbage Collector)가 정리하기 전까지 유지되는 영역으로 쉽게 소멸되는 데이터가 아니다.\nGC의 대상이 되는 공간이며 효율적인 GC를 실행하기 위해서 5가지 영역으로 나뉘게 된다. Young Generation: Eden Space: 새 객체가 처음 생성되는 곳 Survivor Spaces(S0, S1): Minor GC 후 살아남은 객체가 이동 Old Generation(Tenured): 오래된 객체가 저장됨. Permanent Generation(Java 7까지, 이후 Metaspace로 대체): 클래스 메타데이터 저장 힙은 GC의 주요 대상이며, 더 이상 참조되지 않는 객체를 정리하여 메모리를 회수한다. Minor GC: Young Generation에서 발생 Major/Full GC: Old Generation과 전체 힙을 대상으로 함 모든 스레드가 공유하므로 동기화가 필요하다.\nStack 각 스레드마다 독리적으로 생성되며, 메서드 호출 시마다 프레임(Frame)이 생성되어 스택에 쌓인다.\n프레임 구성: 지역 변수 배열: 메서드의 지역 변수와 매개변수 피연산자 스택: 연산 중간 결과 저장 프레임 데이터: 메서드 호출 정보(리턴 주소 등) 메서드 호출이 종료되면 해당 프레임이 스택에서 제거되고, 지역 변수는 소멸된다. 참조형 변수(예: 객체 참조)는 스택에 저장되지만, 실제 객체는 힙에 저장된다.\n스레드별로 독립적이므로 동기화가 필요 없다.\nPC Register 스레드별로 생성되며, 현재 실행 중인 JVM 명령어의 주소를 저장한다. JVM이 명령어를 실행할 때 다음에 실행할 명령어 위치를 추적한다. 스레드가 실행 중단(예: 컨텍스트 스위칭) 시 현재 상태를 저장하여 재개 시 올바른 위치에서 실행을 계속할 수 있다. Native Method Stack Java가 아닌 네이티브 언어(C, C++ 등)로 작성된 메서드를 실행할 때 사용된다. JNI(Native Method Interface)를 통해 호출된 네이티브 메서드의 호출 스택을 관리한다. 스레드별로 독립적으로 생성된다. Execution Engine 클래스 로더가 로드한 바이트 코드를 실행 가능한 기계어로 변환하여 실행한다.\n구성: Interpreter: 바이트 코드를 한 줄씩 해석하여 실행. 초기 실행 속도는 빠르지만 반복 실행 시 비효율적 JIT(Just-In-Time) Compiler: 자주 실행되는 코드를 런타임에 기계어로 컴파일하여 캐싱. 이후 실행 속도가 빨라진다. Garbage Collector: 더 이상 참조되지 않는 객체를 힙에서 제거한다. 힙과 Method Area는 스레드 간 공유되므로 동기화 매커니즘이 필요하다. synchronized 키워드나 Lock 객체를 사용하여 동기화 처리.\nNative Method Interface (JNI) Java 코드와 네이티브 코드(C/C++ 등) 간의 인터페이스를 제공하는 프레임워크. 성능 최적화, 기존 C 라이브러리 활용하는 용도이다.\nJava 코드에서 native 키워드로 선언된 메서드를 호출 JNI가 해당 네이티브 메서드를 찾아 실행 네이티브 메서드는 네이티브 메소드 스택을 사용하여 실행 Native Method Libraries JNI가 호출하는 네이티브 메서드의 구현체가 포함된 라이브러리.\n.dll (Windows), .so (Linux) 등의 동적 라이브러리 형태로 제공 JVM은 System.loadLibrary()를 통해 필요한 라이브러리를 로드한다. Java의 실행과 각 메모리 영역 Java 프로그램의 생명 주기를 단계별로 살펴보며 각 메모리 영역이 어떻게 활용되는지 분석한다.\n소스 코드 작성 및 컴파일: .java 파일을 작성하고 javac로 컴파일하여 .class 파일(바이트 코드) 생성. 바이트 코드는 플랫폼 독립적이며, JVM이 이해할 수 있는 중간 언어. Class Loading: Class Loader가 .class 파일을 읽어 Method Area에 클래스 정보(메타 데이터, 바이트 코드 등)를 로드한다. 정적 변수(static)도 Method area에 할당한다. 프로그램 실행: Execution Engine이 바이트 코드를 해석/컴파일하여 실행한다. main 메서드 호출 시 스레드가 생성되고, 해당 스레드의 Stack에 main 메서드 프레임이 생성된다. PC Register는 현재 실행 중인 명령어 주소를 추적한다. 객체 생성: new 키워드로 객체를 생성하면 Heap에 객체가 할당된다. 객체 참조는 Stack의 지역 변수로 저장된다. 메서드 호출: 메서드 호출 시 새로운 프레임이 Stack에 추가된다. 지역 변수와 매개변수는 프레임 내에 저장된다. 메서드 종료 시 프레임이 제거되고 지역 변수는 소멸된다. 스레드 동기화: 여러 스레드가 Heap 또는 Method Area에 데이터를 공유할 경우 동기화 필요 예: synchronized 블록을 사용하여 공유 자원에 대한 접근 제어. 가비지 컬렉션: 더 이상 참조되지 않는 객체를 Heap에서 제거한다. Young Generation에서 자주 발생하는 Minor GC와 전체 힙을 대상으로 하는 Full GC로 나뉜다. 네이티브 메서드 호출: native 메서드 호출 시 JNI를 통해 네이티브 코드를 실행한다. Native Method Stack에 호출 정보 저장 Native Method Libraries에서 해당 메서드 구현체 실행 효율적인 메모리 관리 전략 Heap 메모리 최적화: 불필요한 객체 생성을 최조화 객체 풀(Object Pool)을 사용하여 자주 생성/소멸되는 객체 재사용. 적절한 GC 튜닝(예: -Xms, -Xmx로 초기/최대 힙 크기 설정) Stack 메모리 관리: 깊은 재귀 호출을 피하여 StackOverflowError 방지 지역 변수 사용을 최소화하고, 불필요한 변수 선언 줄이기 Method Area 관리: 클래스 로딩 최소화: 불필요한 클래스 로딩 방지 Metaspace 크기 조정 (예: -XX:MaxMetaspaceSize 옵션) 가비지 컬렉션 최적화: Young/Old Generation 크기 조정 적절한 GC 알고리즘 선택 (예: G1GC, CMS) 메모리 누수 방지: 강한 참조(Strong Reference) 대신 약한 참조(Weak Reference) 사용 네이티브 메모리 관리: 네이티브 메서드 호출 시 메모리 누수 주의 JNI로 할당된 네이티브 메모리를 적절히 해제 📌Java 실행 예제 Native Method Libraries Native Method Libraries는 JDK에 포함된 기본 라이브러리가 아니라 사용자가 별도로 작성하거나 서드파티 라이브러리로 제공 받아야 하는 네이티브 라이브러리다.\n프로젝트 생성 Windows에서 Gradle, Java 17, Groovy 프로젝트 생성 후 테스트할 예정이다. com.java.JVMMemoryExample 클래스를 생성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 import java.util.ArrayList; import java.util.List; public class JVMMemoryExample { // 정적 변수: Method Area에 저장됨 private static int staticCounter = 0; // 인스턴스 변수: Heap에 저장됨 (객체가 생성될 때) private String message; public JVMMemoryExample(String message) { this.message = message; } // 메서드: Method Area에 바이트 코드로 저장됨 public synchronized void incrementCounter() { // 지역 변수: Stack에 저장됨 int localVar = 10; // 동기화: Heap/Method Area의 공유 자원(staticCounter)에 접근 staticCounter += localVar; System.out.println(Thread.currentThread().getName() + \u0026#34; - Counter: \u0026#34; + staticCounter); } // 네이티브 메서드 선언: JNI를 통해 호출 public native void callNativeMethod(); // 네이티브 라이브러리 로드: Native Method Libraries에서 로드 static { System.loadLibrary(\u0026#34;NativeLib\u0026#34;); } public static void main(String[] args) { System.out.println(\u0026#34;프로그램 시작 - VisualVM 연결을 위해 15초 대기...\u0026#34;); sleep(15000); // VisualVM 연결 시간 확보 System.out.println(\u0026#34;1. 객체 생성 단계 시작 (5초 간격)\u0026#34;); List\u0026lt;JVMMemoryExample\u0026gt; examples = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; 83; i++) { examples.add(new JVMMemoryExample(\u0026#34;Object-\u0026#34; + i)); if (i % 20 == 0) { // 20개마다 일시 정지 System.out.println(\u0026#34; 생성된 객체: \u0026#34; + (i+1) + \u0026#34;개, 5초 대기...\u0026#34;); sleep(5000); } } System.out.println(\u0026#34;2. 개별 객체 생성 및 스레드 시작 (10초 대기)\u0026#34;); JVMMemoryExample example1 = new JVMMemoryExample(\u0026#34;Hello\u0026#34;); JVMMemoryExample example2 = new JVMMemoryExample(\u0026#34;World\u0026#34;); sleep(5000); // 객체 생성 후 대기 JVMMemoryExample finalExample = example1; Thread t1 = new Thread(finalExample::incrementCounter, \u0026#34;Thread-1\u0026#34;); JVMMemoryExample finalExample1 = example2; Thread t2 = new Thread(finalExample1::incrementCounter, \u0026#34;Thread-2\u0026#34;); t1.start(); t2.start(); try { t1.join(); t2.join(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;3. 네이티브 메서드 호출 전 10초 대기...\u0026#34;); sleep(10000); System.out.println(\u0026#34;4. 네이티브 메서드 호출\u0026#34;); example1.callNativeMethod(); sleep(5000); // 네이티브 호출 후 대기 System.out.println(\u0026#34;5. 참조 해제 및 GC 호출 단계\u0026#34;); System.out.println(\u0026#34; example1, example2 참조 해제 (5초 대기)\u0026#34;); example1 = null; example2 = null; sleep(5000); System.out.println(\u0026#34; 첫 번째 GC 호출\u0026#34;); System.gc(); sleep(10000); // GC 후 메모리 변화 관찰 System.out.println(\u0026#34; examples 리스트 클리어 (5초 대기)\u0026#34;); examples.clear(); sleep(5000); System.out.println(\u0026#34; 두 번째 GC 호출\u0026#34;); System.gc(); sleep(10000); // GC 후 메모리 변화 관찰 System.out.println(\u0026#34;6. 프로그램 종료 대기 (30초)\u0026#34;); sleep(30000); } private static void sleep(long millis) { try { Thread.sleep(millis); } catch (InterruptedException e) { e.printStackTrace(); } } } NativeLib 생성 NativeLib을 생성해서 라이브러리까지 테스트한다. 이는 JNI(Java Native Interface)를 사용하여 C/C++ 코드를 작성하고, 이를 동적 라이브러리 (.dll, .so, .dylib)로 컴파일 하는 과정을 포함한다.\n네이티브 메서드 헤더 파일 생성 JVMMemoryExample 클래스를 컴파일한다. 1 2 3 4 5 javac JVMMemoryExample.java # 컴파일부터 인코딩 오류... 파일은 UTF-8, javac는 기본 ms-949 # 인코딩 옵션 추가해서 테스트 javac -encoding UTF-8 JVMMemoryExample.java javac -h 명령어로 헤더 파일을 생성 1 2 3 4 5 6 7 javac -h . JVMMemoryExample.java # 인코딩... javac -encoding UTF-8 -h . JVMMemoryExample.java # JVMMemoryExample.h 라는 파일이 생성된다. 내용은 아래와 같은 파일이 만들어진다. # JNIEXPORT void JNICALL Java_JVMMemoryExample_callNativeMethod(JNIEnv *, jobject); C 코드 작성 NativeLib.c 파일을 생성하고 네이티브 메서드를 구현한다.\n1 2 3 4 5 6 7 #include \u0026lt;jni.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026#34;JVMMemoryExample.h\u0026#34; JNIEXPORT void JNICALL Java_JVMMemoryExample_callNativeMethod(JNIEnv *env, jobject obj) { printf(\u0026#34;네이티브 메서드가 호출되었습니다!\\n\u0026#34;); } 동적 라이브러리 컴파일 MinGW를 사용한다. Java 환경 변수는 설정됐다고 가정한다.\ngcc 컴파일러를 사용하기 위해 SourceForge에서 MinGW-w64 바이너리 배포판을 다운로드 받아 C:\\mingw-w64 폴더 아래에 배치하고 환경 변수를 등록했다.\n1 2 3 4 5 6 C:\\mingw-w64\\ mingw64\\ bin\\ \u0026lt;- gcc.exe, g++.exe 등이 포함됨 include\\ lib\\ ... 환경 변수 등록 후 gcc 버전 확인\n1 2 3 4 gcc --version # gcc (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 8.1.0 # Copyright (C) 2018 Free Software Foundation, Inc. 이후 컴파일 명령어 실행하면 NativeLib.dll 파일이 생성된다.\n1 gcc -I\u0026#34;%JAVA_HOME%\\include\u0026#34; -I\u0026#34;%JAVA_HOME%\\include\\win32\u0026#34; -shared -o NativeLib.dll NativeLib.c 라이브러리 경로 설정 생성된 NativeLib.dll을 java.library.path에 추가하거나 IntelliJ에서 실행 시 경로를 지정한다. Run \u0026gt; Edit Configurations \u0026gt; VM options에 추가해서 실행한다.\n1 2 -Djava.library.path=NativeLib_파일_경로 # 예: -Djava.library.path=C:\\libs 실행 결과 일반적인 실행 결과\n1 2 3 4 5 Thread-1 - Counter: 10 Thread-2 - Counter: 20 네이티브 메서드가 호출되었습니다! BUILD SUCCESSFUL in 444ms 이 실행을 통해 JVM 메모리 구조를 확인하기 위해선 VisualVM, JConsole 또는 JVM 로그 옵션 -XLog:gc를 사용할 수 있다.\nVisualVM으로 JVM 메모리 구조 확인 뭔가 이름부터 눈으로 확인하기 좋을 것 같아서 JDK에 포함되어 있는 강력한 모니터링 도구 VisualVM으로 JVM의 메모리 사용량, 스레드 상태, GC 동작 등을 확인한다.\nJDK 설치 경로에서 jvisualvm.exe이 있다면 실행한다. 없다면 직접 VisualVM을 다운로드 받는다.\nOpenJDK를 사용하고 있고 jvisualvm.exe가 포함되지 않은 배포판인 듯. https://visualvm.github.io/download.html 직접 VisualVM을 다운로드 받았다. 독립적으로 실행 가능해서 별도 위치에 배치한 후 visualvm_2110\\bin\\visualvm.exe를 실행했다. 실행 후 자동으로 JDK가 감지되지만 직접 JDK를 지정하여 VisualVM을 실행할 수 있다.\n1 2 3 # VisualVM이 설치된 디렉토리로 이동한 후 JDK 경로를 지정하여 VisualVM을 실행 # 예시 경로이므로 실제 경로를 사용한다. visualvm.exe --jdkhome \u0026#34;C:\\Program Files\\java\\openlogic-openjdk-17.0.14+7-windows-x64\u0026#34; 실행 후 좌측 패널에서 JVMMemoryExample을 확인할 수 있고 각 탭에서 필요한 정보를 찾아 확인해볼 수 있다.\n실행 예제를 테스트하기 위해 VisualVM의 상단 툴바에서 Tools \u0026gt; Plugins \u0026gt; Available Plugins을 보면 Visual GC가 있다. 이걸 설치하고 확인해본다.\n🎯결론 JVM의 메모리 구조를 이해하고 프로그램이 실행될 때 시각적으로 확인해볼 수 있었다. VisualVM은 다시 한 번 다뤄야겠다.\n⚙️EndNote Garbage Collector 가비지 컬렉터 GC는 더 이상 참조되지 않는 객체를 힙에서 제거하여 메모리를 회수하는 역할을 갖는다.\n종류 Serial GC: 단일 스레드로 GC 수행. 소규모 애플리케이션에 적합 Parallel GC: 여러 스레드로 GC 수행. 처리량 중점 CMS (Concurrent Mark-Sweep): 애플리케이션 스레드와 동시에 실행. 낮은 지연 시간 중점 G1GC: 대규모 힙에서 효율적인 GC. Java 9부터 기본 GC. 동작 Mark: 더 이상 참조되지 않는 객체 식별 Sweep: 식별된 객체 제거 Compact (일부 GC): 메모리 조각화 방지 최적화 팁 GC 로그 분석 (-Xlog:gc 옵션) 애플리케이션 특성에 맞는 GC 선택 불필요한 객체 생성 방지 ","date":"2025-04-06T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-06-java-memory-structure-jvm/","title":"Java 실행 환경: JVM 메모리 구조의 이해"},{"content":"📌개요 특정 커밋에 추가적인 기록을 남길 수 있는 tag를 알아본다.\n📌내용 tag Git tag는 Git 버전 관리 시스템에서 특정 커밋을 가리키는 참조 포인터다. 주로 프로젝트의 중요한 시점인 release, alpha, beta 등을 표시하기 위해 사용된다.\ntag 사용 시나리오 소프트웨어 release 버전 표시 (v1.0.0, v2.1.3 등) 중요한 개발 이정표 (알파/베타, 주요 기능 완성) 프로덕션 배포 시점 기록 버그 수정을 위한 특정 시점 참조 의미 있는 버전 명명 규칙 MAJOR.MINOR.PATCH -\u0026gt; v2.1.3 형태로 Semantic Versioning (SemVer)을 추천한다.\nMAJOR: 설계의 변경 또는 큰 구조적 변경 MINOR: 기능 추가 등 중요도가 중간 정도 되는 경우 PATCH: 작은 버그 수정, 문서 수정 등 각 버전은 꼭 1, 2자리 제한이 아니라 관리 규칙에 따라 계속 증가할 수 있다. 예시 1 2 3 4 5 6 7 8 9 10 git tag -a v1.4.0-alpha.1 -m \u0026#34;Alpha release\u0026#34; # 초기 개발 단계 git tag -a v0.1.0 -m \u0026#34;프로젝트 구성\u0026#34; # 첫 안정판 git tag -a v1.0.0 -m \u0026#34;안정판 배포\u0026#34; # 핫픽스 적용 후 git tag -a v1.0.1 -m \u0026#34;긴급 건 핫픽스\u0026#34; 주석 tag 우선 사용 가능하면 주석 태그 사용하는 것이 좋다.\n예시 1 2 3 4 5 6 7 8 # 명령줄에서 큰따옴표를 닫지 않고 줄바꿈하여 작성 git tag -a v1.2.0 -m \u0026#34;안정화 버전 v1.2.0 - 검색 기능 추가 - 모바일 레이아웃 이슈 수정 - 의존성 업데이트\u0026#34; # 자동으로 열리는 기본 에디터를 사용할 수도 있다. git tag -a v2.1.0 특정 커밋에 tag 특정 커밋을 확인 후 해시값으로 태그를 생성할 수 있다.\n1 2 3 4 5 # 커밋 해시 확인 git log # 해시값으로 태그 생성 git tag -a v1.1.1 a1b2c3d -m \u0026#34;이게 짱짱 버전임\u0026#34; tag 수정 Git tag는 일반적으로 수정이 불가능하다. 삭제 후 재생성해야 한다.\n수정 불가 이유 태그는 특정 커밋을 가리키는 고정된 참조로 일단 생성되면 내용 변경이 불가능하다. 이는 버전 관리의 핵심 원칙 중 하나로 릴리스 버전의 무결성을 보장하기 위함이다.\n버전 관리의 신뢰성: 한 번 릴리스된 버전은 변하지 않아야 한다. 배포 추적 용이성: 프로덕션에서 실행 중인 코드 버전을 정확히 추적 가능 의존성 관리: 다른 프로젝트에서 특정 태그 버전을 의존할 때 문제 방지 삭제 후 재등록 수정이 불가하니 태그를 삭제하고 재등록해서 깔끔한 내용을 유지한다.\n1 2 3 4 5 6 7 8 9 # 1. 로컬 태그 삭제 git tag -d v1.0.0 # 2. 원격 태그 삭제 (필요시) git push origin :refs/tags/v1.0.0 # 3. 새 태그 생성 git tag -a v1.0.0 {새로운 커밋 해시} -m \u0026#34;수정된 메시지\u0026#34; git push origin v1.0.0 강제 등록 이미 푸시된 태그를 덮어쓰거나 강제로 처리해야 하는 경우 특별히 주의해서 적용한다.\n1 2 git tag -f -a v1.0.0 {커밋 해시} -m \u0026#34;새 메시지\u0026#34; # 로컬에서 강제 재설정 git push -f origin v1.0.0 # 원격 강제 푸시 tag push 단일 tag push\n1 git push origin v1.0.0 모든 tag를 한 번에 push\n1 git push origin --tags push한 tags 확인\n1 git ls-remote --tags origin tag push 실패 시 태그 중복 또는 최신화가 되지 않아서 push 거절되는 경우\n1 2 3 4 5 # 최신 tags 동기화 git fetch --tags # 충돌 시 강제로 push (주의) git push -f origin v1.0.0 CI/CD 연동 예시 Github Actions에서 특정 tag를 커밋했을 때를 가정하면 아래처럼 작성할 수 있다.\n1 2 3 4 5 6 7 8 # .github/workflows/release.yml name: Release Build on: push: tags: - \u0026#39;v*\u0026#39; # v로 시작하는 tag push될 때 실행 ... tag 장단점 장점 버전 관리에 용이하다. 명확한 릴리스 포인트를 제공한다. 어떤 코드가 어떤 버전으로 배포되었는지 추적할 수 있다. 태그된 버전은 변경되지 않는 참조 지점을 제공하므로 안정성을 보장한다. 팀원들이 특정 버전을 쉽게 확인할 수 있어서 협업 효율성에 좋다. 단점 과도하게 사용 시 관리 복잡성이 증가한다. 일단 생성된 태그는 이동할 수 없다. 재설정을 해야 한다. CI/CD 파이프라인과의 통합이 브랜치보다 덜 직관적이다. 📌고급 활용 tag 메시지 템플릿 시스템 전역 템플릿을 설정해서 일관된 tag 이력을 남길 수 있다. 특정 위치에 템플릿 파일 생성한다. 예: ~/.git-templates/tag_template\n1 2 3 4 5 6 7 8 9 10 11 12 13 # [%(tag)] 릴리스 노트 ## 주요 변경 사항 %(body) ## 기술적 세부사항 - 커밋 해시: %(object) - 태그 생성일: %(taggerdate:iso8601) ## QA 체크리스트 - [ ] 스모크 테스트 통과 - [ ] 성능 테스트 완료 - [ ] 보안 검증 완료 파일 생성 후 git config 적용\n1 git config --global tag.template ~/#.git-templates/tag_template 등 파일 템플릿 적용 예시 자동으로 에디터가 열릴 수 있게 명령어 실행\n1 git tag -a v1.3.0 # 자동으로 템플릿 로드 에디터에 표시될 내용\n1 2 3 4 5 6 7 8 9 10 11 12 13 # [v1.3.0] 릴리스 노트 ## 주요 변경 사항 이곳에 변경 내용 작성 ## 기술적 세부사항 - 커밋 해시: q1w2e3r4t5 - 태그 생성일: 2023-11-15T14:30:00+09:00 ## QA 체크리스트 - [ ] 스모크 테스트 통과 - [ ] 성능 테스트 완료 - [ ] 보안 검증 완료 ⚙️EndNote 고급 활용 Git 템플릿 작성 방법을 숙지하면 커스텀 태그 메시지 템플릿을 효과적으로 관리할 수 있다. 자동 버전 관리 시스템을 구축하여 프로젝트 버전 관리를 자동화할 수 있다. CI/CD 파이프라인 연동 시 사용하는 배포 플랫폼(GitHub Actions, GitLab CI, Jenkins 등)에 따라 구현 방식이 달라질 수 있으니 주의가 필요하다. PR 요청 시 tag는? 태그는 본인 저장소에서 관리하기 위함이고 Fork한 원본 저장소에 PR을 하는 등의 작업에 tag는 같이 옮겨지지 않는다.\nFork 저장소에서 원본 저장소 PR 시나리오 자신의 Fork에만 태그가 있는 경우 로컬/Fork 저장소에서 태그 생성 1 2 git tag v1.0.0 git push origin v1.0.0 # 자신의 Fork에만 태그 존재 원본 저장소로 PR 생성 -\u0026gt; 태그는 전송되지 않음. 원본 저장소의 태그를 참조하는 경우 원본 저장소에 이미 존재하는 태그(v1.0.0)를 기반으로 작업 1 2 git fetch upstream # 원본 저장소의 태그 동기화 git checkout v1.0.0 새 커밋 후 PR 생성 -\u0026gt; 원본의 태그는 이동하지 않음. 태그가 가리키는 커밋은 고정되어 있음. 왜 태그가 PR과 함께 전송되지 않지? 보안성: 태그는 릴리스 버전을 표시하는 중요한 참조이므로, 임의로 변경되는 것 방지. 권한 분리: 일반 기여자는 브랜치로만 PR 전송 가능. 태그 생성/수정은 저장소 관리자(maintainer) 권한이 필요. 버전 관리 무결성: 원본 저장소의 태그는 공식 릴리스로 간주되므로, PR을 통해 덮어쓸 수 없음. 만약 태그를 원본 저장소에 반영해야 한다면? 방법이 다양하겠지만 현실적이고 보편적인 방법 두 가지로 정리해본다.\n관리자에게 요청 1 2 3 4 [요청 내용] - 태그 이름: v1.0.0 - 대상 커밋: q1w2e3r4 - 이유: 버전 1.0.0 릴리스 준비 완료 Github Releases 활용 PR 머지 후, 원본 저장소에서 Release 탭에서 수동으로 태그 생성 ","date":"2025-04-03T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-03-git-tag/","title":"Git tag"},{"content":"📌개요 Java 8에서 도입된 이중 콜론 연산자 ::는 메서드 참조(Methods Reference)를 간결하게 표현하기 위한 연산자다.\n코드의 간결성과 재사용성을 높이는 데 초점을 맞춘 문법으로, 람다 표현식(lambda expression)을 더 간단히 작성할 수 있게 해준다.\n주로 함수형 인터페이스와 함께 사용되며, Java의 함수형 프로그래밍 스타일을 지원하는 핵심 기능 중 하나다.\n📌내용 기본 사용법 이중 콜론 연산자는 다음과 같은 형태로 사용된다.\n1 2 3 ClassName::methodName // 클래스명::메서드명 // 또는 object::methodName //객체::메서드명 메서드 참조의 유형 이중 콜론 연산자는 4가지 주요 유형으로 메서드를 참조할 수 있다. 각 유형은 호출 방식과 컨텍스트에 따라 다르다.\n정적 메서드 참조 클래스의 정적 메서드(static method)를 참조한다. 객체 생성 없이 클래스 이름으로 직접 호출 가능한 메서드를 대상으로 한다.\n형태: ClassName::staticMethod 람다 대체: (args) -\u0026gt; ClassName.staticMethod(args) Integer::parseInt는 Integer 클래스의 정적 메서드 parseInt를 참조하며, 객체 없이 동작한다.\n1 2 3 4 5 6 7 // 람다 표현식 Function\u0026lt;String, Integer\u0026gt; lambda = (s) -\u0026gt; Integer.parsInt(s); // 메서드 참조 Function\u0026lt;String, Integer\u0026gt; ref = Integer::parseInt; System.out.println(ref.apply(\u0026#34;123\u0026#34;)); 특정 객체의 인스턴스 메서드 참조 이미 생성된 특정 객체의 인스턴스 메서드를 참조한다. 메서드를 호출할 객체가 고정되어 있다.\n형태: object::instanceMethod 람다 대체: (args) -\u0026gt; object.instanceMethod(args) System.out 객체의 println 메서드를 참조해 리스트를 출력한다.\n1 2 3 4 5 6 7 8 9 10 11 12 List\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;한놈\u0026#34;, \u0026#34;두식\u0026#34;, \u0026#34;석삼\u0026#34;); // 람다 표현식 names.forEach(s -\u0026gt; System.out.println(s)); // 메서드 참조 names.forEach(System.out::println); // 출력: // 한놈 // 두식 // 석삼 임의 객체의 인스턴스 메서드 참조 특정 객체의 인스턴스 메서드 참조랑 뭐가 다른 거야? 특정 객체의 인스턴스 메서드 참조는 고정된 객체의 메서드를 호출하는 반면, 임의 객체의 인스턴스 메서드 참조는 호출 시 제공되는 객체에 따라 메서드가 실행된다.\n특정 타입의 객체에서 호출할 인스턴스 메서드를 참조한다. 메서드를 호출할 객체는 호출 시점에 외부에서 제공된다.\n형태: ObjectType::instanceMethod 람다 대체: (obj, args) -\u0026gt; obj.instanceMethod(args) String 클래스의 toUpperCase 메서드를 참조해 문자열 변환 String::toUpperCase는 String 타입의 어떤 객체든 받아서 toUpperCase를 호출한다. 호출 시 제공된 객체에 따라 동작한다.\n1 2 3 4 5 6 7 8 9 10 11 List\u0026lt;String\u0026gt; words = Arrays.asList(\u0026#34;java\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;fun\u0026#34;); // 람다 표현식 words.stream().map(s -\u0026gt; s.toUpperCase()).forEach(System.out::println); // 메서드 참조 words.stream().map(String::toUpperCase).forEach(System.out::println); // 출력: // JAVA // IS // FUN 생성자 참조 객체 생성을 위한 생성자를 참조한다. 새로운 객체를 생성하는 작업을 간소화한다.\n형태: ClassName::new 람다 대체: (args) -\u0026gt; new ClassName(args) ArrayList의 기본 생성자를 참조해 리스트 생성 ArrayList::new는 ArrayList 클래스의 기본 생성자를 참조하며, 호출 시점에 새 객체를 생성한다.\n1 2 3 4 5 6 7 // 람다 표현식 Supplier\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; lambda = () -\u0026gt; new ArrayList\u0026lt;\u0026gt;(); // 메서드 참조 Supplier\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; ref = ArrayList::new; List\u0026lt;String\u0026gt; list = ref.get(); // new ArrayList\u0026lt;\u0026gt;()와 동일 🎯결론 이중 콜론 연산자 :: 의 장점\n코드 가독성 향상: 불필요한 람다 표현식을 줄여 코드 가독성을 높인다. 불필요한 매개변수 제거: 단순히 메서드를 호출하는 경우 중복 코드를 없앤다. 의도가 명확성: 메서드 이름만으로 동작을 직관적으로 이해할 수 있다. 이 연산자는 주로 Stream API, Optional 함수형 인터페이스(Function, Supplier, Consumer)와 함께 사용되며, Java의 함수형 프로그래밍 스타일을 강화한다.\n⚙️EndNote :: 왜 이런 모양이야? 카더라 같긴 하지만 유력한? C++ 같은 언어에서 이미 정적 참조나 스코프를 나타낼 때 :: 형태로 사용한 것으로 보인다. 이것으 영향을 받은 것이 아닐까?\n생성자 참조 더 알아보기 생성자를 참조하게 되면 지연 생성에 유리하다고 하는데, 일부러 지연을 시킨다는 게 이해가 잘 되지 않았다.\n지연의 목적은 필요할 때 객체를 생성해서 메모리를 효율적으로 관리하기 위한 것. 결국 동작은 생성자 참조를 보관했다가 나중에 생성하는 것.\nJava 8 이전과 이후를 비교해서 무슨 의미인지 더 알아본다.\nJava 7까지 생성자 참조 불가능 생성자를 변수에 저장할 수 없었다. 객체 생성을 지연시키려면 익명 클래스나 리플렉션 같은 복잡한 방법을 써야 했다. 1 2 3 4 5 6 7 // Java 7 방식: 익명 클래스로 우회 Supplier\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; supplier = new Supplier\u0026lt;\u0026gt;() { @Override public List\u0026lt;String\u0026gt; get() { return new ArrayList\u0026lt;\u0026gt;(); } } Java 8 이후 생성자 참조 가능 1 2 // Java 8+ : 생성자 참조 Supplier\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; supplier = ArrayList::new; // new ArrayList\u0026lt;\u0026gt;()와 동일 왜 이전에는 안 됐는데? 생성자는 일급 객체가 아니었다. Java 8 이전에는 생성자가 메서드처럼 독립적으로 전달될 수 없는 개념이었다. 람다와 함수형 인터페이스가 도입되며 생성자도 함수처럼 다루는 것이 가능해졌다. 컴파일러의 한계 :: 이중 콜론 연산자(Double Colon Operator)와 타입 추론 기능이 없어서 기술적으로 구현이 어려웠다. 비교 보기 비교 항목 Java 7 이전 Java 8 이후 생성자 전달 불가능 (new는 문법적 예약어) ClassName::new로 참조 가능 지연 생성 익명 클래스로 복잡하게 구현 함수형 인터페이스로 간단히 구현 유연성 하드코딩된 생성 런타임에 생성 방식 변경 가능 예를 들어 조건에 따라 다른 생성자 사용이 가능하다.\n1 2 Supplier\u0026lt;Shape\u0026gt; shapeSupplier = isCircle ? Circle::new : Square::new; Shape shape = shapeSupplier.get(); // 조건에 맞는 객체 생성 ","date":"2025-04-03T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-04-03-java-double-colon-operator/","title":"Java의 이중 콜론 연산자"},{"content":"📌개요 git reset과 git checkout은 과거 커밋으로 이동하거나 작업 상태를 수정하는 데 사용되는 명령어다. 두 명령어는 서로 다른 목적과 작동 방식을 가지며, 특히 staging 영역을 다룰 때도 활용한다. 그 목적과 작동 방식에 대한 차이를 알아본다.\n📌내용 reset 잘못된 커밋을 취소하거나 과거의 특정 시점으로 브랜치를 완전히 되돌려야 할 때 사용한다. 이 명령어는 커밋 이력뿐만 아니라 staging 영역과 작업 디렉토리 상태를 조정할 수 있다.\n3가지 옵션과 함께 사용할 수 있으며 git reset [--option] [commitHash] 형태로 사용한다. 옵션을 지정하지 않으면 기본 옵션은 --mixed로 실행된다.\n1 2 3 4 5 6 7 # 커밋 기록만 되돌리고 작업 디렉토리와 스테이징 영역의 변경 사항은 유지 git reset --soft # 커밋 기록과 스테이징 영역을 되돌리고 작업 디렉토리에 변경 사항은 유지 git reset git reset --mixed # 커밋 기록, 스테이징 영역, 작업 디렉토리 모두를 되돌린다.(주의: 되돌린 변경 사항은 복구할 수 없다.) git reset --hard 옵션 커밋 기록 스테이징 영역 작업 디렉토리 설명 --soft ❗지정된 커밋으로 되돌림 변경 사항 유지 변경 사항 유지 커밋 기록만 되돌리고 변경 사항은 유지한다. 되돌린 커밋 이후의 변경 사항을 다시 커밋하려는 경우에 유용하다. --mixed ❗지정된 커밋으로 되돌림 ❗지정된 커밋 상태로 되돌림 변경 사항 유지 커밋 기록과 스테이징 영역을 되돌리고 작업 디렉토리의 변경 사항은 유지된다. 되돌린 커밋 이후의 변경 사항을 수정하거나 다시 스테이징하려는 경우 사용한다. (기본 옵션) --hard ❗지정된 커밋으로 되돌림 ❗지정된 커밋 상태로 되돌림 ❗지정된 커밋 상태로 되돌림 ❗커밋 기록, 스테이징 영역, 작업 디렉토리 모두를 되돌린다. 되돌린 커밋 이후의 모든 변경 사항은 완전히 삭제되며, 복구할 수 없다. 변경 사항을 완전히 버리고 과거 시점으로 되돌아 가려는 경우에 사용해야 한다. (주의해서 사용해야 한다.) staging 영역과 reset git reset이 스테이징 영역에 사용되는 이유는 --mixed와 --soft 옵션은 스테이징 영역을 조작할 수 있기 때문이다. 예를 들어 git reset --mixed HEAD는 스테이징 영역의 모든 파일을 unstage 상태로 내리고 작업 디렉토리의 변경 사항은 유지한다. 이는 잘못 스테이징한 파일을 내리거나 커밋 전에 스테이징 상태를 재조정할 때 유용하다.\n1 2 git add file1.txt # file1.txt를 staging에 추가 git reset HEAD file1.txt # file1.txt를 staging에서 내림 reset 명령 후 기존 HEAD로 돌아가는 방법 ORIG_HEAD 사용 git reset 명령어를 실행하면 Git은 ORIG_HEAD라는 특수한 참조에 이전 HEAD 위치를 저장한다. git reset --hard ORIG_HEAD 명령어를 사용하여 reset 실행 직전의 HEAD 위치로 돌아갈 수 있다. 이 방법은 reset 실행 직후에만 사용할 수 있으며, 다른 명령어를 실행하면 ORIG_HEAD가 변경될 수 있다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ git log --oneline 2f8b5a0 세 번째 커밋 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git reset HEAD~1 Unstaged changes after reset: M file2.txt $ git log --oneline 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git reset --hard ORIG_HEAD HEAD is now at 2f8b5a0 세 번째 커밋 $ git log --oneline 2f8b5a0 세 번째 커밋 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 reflog 사용 git reflog는 Git 저장소에서 HEAD가 이동한 모든 기록을 보여준다. 이를 통해 reset 실행 전의 HEAD 위치를 찾을 수 있다. git reflog 명령어를 실행하면 HEAD가 이동한 기록과 해당 커밋의 해시값이 표시된다. 기존 HEAD위치의 커밋 해시값을 찾은 후 git reset --hard [커밋 해시값] 명령어를 사용하여 해당 위치로 돌아갈 수 있다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ git log --oneline 2f8b5a0 세 번째 커밋 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git reset HEAD~1 Unstaged changes after reset: M file2.txt $ git log --oneline 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git reflog 817f30a HEAD@{0}: reset: moving to HEAD~1 2f8b5a0 HEAD@{1}: commit: 세 번째 커밋 817f30a HEAD@{2}: commit: 두 번째 커밋 55c82a4 HEAD@{3}: commit (initial): 첫 번째 커밋 $ git reset --hard HEAD@{1} HEAD is now at 2f8b5a0 세 번째 커밋 $ git log --oneline 2f8b5a0 세 번째 커밋 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 checkout 과거의 특정 시점의 파일 상태를 확인하거나, 다른 브랜치로 전환할 때 사용한다.\n브랜치로 이동하면 해당 브랜치의 최신 커밋으로 작업 디렉토리와 스테이징 영역의 상태가 변경되고 새로운 커밋은 해당 브랜치에 추가된다. 특정 커밋으로 이동하면 detached HEAD 상태가 되어 새로운 커밋을 만들면 현재 브랜치와 분리된 별도의 커밋 기록이 생성된다. 현재 브랜치와 분리된 임시적인 커밋 기록에 생성된다. 이 임시 커밋 기록은 특정 브랜치에 열결되지 않으므로 나중에 브랜치에 병합하거나 저장하지 않으면 사라질 수 있다. 임시 커밋의 작업 내용을 유지하려면 새로운 브랜치를 생성해야 한다. staging 영역과 checkout git checkout이 스테이징 영역에 사용되는 이유는 git checkout -- [file] 명령은 특정 파일을 마지막 커밋 상태로 되돌리며, 스테이징 영역에서도 해당 파일을 unstaged 상태로 내린다. 이는 작업 디렉토리와 스테이징 영역의 변경 사항을 취소하고 저장소 상태로 되돌릴 때 유용하다.\n1 2 git add file1.txt # file1.txt를 staging에 추가 git checkout -- file1.txt # file1.txt의 변경 사항 checkout 명령 후 기존 HEAD로 돌아가는 방법 ORIG_HEAD 사용 git reset 명령어를 실행하면 Git은 ORIG_HEAD라는 특수한 참조에 이전 HEAD 위치를 저장한다. git checkout ORIG_HEAD 명령어를 사용하여 checkout 실행 직전의 HEAD 위치로 돌아갈 수 있다. 이 방법은 reset 실행 직후에만 사용할 수 있으며, 다른 명령어를 실행하면 ORIG_HEAD가 변경될 수 있다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ git log --oneline 2f8b5a0 세 번째 커밋 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git checkout HEAD~1 HEAD is now at 817f30a 두 번째 커밋 $ git log --oneline 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git branch * (HEAD detached at 817f30a) master $ git checkout ORIG_HEAD Switched to branch \u0026#39;master\u0026#39; $ git log --oneline 2f8b5a0 세 번째 커밋 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git branch * master reflog 사용 git reflog는 Git 저장소에서 HEAD가 이동한 모든 기록을 보여준다. 이를 통해 checkout 실행 전의 HEAD 위치를 찾을 수 있다. git reflog 명령어를 실행하면 HEAD가 이동한 기록과 해당 커밋의 해시값이 표시된다. 기존 HEAD 위치의 커밋 해시값을 찾은 후 git checkout [커밋 해시값] 명령어를 사용하여 해당 위치로 돌아간다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 $ git log --oneline 2f8b5a0 세 번째 커밋 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git checkout HEAD~1 HEAD is now at 817f30a 두 번째 커밋 $ git log --oneline 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git branch * (HEAD detached at 817f30a) master $ git reflog 817f30a HEAD@{0}: checkout: moving to HEAD~1 2f8b5a0 HEAD@{1}: commit: 세 번째 커밋 817f30a HEAD@{2}: commit: 두 번째 커밋 55c82a4 HEAD@{3}: commit (initial): 첫 번째 커밋 $ git checkout HEAD@{1} Switched to branch \u0026#39;master\u0026#39; $ git log --oneline 2f8b5a0 세 번째 커밋 817f30a 두 번째 커밋 55c82a4 첫 번째 커밋 $ git branch * master staging 영역에서 reset과 checkout의 사용 이유 git reset과 git checkout은 스테이징 영역을 다룰 때 활용되는데 이는 두 명령어가 파일 상태를 조정하는 방식 때문이다.\nreset 스테이징 영역의 파일을 내리는 데 주로 사용된다. 변경된 파일을 작업 디렉토리에 남기고 스테이징 상태만 초기화할 때 유용하다. 예: 실수로 스테이징한 파일을 내리고 다시 수정하고 싶을 때 checkout 스테이징 영역과 작업 디렉토리의 변경 사항을 모두 취소하며 파일을 마지막 커밋 상태로 되돌린다. 스테이징에 올린 파일을 버리고 원래 상태로 복구할 때 사용된다. 예: 실수로 수정한 파일을 완전히 되돌리고 싶을 때 🎯결론 reset은 과거를 수정하고, checkout은 현재를 전환한다고 볼 수 있다. 두 명령어는 스테이징 영역을 다룰 때 보완적으로 사용되며, 상황에 따라 적절히 선택해야 한다.\nreset 주로 커밋 히스토리를 수정하거나 스테이징 영역과 작업 디렉토리를 특정 상태로 되돌릴 때 사용한다.\ngit reset은 브랜치의 커밋 이력을 변경하여 특정 커밋으로 되돌리는데 사용된다. 커밋이력 자체를 변경한다. checkout 브랜치 간 전환이나 특정 파일/커밋 상태를 작업 디렉토리에 반영할 때 사용한다.\ngit checkout은 작업 디렉토리와 스테이징 영역의 상태를 특정 커밋 또는 브랜치로 변경하는 데 사용된다. 파일의 특정 시점 정보를 확인하기 위함. 브랜치 전환에 사용. 차이점 reset은 커밋 히스토리와 상태를 적극적으로 변경하며 되돌리기 중심. checkout은 주로 상태 확인이나 전환에 초점, 히스토리 변경 없음. ","date":"2025-03-30T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-30-git-reset-vs-checkout/","title":"Git Reset VS Checkout"},{"content":"📌개요 stash 명령어는 작업 중인 변경사항을 임시로 저장하고 나중에 다시 적용할 수 있게 해주는 유용한 기능이다. stack 자료구조 형태로 동작한다.\n가장 최근에 저장한 stash가 맨 위 stash@{0} 그 다음으로 최근에 저장한 stash가 stash@{1} 이런 식으로 숫자가 커질 수록 오래된 stash가 된다. 📌내용 stash stash는 현재 작업 중인 변경사항을 임시로 저장하여 작업 디렉토리를 깨끗한 상태(HEAD와 동일한 상태)로 만들고 나중에 저장했던 변경 사항을 다시 적용할 수 있게 해주는 Git 명령어다.\n사용 목적 브랜치 전환: 현재 작업 중인 변경 사항을 커밋하지 않고 다른 브랜치로 전환해야 할 때 긴급 수정: 갑자기 긴급한 버그 수정이 필요할 때 현재 작업을 일시 중단해야 할 경우 작업 중단 및 재개: 작업을 임시로 중단하고 나중에 다시 시작해야 할 때 충돌 방지: pull 등의 명령어로 인한 충돌을 피하고 싶을 때 실험적 변경 사항 관리: 확신이 없는 변경 사항을 안전하게 저장해두고 싶을 때 기본 명령어 대괄호([])는 명령어 설명에서 선택적 매개변수를 나타낸다. 중괄호({})는 stash 인덱스의 실제 구문의 일부이므로 명령어 실행 시 반드시 포함해야 한다. 변경 사항 저장 1 2 3 4 5 6 7 8 # 기본 git stash # 최신 문법 (권장) git stash push -m \u0026#34;메시지\u0026#34; # 옛 문법 (Git 2.13 이전, 현재는 deprecated) git stash save \u0026#34;메시지\u0026#34; 저장된 stash 목록 확인 1 2 3 4 5 git stash list # 출력 예시 stash@{0}: WIP on main: 1a2b3c4 이전 커밋 메시지 stash@{1}: On feature-branch: 상세 메시지 저장된 stash 적용하기 1 2 3 4 5 # 가장 최근의 stash 적용 git stash apply # 특정 stash 적용 git stash apply stash@{n} stash 적용 후 삭제 1 2 3 4 5 6 # 가장 최근의 stash 적용 후 삭제 # 단, 적용 시 충돌이나 실패가 발생하는 경우 삭제되지 않고 유지된다. git stash pop # 특정 stash 적용 후 삭제 git stash pop stash@{n} 특정 stash 삭제 1 git stash drop stash@{n} 모든 stash 삭제 1 git stash clear stash의 내용 확인하기 1 2 3 4 5 6 7 8 # 최근 stash와 현재 디렉토리의 차이점 보기 git stash show # 더 자세한 차이점 보기 git stash show -p # 특정 stash의 차이점 보기 git stash show -p stash@{n} 심화 사용법 대괄호([])는 명령어 설명에서 선택적 매개변수를 나타낸다. 중괄호({})는 stash 인덱스의 실제 구문의 일부이므로 명령어 실행 시 반드시 포함해야 한다. 특정 파일만 stash 1 git stash push -m \u0026#34;메시지\u0026#34; 파일1 파일2 staged 된 파일만 stash 1 git stash push --staged -m \u0026#34;메시지\u0026#34; Untracked 파일도 함께 stash하기 1 2 3 git stash -u # 또는 git stash --include-untracked 새 브랜치에 stash 적용 1 git stash branch 새브랜치명 [stash@{n}] stash로부터 특정 파일만 복원 1 git checkout stash@{n} -- 파일경로 📌실제 활용 시나리오 작업 중 급한 버그 수정하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 현재 작업 중인 변경 사항 저장 git stash push -m \u0026#34;현재 기능 개발 중\u0026#34; # main 브랜치로 전환 git checkout main # 버그 수정 브랜치 생성 git checkout -b hotfix # 버그 수정 작업... # 버그 수정 커밋 git commit -a -m \u0026#34;중요 버그 수정\u0026#34; # main 브랜치에 병합 git checkout main git merge hotfix git branch -d hotfix # 원래 작업 브랜치로 돌아가기 git checkout feature-branch # 저장해둔 작업 복원 git stash pop 충돌 해결하기 1 2 3 4 5 6 7 8 # 변경 사항 저장 git stash # 원격 변경 사항 가져오기 git pull # 저장한 변경 사항 적용 시도 git stash pop 충돌이 발생하는 경우 충돌을 텍스트 편집기, IDE 등에서 수동으로 해결한 후\n1 2 git add . git commit -m \u0026#34;충돌 해결 및 stash 적용\u0026#34; 여러 개의 stash 관리 1 2 3 4 5 6 7 8 9 10 11 # 첫 번째 작업 저장 git stash push -m \u0026#34;기능 A 작업 중\u0026#34; # 다른 작업 후 저장 git stash push -m \u0026#34;기능 B 작업 중\u0026#34; # stash 목록 확인 git stash list # 기능 A 작업으로 돌아가기 git stash apply stash@{1} 📌주의사항 git stash save는 Git 2.13 이후 deprecated 되었으며, 공식 문서에서도 git stash push 사용을 권장한다. 최신 Git에서는 save 명령이 제대로 동작하지 않거나 오류가 발생할 수 있다. 모든 stash 관련 명령은 push, apply, pop, list 등의 명확한 옵션 기반 사용법으로 전환하는 것이 좋다. stash는 임시 저장소이므로 너무 오랫동안 중요한 변경 사항을 stash에만 보관하지 않는 것이 좋다. 여러 stash를 사용할 때는 명확한 메시지를 사용하여 구분하는 것이 중요하다. stash는 주로 로컬 작업에 사용되며 원격 저장소에 공유되지 않는다. 병합 충돌이 있는 파일은 stash할 수 없다. ⚙️EndNote Git config을 통한 stash 관련 설정 필요에 따라 적용한다.\n1 2 3 4 5 6 7 # stash 시 untracked 파일을 항상 포함하도록 설정 git config --global stash.showIncludeUntracked true # stash 명령 별칭(alias) 설정 git config --global alias.st stash git config --global alias.stp \u0026#39;stash pop\u0026#39; git config --global alias.stl \u0026#39;stash list\u0026#39; stash options 옵션 축약형 영어 설명 한글 설명 --all -a include ignored files 무시된(ignored) 파일까지 포함하여 stash한다. --include-untracked -u include untracked files 추적되지 않는(untracked) 파일까지 포함하여 stash한다. --keep-index -k all changes already added to the index are left intact 이미 인덱스(스테이징 영역)에 추가된 변경사항은 그대로 유지한다. --message -m specify stash description stash에 대한 설명(메시지)을 지정한다. --no-keep-index all changes already added to the index are undone 인덱스에 추가된 변경사항도 모두 되돌린다. --patch -p interactively select hunks from diff between HEAD and working tree to stash HEAD와 작업 디렉토리 간의 차이를 대화형으로 선택하여 stash한다. --pathspec-file-nul pathspec elements are separated with NUL character 경로 지정자(pathspec) 요소가 NUL 문자로 구분된다. --pathspec-from-file read pathspec from file 파일에서 경로 지정자(pathspec)를 읽는다. --quiet -q suppress all output 모든 출력을 억제한다. stash show options Git Stash Show 옵션 테이블 옵션 축약형 영어 설명 한글 설명 --abbrev use specified digits to display object names 객체 이름을 표시할 때 지정된 자릿수를 사용한다. --anchored generate diffs using the \u0026ldquo;anchored diff\u0026rdquo; algorithm \u0026ldquo;anchored diff\u0026rdquo; 알고리즘을 사용하여 차이점을 생성한다. --binary in addition to \u0026ndash;full-index, output binary diffs for git-apply --full-index와 함께 사용하면 git-apply를 위한 바이너리 차이점을 출력한다. --break-rewrites -B break complete rewrite changes into pairs of given size 완전 재작성된 변경사항을 지정된 크기의 쌍으로 분리한다. --cc -c combined diff format for merge commits 병합 커밋을 위한 결합된 diff 형식을 사용한다. --check warn if changes introduce trailing whitespace or space/tab indents 변경사항이 후행 공백이나 공백/탭 들여쓰기를 도입하는 경우 경고한다. --color show colored diff 색상이 있는 diff를 표시한다. --color-moved color moved lines differently 이동된 라인을 다른 색상으로 표시한다. --color-moved-ws configure how whitespace is ignored when performing move detection for \u0026ndash;color-moved --color-moved 사용 시 이동 감지에서 공백을 무시하는 방법을 설정한다. --color-words show colored-word diff 단어 단위로 색상이 있는 diff를 표시한다. --compact-summary generate compact summary in diffstat diffstat에 간결한 요약을 생성한다. --cumulative synonym for \u0026ndash;dirstat=cumulative --dirstat=cumulative의 동의어. --diff-algorithm choose a diff algorithm diff 알고리즘을 선택한다. --diff-filter select certain kinds of files for diff diff를 위한 특정 종류의 파일을 선택한다. --dirstat generate dirstat by amount of changes 변경 양에 따라 dirstat을 생성한다. --dirstat-by-file generate dirstat by number of files 파일 수에 따라 dirstat을 생성한다. --dst-prefix use given prefix for destination 대상 파일에 주어진 접두사를 사용한다. --exit-code report exit code 1 if differences, 0 otherwise 차이가 있으면 종료 코드 1, 그렇지 않으면 0을 반환한다. --ext-diff allow external diff helper to be executed 외부 diff 도우미 실행을 허용한다. --find-copies -C detect copies as well as renames with given scope 주어진 범위 내에서 이름 변경뿐만 아니라 복사도 감지한다. --find-copies-harder try harder to find copies 복사본을 찾기 위해 더 많은 노력을 기울인다. --find-object look for differences that change the number of occurrences of specified object 지정된 객체의 발생 횟수를 변경하는 차이점을 찾는다. --find-renames -M detect renames with given scope 주어진 범위 내에서 이름 변경을 감지한다. --follow continue listing the history of a file beyond renames 파일 이름이 변경된 경우에도 파일 기록을 계속 나열한다. --full-index show full object name of pre- and post-image blob 변경 전후 이미지 blob의 전체 객체 이름을 표시한다. --histogram generate diffs with histogram algorithm 히스토그램 알고리즘으로 diff를 생성한다. --ignore-all-space -w ignore white space when comparing lines 라인 비교 시 모든 공백을 무시한다. --ignore-blank-lines ignore changes whose lines are all blank 빈 줄만 있는 변경사항을 무시한다. --ignore-cr-at-eol ignore carriage-return at end of line 줄 끝의 캐리지 리턴(CR)을 무시한다. --ignore-matching-lines -I ignore changes whose lines all match regex 모든 라인이 정규식과 일치하는 변경사항을 무시한다. --ignore-space-at-eol ignore changes in whitespace at end of line 줄 끝의 공백 변경을 무시한다. --ignore-space-change -b ignore changes in amount of white space 공백 양의 변경을 무시한다. --ignore-submodules ignore changes to submodules 서브모듈 변경을 무시한다. --inter-hunk-context combine hunks closer than N lines N 라인보다 가까운 헝크를 결합한다. --irreversible-delete -D omit the preimage for deletes 삭제에 대한 이전 이미지를 생략한다. --ita-invisible-in-index hide \u0026lsquo;git add -N\u0026rsquo; entries from the index 인덱스에서 \u0026lsquo;git add -N\u0026rsquo; 항목을 숨긴다. --line-prefix prepend additional prefix to every line of output 출력의 모든 라인에 추가 접두사를 붙인다. --minimal spend extra time to make sure the smallest possible diff is produced 가능한 가장 작은 diff를 생성하기 위해 추가 시간을 사용한다. --name-only show only names of changed files 변경된 파일의 이름만 표시한다. --name-status show only names and status of changed files 변경된 파일의 이름과 상태만 표시한다. --no-color turn off colored diff 색상 diff를 끈다. --no-color-moved-ws don\u0026rsquo;t ignore whitespace when performing move detection 이동 감지 시 공백을 무시하지 않는다. --no-ext-diff disallow external diff helper to be executed 외부 diff 도우미 실행을 허용하지 않는다. --no-indent-heuristic disable heuristic that shifts diff hunk boundaries to make patches easier to read 패치를 더 쉽게 읽을 수 있게 하는 diff 헝크 경계 이동 휴리스틱을 비활성화한다. --no-patch -s suppress diff output diff 출력을 억제한다. --no-prefix do not show any source or destination prefix 소스나 대상 접두사를 표시하지 않는다. --no-renames turn off rename detection 이름 변경 감지를 끈다. --no-textconv do not allow external text conversion filters to be run when comparing binary files 바이너리 파일 비교 시 외부 텍스트 변환 필터 실행을 허용하지 않는다. --numstat generate more machine-friendly diffstat 기계 친화적인 diffstat을 생성한다. --output output to a specific file 특정 파일로 출력한다. --output-indicator-context specify the character to indicate a context line 컨텍스트 라인을 나타내는 문자를 지정한다. --output-indicator-new specify the character to indicate a new line 새 라인을 나타내는 문자를 지정한다. --output-indicator-old specify the character to indicate a old line 이전 라인을 나타내는 문자를 지정한다. --patch -u, -p generate diff in patch format 패치 형식으로 diff를 생성한다. --patch-with-raw generate patch but also keep the default raw diff output 패치를 생성하지만 기본 raw diff 출력도 유지한다. --patch-with-stat generate patch and prepend its diffstat 패치를 생성하고 그 앞에 diffstat을 추가한다. --patience generate diffs with patience algorithm patience 알고리즘으로 diff를 생성한다. --pickaxe-all when -S finds a change, show all changes in that changeset -S가 변경을 찾으면 해당 변경 세트의 모든 변경을 표시한다. --pickaxe-regex treat argument of -S as regular expression -S의 인수를 정규식으로 처리한다. --raw generate default raw diff output 기본 raw diff 출력을 생성한다. --relative exclude changes outside and output relative to given directory 주어진 디렉토리 외부의 변경을 제외하고 상대적으로 출력한다. --rename-empty use empty blobs as rename source 빈 blob을 이름 변경 소스로 사용한다. --rotate-to show the change in specified path first 지정된 경로의 변경을 먼저 표시한다. --shortstat generate summary diffstat 요약 diffstat을 생성한다. --skip-to skip the output to the specified path 지정된 경로까지의 출력을 건너뛴다. --src-prefix use given prefix for source 소스에 주어진 접두사를 사용한다. --stat generate diffstat instead of patch 패치 대신 diffstat을 생성한다. --stat-count generate diffstat with limited lines 제한된 라인으로 diffstat을 생성한다. --stat-graph-width generate diffstat with a given graph width 주어진 그래프 너비로 diffstat을 생성한다. --stat-width generate diffstat with a given width 주어진 너비로 diffstat을 생성한다. --submodule select output format for submodule differences 서브모듈 차이에 대한 출력 형식을 선택한다. --summary generate condensed summary of extended header information 확장 헤더 정보의 간결한 요약을 생성한다. --text -a treat all files as text 모든 파일을 텍스트로 취급한다. --textconv allow external text conversion filters to be run when comparing binary files 바이너리 파일 비교 시 외부 텍스트 변환 필터 실행을 허용한다. --unified -U generate diff with given lines of context 주어진 라인 수의 컨텍스트로 diff를 생성한다. --word-diff show word diff 단어 단위 diff를 표시한다. --word-diff-regex specify what constitutes a word 단어를 구성하는 것을 지정한다. --ws-error-highlight specify where to highlight whitespace errors 공백 오류를 강조 표시할 위치를 지정한다. -G look for differences whose added or removed line matches the given regex 추가되거나 제거된 라인이 주어진 정규식과 일치하는 차이점을 찾는다. -O output patch in the order of glob-pattern lines in given file 주어진 파일의 glob-패턴 라인 순서대로 패치를 출력한다. -R do a reverse diff 역방향 diff를 수행한다. -S look for differences that add or remove the given string 주어진 문자열을 추가하거나 제거하는 차이점을 찾는다. -l limit number of rename/copy targets to run 실행할 이름 변경/복사 대상 수를 제한한다. -z use NUL termination on output 출력에 NUL 종료를 사용한다. ","date":"2025-03-30T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-30-git-stash-how-to-use/","title":"Git Stash"},{"content":"📌개요 Dynamic Binding(동적 바인딩)은 Java의 객체지향 프로그래밍에서 핵심적인 개념이다. 런타임에 호출될 메서드를 결정하는 과정을 의미한다. 이 개념은 Polymorphism(다형성)을 구현하는 데 필수적이며, 코드의 유연성과 확장성을 높이는 데 기여한다.\n📌내용 기본 개념 정적 바인딩은 컴파일 시점에 호출될 메서드가 결정되는 반면, 동적 바인딩은 런타임에 결정된다. Java에서 일반적인 메서드 호출은 정적 바인딩으로 처리되지만, 오버라이딩된 메서드 호출은 동적 바인딩으로 처리된다. 동적 바인딩은 다형성을 구현하는 핵심적인 매커니즘이다. 상위 클래스 타입의 참조 변수로 하위 클래스 객체를 참조할 때, 런타임에 실제 객체의 타입에 따라 호출될 메서드가 결정된다. 작동 방식 메서드 테이블 Java의 객체는 Method Table(메서드 테이블)이라는 특별한 테이블을 가지고 있다. 메서드 테이블은 객체의 클래스에 정의된 메서드들의 주소를 저장한다. 하위 클래스에서 상위 클래스의 메서드를 오버라이딩하면 메서드 테이블에서 해당 메서드의 주소가 하위 클래스의 메서드 주소로 갱신된다. 런타임 메서드 결정 JVM은 런타임에 객체의 메서드를 호출할 때 메서드 테이블을 참조하여 실제 객체의 타입에 맞는 메서드를 호출한다. 이를 통해 상위 클래스 타입의 참조 변수로 하위 클래스 객체의 메서드를 호출할 수 있다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Animal { public void sound() { System.out.println(\u0026#34;동물이 소리를 낸다!\u0026#34;) } } class Dog extends Animal { @Override public void sound() { System.out.println(\u0026#34;강아지가 멍멍 짖는다.\u0026#34;); } } class Cat extends Animal { @Override public void sound() { System.out.println(\u0026#34;고양이가 야옹 운다.\u0026#34;); } } public class DynamicBindingExample { public static void main(String[] args) { Animal animal1 = new Animal(); Animal animal2 = new Dog(); // 다형성 Animal animal3 = new Cat(); // 다형성 animal1.sound(); // 동물이 소리를 낸다! animal2.sound(); // 강아지가 멍멍 짖는다. (동적 바인딩) animal3.sound(); // 고양이가 야옹 운다. (동적 바인딩) } } ⚙️EndNote 메서드 테이블 Java의 메서드 테이블은 객체 지향 프로그래밍에서 동적 바인딩을 구현하는 데 핵심적인 역할을 하는 자료 구조이다. 이 테이블은 객체의 클래스에 정의된 메서드들의 주소를 저장하며, 런타임에 어떤 메서드를 호출해야 하는지 결정하는 데 사용된다.\n기본 개념 메서드 주소 저장 메서드 테이블은 객체의 클래스에 정의된 메서드들의 실제 메모리 주소를 저장한다. 이를 통해 JVM은 런타임 메서드를 호출할 때 실제 실행될 메서드의 위치를 빠르게 찾을 수 있다. 상속과 오버라이딩 하위 클래스가 상위 클래스의 메서드를 오버라이딩하면, 메서드 테이블에서 해당 메서드의 주소가 하위 클래스의 메서드 주소로 갱신된다. 이를 통해 상위 클래스 타입의 참조 변수로 하위 클래스 객체의 메서드를 호출할 수 있다. 동적 바인딩 구현 JVM은 런타임에 객체의 메서드를 호출할 때, 메서드 테이블을 참조하여 실제 객체의 타입에 맞는 메서드를 호출한다. 이를 통해 다형성을 구현할 수 있다. 작동 방식 객체 생성 객체가 생성될 때, 해당 객체의 클래스에 대한 메서드 테이블이 생성된다. 메서드 테이블은 클래스에 정의된 모든 메서드의 주소를 저장한다. 메서드 호출 객체의 메서드를 호출할 때, JVM은 객체의 메서드 테이블을 참조한다. 메서드 테이블에서 호출할 때 메서드의 주소를 찾아 해당 메서드를 실행한다. 오버라이딩된 메서드 호출 오버라이딩된 메서드를 호출할 때, JVM은 객체의 실제 타입을 확인하고 해당 타입의 메서드 테이블을 참조한다. 이를 통해 하위 클래스에서 오버라이딩된 메서드를 호출할 수 있다. ","date":"2025-03-29T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-29-java-object-dynamic-binding/","title":"Java 객체지향: 동적 바인딩"},{"content":"📌개요 Java의 객체지향 프로그래밍에서 Overload(오버로드)와 Override(오버라이딩)은 Polymorphism(다형성)을 구현하는 핵심적인 개념이다. 이 두 개념을 이해하면 코드의 유연성과 재사용성을 높일 수 있다.\n📌내용 Overload 같은 이름으로 선언할 수 있는 이유는 아래 메서드 시그니처에서 확인하자.\n하나의 클래스 내에서 같은 이름을 가진 여러 개의 메서드를 정의하는 것. 같은 기능을 수행하지만, 다양한 형태의 입력을 처리할 목적으로 사용된다. 다양한 자료형의 데이터를 처리하거나, 여러 개의 매개변수를 사용하는 메서드를 만들 때 유용하다.\n메서드 이름은 동일해야 한다. 매개변수의 타입, 개수 또는 순서가 달라야 한다. 반환 타입은 오버로드와 관련이 없다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class Calculator { // 정수 덧셈 public int add(int a, int b) { return a + b; } // 실수 덧셈 public double add(double a, double b) { return a + b; } // 세 개의 정수 덧셈 public int add(int a, int b, int c) { return a + b + c; } public static void main(String[] args) { Calculator calc = new Calculator(); System.out.println(calc.add(1, 2)); // 3 System.out.println(calc.add(1.5, 2.5)) // 4.0 System.out.println(calc.add(1, 2, 3)) // 6 } } Override 상위 클래스에서 상속 받은 메서드를 하위 클래스에서 재정의하는 것. @Override 어노테이션을 사용하여 오버라이딩을 명시할 수 있다. 상속 받은 클래스의 기능을 확장하거나 변경해야 할 때 유용하다. 특히, 추상 클래스나 인터페이스를 구현할 때 필수적으로 사용된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Animal { public void sound() { System.out.println(\u0026#34;동물이 소리를 낸다!\u0026#34;); } } class Dog extends Animal() { @Override public void sound() { System.out.println(\u0026#34;강아지가 멍멍 짖는다.\u0026#34;); } } public class OverrideExample { public static void main(String[] args) { Animal animal = new Animal(); Dog dog = new Dog(); animal.sound(); // 동물이 소리를 낸다! dog.sound(); // 강아지가 멍멍 짖는다. } } 오버로드와 오버라이딩의 차이점 특징 Overload(오버로드) Override(오버라이딩) 클래스 관계 동일 클래스 상속 관계 (상위-하위) 메서드 이름 동일 동일 매개변수 목록 다름 동일 반환 타입 무관 동일 목적 다양한 입력 처리 메서드 재정의 메서드 시그니처 Method Signature(메서드 시그니처)는 Java 가상 머신 JVM(Java Virtual Machine)이 메서드를 정확하게 식별하는 데 핵심적인 역할을 한다. 또한, 오버로딩과 오버라이딩과 같은 객체 지향 프로그래밍 개념에도 큰 영향을 미친다.\nJVM의 메서드 식별 JVM은 메서드를 호출할 때 메서드 시그니처를 사용하여 어떤 메서드를 실행해야 하는지 결정한다. 메서드 시그니처는 메서드 이름과 매개변수의 타입, 개수, 순서로 구성되며 JVM은 이 정보를 바탕으로 고유하게 식별한다. 반환 타입은 메서드 시그니처에 포함되지 않으므로 반환 타입만 다른 메서드는 JVM에서 구별할 수 없다. 메서드 시그니처와 오버로딩 오버로딩은 같은 이름의 메서드를 여러 개 정의하는 건데 이때 메서드 시그니처가 달라야 한다. 컴파일러는 메서드 호출 시 전달된 인수의 타입과 개수를 기반으로 적절한 메서드를 선택하며 이는 메서드 시그니처를 통해 가능하다. 메서드 시그니처가 다르다는 것은 매개변수의 타입, 개수, 순서 중 하나 이상이 다르다는 것을 의미한다. 메서드 시그니처와 오버라이딩 오버라이딩은 상위 클래스의 메서드를 하위 클래스에서 재정의하는 건데 이때 메서드 시그니처가 동일해야 한다. JVM은 런타임에 실제 객체의 타입을 확인하고, 하위 클래스에서 오버라이딩된 메서드가 있다면 해당 메서드를 호출한다. @Override 어노테이션은 오버라이딩 규칙을 준수했는지 컴파일러에게 확인하도록 지시한다. ","date":"2025-03-29T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-29-java-object-oriented-overload-and-override/","title":"Java 객체지향: 오버로드와 오버라이딩"},{"content":"📌개요 Java의 Class는 OOP(객체 지향 프로그래밍)의 핵심 요소이다. Field(필드), Constructor(생성자), Method (메서드)라는 세 가지 주요 구성 멤버로 이루어져 있다. 각 구성 멤버는 클래스의 속성과 동작을 정의하는 데 중요한 역할을 한다.\n📌내용 Field 클래스 내에서 선언된 변수를 의미하며, 객체의 속성 또는 상태를 나타낸다. 객체의 데이터를 저장하고 관리한다.\n1 2 3 4 public class Person { String name; // 필드: 이름 int age; // 필드: 나이 } Constructor 객체가 생성될 때 호출되는 특별한 메서드\n객체의 초기화를 담당하며, 필드의 초기값을 설정한다. 반환 타입이 없다. 클래스의 이름과 동일한 이름을 가진다. 여러 개의 생성자를 정의하여 다양한 방법으로 객체를 초기화할 수 있다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class Person { String name; int age; // 기본 생성자 public Person() { name = \u0026#34;이름 없음\u0026#34;; age = 0; } // 매개변수가 있는 생성자 public Person(String name, int age) { this.name = name; this.age = age; } } 생성자를 정의하지 않았을 때 Java의 기본 동작 Java에서 클래스를 정의할 때 생성자를 명시적으로 작성하지 않으면, 컴파일러는 자동으로 기본 생성자(default contructor)를 생성한다. 기본 생성자는 매개변수가 없는 생성자이며, 클래스의 필드를 기본값으로 초기화한다.\n기본 생성자의 특징 기본 생성자는 어떠한 매개변수도 받지 않는다. 클래스에 생성자가 하나도 정의되어 있지 않은 경우에만 컴파일러가 자동으로 생성한다. 객체 생성 시 필드를 기본값으로 초기화한다. 숫자 타입(int, double 등): 0 또는 0.0 boolean 타입: false 참조 타입(String, 객체 등): null 1 2 3 4 5 6 7 8 9 10 11 12 13 public class Person { String name; int age; public void introduce() { System.out.println(\u0026#34;이름: \u0026#34; + name + \u0026#34;, 나이: \u0026#34; + age); } public static void main(String[] args) { Person person = new Person(); // 기본 생성자 호출 person.introduce(); // 이름: null, 나이: 0 } } 클래스에 매개변수가 있는 생성자라도 하나 이상 정의하면 컴파일러는 기본 생성자를 자동으로 생성하지 않는다. 이 경우 매개 변수가 없는 생성자가 필요한 경우 명시적으로 정의해야 한다. 기본 생성자는 객체를 생성하고 필드를 기본값으로 초기화하는 간단한 작업을 수행하지만, 때로는 객체 생성 시 특정 값으로 초기화해야 하는 경우가 있다. 이 경우 매개변수가 있는 생성자를 정의해야 한다. Method 클래스 내에서 정의된 함수를 의미하며, 객체의 동작 또는 기능을 나타낸다. 객체의 행위를 정의하고, 필요한 연산을 수행한다.\n1 2 3 4 5 6 7 8 9 public class Person { String name; int age; // 메서드: 자기소개 public void introduce() { System.out.println(\u0026#34;제 이름은 \u0026#34; + name + \u0026#34;이고, \u0026#34; + age + \u0026#34;살입니다.\u0026#34;); } } 예제 코드 종합 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public class Person { String name; int age; // 기본 생성자 public Person() { name = \u0026#34;이름 없음\u0026#34;; age = 0; } // 매개변수가 있는 생성자 public Person(String name, int age) { this.name = name; this.age = age; } // 메서드: 자기소개 public void introduce() { System.out.println(\u0026#34;제 이름은 \u0026#34; + name + \u0026#34;이고, \u0026#34; + age + \u0026#34;살입니다.\u0026#34;); } public static void main(String[] args) { // 기본 생성자를 사용하여 객체 생성 Person person1 = new Person(); person1.introduce(); // 제 이름은 이름 없음이고, 0살입니다. // 매개 변수가 있는 생성자를 사용하여 객체 생성 Person person2 = new Person(\u0026#34;홍길동\u0026#34;, 30); person2.introduce(); // 제 이름은 홍길동이고, 30살입니다. } } ⚙️EndNote 메서드 시그니처 메서드의 이름이 같더라도 매개변수의 타입, 개수, 순서가 다르면 여러 개의 생성자를 선언할 수 있다. 이는 Java의 Method Overloading 오버로딩 개념에 해당되며, 생성자도 메서드의 한 종류이기 때문에 오버로딩이 적용된다.\n","date":"2025-03-29T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-29-java-object-oriented-members-of-class/","title":"Java 객체지향: 클래스의 구성 멤버"},{"content":"📌개요 Java의 Variable Arguments(가변 인자)는 메서드에 전달되는 인자의 개수를 유동적으로 처리할 수 있는 기능이다. 가변 인자를 사용하면 메서드 호출 시 인자의 개수를 미리 정하지 않아도 되므로, 코드의 유연성과 가독성을 높일 수 있다.\n📌내용 Variable Arguments 가변 인자는 메서드 매개변수 목록에서 타입... 변수명 형태로 선언된다.\n1 2 3 public void printNumbers(int... numbers) { // ... } 가변 인자는 내부적으로 배열로 처리된다. 따라서 메서드 내부에서는 가변 인자를 배열처럼 다룰 수 있다. 가변 인자에는 0개 이상의 인자를 전달할 수 있고 개수 제한이 없다. 가변 인자는 메서드 매개변수 목록에서 반드시 마지막에 위치해야 한다. 동작 방식 메서드를 호출할 때 가변 인자에 해당하는 인자들을 쉼표로 구분하여 전달한다. 컴파일러는 전달된 인자들을 배열로 묶어서 메서드에 전달한다. 메서드 내부에서는 전달 받은 배열을 사용하여 작업을 수행한다. 가변 인자의 장점 다양한 개수의 인자를 처리하는 메서드를 여러 개 오버로드할 필요 없이 하나의 메서드로 처리할 수 있어서 코드가 간결해진다. 메서드 호출 시 인자의 개수를 유연하게 조절할 수 있다. 배열을 명시적으로 생성하지 않고도 여러 인자를 쉽게 전달할 수 있다. 📌대체 또는 보완 가변 인자는 편리하지만, 내부적으로 배열을 생성하므로 많은 인자가 자주 전달되는 경우에는 성능에 영향을 미칠 수 있다. 이러한 성능 문제를 해결하거나 완화할 수 있는 몇 가지 방법이 있다.\n배열 또는 컬렉션 직접 전달 가변 인자 대신 배열이나 List와 같은 컬렉션을 직접 메서드에 전달하는 것이 좋다. 메서드 호출 시마다 배열을 새로 생성하는 오버헤드를 줄일 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public void processNumbers(int[] numbers) { // ... } public void processNumbers(List\u0026lt;Integer\u0026gt; numbers) { // ... } int[] numbers = {1, 2, 3, 4, 5}; processNumbers(numbers); List\u0026lt;Integer\u0026gt; numberList = Array.asList(1, 2, 3, 4, 5); processNumbers(numberList); 메서드 오버로딩 자주 사용되는 인자 개수에 따라 메서드를 오버로딩하여 가변 인자 사용을 최소화할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public void processNumbers(int a) { // ... } public void processNumbers(int a, int b) { // ... } public void processNumbers(int a, int b, int c) { // ... } public void processNumbers(int... numbers) { // ... } 성능에 민감한 경우 특수화된 메서드 사용 성능이 매우 중요한 경우에는 가변 인자를 전혀 사용하지 않고 특정 개수의 인자를 받는 특수화된 메서드를 사용하는 것이 좋다. 예를 들어, 인자가 최대 10개까지 자주 사용된다면, 인자가 1개부터 10개까지인 메서드를 모두 구현할 수 있다.\n📌사용 시 주의사항 매개변수의 마지막에 위치해야 한다 가변 인자는 메서드 매개변수 목록에서 반드시 마지막에 위치해야 한다. 그렇지 않으면 컴파일 에러가 발생한다.\n오류 예시 1 2 3 4 5 6 7 8 9 10 11 public class VarargsErrorExample { public static void printValues(int... numbers, String message) { // ... } public static void main(String[] args) { printValues(1, 2, 3, \u0026#34;Hello\u0026#34;); } } // VarargsErrorExample.java:2: error: variable arity parameter must be the last parameter public static void printValues(int... numbers, String message) { ^ 1 error 올바른 사용 1 2 3 4 5 6 7 8 9 10 11 12 public class VarargsCorrectExample { public static void printValues(String message, int... numbers) { System.out.println(message); for(int number : numbers) { System.out.println(number); } } public static void main(String[] args) { printValues(\u0026#34;Numbers:\u0026#34;, 1, 2, 3); } } 오버로딩 시 주의 가변 인자를 사용하는 메서드를 오버로딩할 때 모호성이 발생할 수 있으므로 주의해야 한다.\n오류 예시 비단 예시만이 아니라 모호성 오류가 발생할 수 있는 케이스는 상속 관계와 제네릭, null 값의 전달, 와일드 카드 제네릭 등 더 있지만 간단한 예제로 알아본다.\nJava는 기본 자료형인 int와 래퍼 클래스인 Integer 사이의 자동 변환을 지원한다. 이 때문에 int 값을 Integer로 자동 변환하여 Integer... 매개 변수에 전달할 수 있게 된다.\n컴파일러는 전달된 인자들을 배열로 묶어야 하는데 자동 박싱 때문에 int 배열 또는 Integer 배열을 만들 수 있는 두 가지 선택지가 생긴다.\n컴파일러는 어떤 배열을 생성해야 할지 결정할 수 없으므로 모호성 오류가 발생한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class VarargsOverloadingError { public static void printValues(int... numbers) { System.out.println(\u0026#34;int 가변 인자 메서드\u0026#34;); for(int number : numbers) { System.out.print(number + \u0026#34; \u0026#34;); } System.out.println(); } public static void printValues(Integer... numbers) { System.out.println(\u0026#34;Integer 가변 인자 메서드\u0026#34;); for(Integer number : numbers) { System.out.print(number + \u0026#34; \u0026#34;); } System.out.println(); } public static void main(String[] args) { printValues(1, 2, 3); // 컴파일 오류, 어떤 메서드를 호출해야 할지 모호함 } } // reference to printValues is ambiguous both method printValues(int...) in VarargsOverloadingError and method printValues(Integer...) in VarargsOverloadingError match 올바른 사용 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class VarargsOverloadingSuccess { public setatic void printValues(String message, int... numbers) { System.out.println(message); for(int number : numbers) { System.out.print(number + \u0026#34; \u0026#34;); } System.out.println(); } public static void printValues(int... numbers) { System.out.println(\u0026#34;int 가변 인자 메서드\u0026#34;); for(int number : numbers) { System.out.print(number + \u0026#34; \u0026#34;); } System.out.println(); } public static void main(String[] args) { printValues(1, 2, 3); // int 가변 인자 메서드 호출 printValues(\u0026#34;Numbers:\u0026#34;, 4, 5, 6); // String, int 가변 인자 메서드 호출 } } 🎯결론 가변 인자는 편리하지만 성능 문제가 발생할 수 있으므로 상횡에 따라 적절한 방법을 선택하여 사용하는 것이 중요하다. 특히, 많은 인자가 자주 전달되는 경우에는 배열 또는 컬렉션을 직접 전달하거나 메서드 오버로딩과 같은 방법을 고려하는 것이 좋다. 성능이 매우 중요한 경우에는 가변 인자를 전혀 사용하지 않고 특정 개수의 인자를 받는 특수화된 메서드를 사용하는 것이 좋다. ⚙️EndNote 컬렉션 빌더 Java9부터 도입된 List.of() 또는 Stream.of()와 같은 컬렉션 빌더를 사용하여 불변 컬렉션을 생성하고 이를 메서드에 전달할 수 있다.\n1 2 3 4 5 public void processNumbers(List\u0026lt;Integer\u0026gt; numbers) { // ... } processNumbers(List.of(1, 2, 3, 4, 5)); 컬렉션 빌더는 가변 인자를 내부적으로 사용하지만, 몇 가지 중요한 차이점으로 성능 및 메모리 관리 측면에서 가변 인자를 직접 사용하는 것과 다르다.\n불변 컬렉션 생성 컬렉션 빌더는 수정할 수 없는 불변 컬렉션을 생성한다. 생성 후에는 요소를 추가, 삭제, 변경할 수 없다. 컬렉션의 크기가 고정되어 있음을 의미하며, 런타임에 동적으로 크기를 조정할 필요가 없다. 최적화된 구현 소수의 요소를 처리하는 경우에 대해 특수화된 구현을 제공하여 배열 생성 및 복사 비용을 최소화한다. 메모리 효율성 불변 컬렉션은 크기가 고정되어 있으므로 필요한 메모리를 미리 할당하고 재할당할 필요가 없다. 또한, 불변성은 메모리 공유 및 캐싱을 용이하게 하여 메모리 효율성을 높일 수 있다. 가변 인자의 직접 사용과의 차이 가변 인자는 인자의 개수에 따라 매번 새로운 배열을 생성하므로 메모리 오버헤드가 발생할 수 있다. 컬렉션 빌더는 특정 범위(예: 0~10개)의 인자를 처리할 때 미리 할당된 메모리를 사용하여 성능을 최적화한다. ","date":"2025-03-29T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-29-java-syntax-varargs/","title":"Java 문법: 가변 인자"},{"content":"📌개요 merge 기능이 있지만 rebase를 사용하는 목적? 사용에 주의가 필요한 기능이라면, 협업에선 어떤 상황에 사용되는지?\n📌내용 Rebase 사용 목적 rebase는 개인적으로 작업하는 브랜치나 깔끔한 히스토리를 유지해야 하는 경우에 유용하게 사용된다. 하지만 공유 브랜치에서는 협업 시 혼란을 야기할 수 있으므로 사용에 신중해야 한다. rebase는 브랜치 변경 사항을 다른 브랜치에 통합하는 강력한 도구로, 특히 다음과 같은 목적을 위해 사용된다.\nmerge와 달리 불필요한 병합 커밋을 생성하지 않고 히스토리를 재작성하여 이해하기 쉽고 추적하기 용이한 선형적인 히스토리를 유지한다. 선형적인 히스토리는 코드 변경 사항을 순차적으로 검토할 수 있도록 도와 코드 리뷰 효율성을 높인다. git bisect와 같은 도구를 사용하여 문제 발생 커밋을 빠르게 찾고 해결하는 데 유용하다. interactive rebase를 통해 커밋 순서 변경, 병합, 삭제 등 다양한 방법으로 커밋 히스토리를 정리하고 관리할 수 있다. Rebase 대체 기능 및 비교 rebase와 유사한 기능을 수행하는 merge가 있으며, 두 기능의 차이점은 다음과 같다.\n기능 특징 장점 단점 merge 브랜치 병합 시 병합 커밋 생성 히스토리 변경 없이 브랜치 병합 가능\n안정적인 협업환경 유지 복잡한 커밋 히스토리 생성\n코드 리뷰 및 문제 해결 어려움 rebase 커밋 히스토리 재작성 깔끔하고 선형적인 커밋 히스토리 유지\n코드 리뷰 및 문제 해결 용이\n커밋 정리 및 관리 용이 커밋 히스토리 변경으로 인한 잠재적 문제 발생 가능성\n숙련되지 않은 사용자에게는 복잡할 수 있음 merge는 히스토리 변경 없이 브랜치를 병합하고 안정적인 협업 환경을 유지하는데 적합하다. rebase는 깔끔하고 선형적인 히스토리를 유지하고 코드 리뷰 및 문제 해결 효율성을 높이는 데 적합하다. Rebase 사용 사례 기능 브랜치 정리, 코드 리뷰 효율성 향상, release 브랜치 관리, 옾느 소스 프로젝트 기여 등에 사용할 수 있다. main 브랜치 병합 전 Rebase를 통한 커밋 정리 여러 개발자가 feature-A라는 브랜치에서 동시에 작업한다고 생각해보자. 각 개발자는 자신의 작업을 커밋하고 원격 저장소에 푸시할 것이다. feature-A 브랜치를 main 브랜치에 병합하기 전에, 각 개발자는 자신의 커밋을 정리하고 main 브랜치의 최신 변경 사항을 반영하기 위해 rebase를 사용한다.\n개발자 A의 작업 개발자 A는 feature-A 브랜치에서 작업을 수행하고 다음과 같은 커밋을 생성한다. 커맷 1: 기능 A의 기본 구조 구현 커밋 2: 기능 A의 UI 개선 커밋 3: 기능 A의 버그 수정 개발자 A는 자신의 커밋을 원격 저장소에 푸시한다. 개발자 B의 작업 개발자 B도 feature-A 브랜치에서 작업을 수행하고 다음과 같은 커밋을 생성한다. 커밋 1: 기능 A의 데이터 처리 로직 구현 커밋 2: 기능 A의 성능 개선 개발자 B는 자신의 커밋을 원격 저장소에 푸시한다. 작업 후 merge 이전의 rebase 브랜치의 작업자가 본인만 있는 경우가 아니라면, 강제 푸시 이전 꼭 협업자와의 소통이 필수. 개발자 B 역시 동일하게 본인의 커밋을 수정할 수 있다.\n두서 없이 작업했던 본인의 커밋 내역을 main 브랜치에 합치기 위해 작업 내역을 정리하는 목적으로 사용할 수 있다.\n최신 변경 사항 가져오기 main 브랜치의 최신 변경 사항을 로컬 feature-A 브랜치에 가져온다.\n1 git pull origin main rebase 시작 rebase를 시작한다. 로컬 feature-A 브랜치의 커밋들을 main 브랜치의 최신 커밋 위에 재배치한다.\n1 git rebase origin/main 충돌 발생 시 해결 충돌이 발생하면 충돌 파일을 수정하고 git add {충돌난 파일} 명령어를 실행한다. git rebase --continue 명령으로 rebase를 계속 진행한다. interactive rebase (선택 사항) 커밋 히스토리를 정리한다.\ncommit을 수정하거나 커밋을 합치거나 불필요한 커밋을 삭제하는 등 1 git rebase -i origin/main 원격 저장소에 강제 푸시 정리된 커밋 히스토리를 원격 저장소에 강제 푸시한다. 이로 인해 협업 규격에 맞는 커밋으로 수정한 뒤 main 브랜치에 합칠 수 있게 된다.\n1 git push --force-with-lease origin feature-A 🎯결론 rebase는 개인 작업의 유연성과 협업 시 커밋 규격 준수를 가능하게 하는 강력한 도구이지만, 히스토리 변경으로 인한 위험성을 항상 염두에 두어야 한다. 따라서, rebase를 사용할 때는 팀원들과 충분히 소통하고 프로젝트의 특성을 고려해서 신중하게 결정해야 한다.\n⚙️EndNote origin main VS origin/main origin main origin main는 나눠서 봐야 한다. origin이라는 원본 저장소, main이라는 브랜치\n예를 들어, git pull origin main 명령이라면 origin이라는 저장소에서 main 브랜치를 내려 받는다.\norigin/main origin/main은 브랜치 자체를 의미한다. 로컬 저장소에 저장된 원격 저장소 origin의 main 브랜치의 상태를 나타내는 일종의 \u0026lsquo;읽기 전용\u0026rsquo; 스냅샷이다.\n예를 들어, git rebase origin/main 명령이라면 origin/main 브랜치에 대해 rebase를 실행한다.\n","date":"2025-03-27T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-27-git-rebase-purpose-and-alternative-features/","title":"Git Rebase 사용 목적 및 대체 기능"},{"content":"📌개요 탄젠트 법칙이 모든 삼각형에서 어떻게 성립하는지 증명을 정리\n📌내용 탄젠트 법칙은 삼각법에서 삼각형의 두 변과 그에 끼인 두 각 사이의 관계를 나타내는 공식이다. 삼각형의 변과 각을 구하는 데 유용하며, 사인 법칙(Sine Rule)이나 코사인 법칙(Cosine Rule)과 함께 삼각형 문제를 해결하는 데 사용된다.\n탄젠트 법칙의 증명 $\\triangle ABC$에서 변의 길이를 $a,b,c$ 각각의 대변에 대응하는 각을 $A,B,C$라고 할 때, 탄젠트 법칙은 다음과 같이 표현된다.\n$$ \\begin{gathered} \\frac{a-b}{a+b} = \\frac{\\tan{(\\frac{A-B}{2})}}{\\tan{(\\frac{A+B}{2})}} \\end{gathered} $$ 즉, 두 변의 차와 합의 비율이 두 각의 반각에 대한 탄젠트 비율과 같다는 것을 의미한다.\n탄젠트 법칙을 삼각함수로 변형 삼각형의 내각합 성질에 의해\n$$ \\begin{gathered} A + B + C = 180^\\circ \\\\ A + B = 180^\\circ - C \\end{gathered} $$사인 법칙 활용 삼각형에서 사인 법칙을 적용한다.\n$$ \\begin{gathered} \\frac{a}{\\sin{A}} = \\frac{b}{\\sin{B}} = \\frac{c}{\\sin{C}} \\\\ \\frac{a}{b} = \\frac{\\sin{A}}{\\sin{B}} \\end{gathered} $$ 양 변을 변형한다. $$ \\begin{gathered} \\frac{a-b}{a+b} = \\frac{\\sin{A}-\\sin{B}}{\\sin{A}+\\sin{B}} \\end{gathered} $$삼각함수 변형 사인 함수의 덧셈과 뺄셈 공식 $$ \\begin{gathered} \\sin{A} - \\sin{B} = 2\\cos{\\frac{A+B}{2}}\\sin{\\frac{A-B}{2}} \\\\ \\sin{A} + \\sin{B} = 2\\sin{\\frac{A+B}{2}}\\cos{\\frac{A-B}{2}} \\end{gathered} $$위의 결과를 이용하여 변형한다. $$ \\frac{a-b}{a+b} = \\frac{2\\cos{\\frac{A+B}{2}}\\sin{\\frac{A-B}{2}}} {2\\sin{\\frac{A+B}{2}}\\cos{\\frac{A-B}{2}}} $$ 양 변의 2를 약분한다. $$ \\frac{a-b}{a+b} = \\frac{\\cos{\\frac{A+B}{2}}\\sin{\\frac{A-B}{2}}} {\\sin{\\frac{A+B}{2}}\\cos{\\frac{A-B}{2}}} $$ 분자와 분모를 나누어 확인한다.\n분자 : $\\cos{\\frac{A+B}{2}}\\sin{\\frac{A-B}{2}}$, 분모 : $\\sin{\\frac{A+B}{2}}\\cos{\\frac{A-B}{2}}$\n그럼 이 식은 다음과 같이 분리할 수 있다.\n$$ \\frac{\\sin{\\frac{A-B}{2}}}{\\cos{\\frac{A-B}{2}}} \\times \\frac{\\cos{\\frac{A+B}{2}}}{\\sin{\\frac{A+B}{2}}} $$ 이때 첫 번째 항과 두 번째 항을 다음과 같이 정리할 수 있다.\n첫 번째 항 : $\\tan{\\frac{A-B}{2}}$, 두 번째 항 : $\\frac{1}{\\tan{\\frac{A+B}{2}}}$\n즉, 전체 식이 다음과 같이 정리된다. $$ \\begin{gathered} \\frac{\\tan{\\frac{A-B}{2}}}{\\tan{\\frac{A+B}{2}}} \\\\ \\therefore \\frac{a-b}{a+b} = \\frac{\\tan{(\\frac{A-B}{2})}}{\\tan{(\\frac{A+B}{2})}} \\end{gathered} $$⚙️EndNote 사인 법칙 삼각형의 각 변과 대응하는 내각의 사인값의 비율이 같음을 의미한다. $$ \\frac{a}{\\sin{A}} = \\frac{b}{\\sin{B}} = \\frac{c}{\\sin{C}} $$사인 덧셈/뺄셈 공식 $$ \\begin{gathered} \\sin{A} - \\sin{B} = 2\\cos{\\frac{A+B}{2}}\\sin{\\frac{A-B}{2}} \\\\ \\sin{A} + \\sin{B} = 2\\sin{\\frac{A+B}{2}}\\cos{\\frac{A-B}{2}} \\end{gathered} $$탄젠트 함수의 정의 탄젠트 함수의 정의는 다음과 같다. $$\\tan{x} = \\frac{\\sin{x}}{\\cos{x}}$$","date":"2025-03-15T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-15-proof-of-the-law-of-tangent/","title":"삼각함수의 탄젠트 법칙 증명"},{"content":"📌개요 코사인 법칙이 모든 삼각형에서 어떻게 성립하는지 증명을 정리\n📌내용 코사인 법칙은 삼각형에서 두 변의 길이와 끼인각(사이의 각)을 이용하여 나머지 한 변의 길이를 구할 수 있는 중요한 공식이다.\n$$a^2=b^2+c^2−2bc\\cos A$$위 식은 직각삼각형뿐만 아니라 모든 삼각형에서 성립하는데, 이는 피타고라스 정리를 일반 삼각형으로 확장한 개념이라고 볼 수 있다.\n코사인 법칙 증명 점 A에서 BC에 대한 수선의 발을 내림 $\\triangle ABC$의 점$A$에서 $\\overline{BC}$에 수선의 발을 내려서 점 $D$를 만든다고 하자. 이때 삼각형은 두 개의 직각삼각형 $\\triangle{ABD}$와 $\\triangle{ACD}$로 나뉜다.\n직각삼각형에서 피타고라스 정리 적용 삼각형의 구성 요소 정리\n$\\angle{ABC} = B$라고 하자. 점 $A$에서 $\\overline{BC}$에 내린 수선의 발을 $D$라고 하자. 이때 $\\overline{BD} = c\\cos{B}$, $\\overline{AD} = c\\sin{B}$이다. 피타고라스 정리 사용\n$\\triangle{ACD}$에서 $\\overline{AD}^2+\\overline{CD}^2=\\overline{AC}^2$ $$ \\begin{gathered} b^2 = (c\\sin B)^2 + (a-c\\cos B)^2 \\\\ \\end{gathered} $$ 위 피타고라스 정리를 전개 $$b^2=c^2\\sin^2{B} + (a^2-2ac\\cos{B}+c^2\\cos^2{B})$$ 삼각함수의 항을 묶어서 정리 $$b^2=c^2(\\sin^2{B}+\\cos^2{B})+a^2-2ac\\cos{B}$$ 삼각함수의 기본 항등식 $\\sin^2{B} + \\cos^2{B} = 1$을 적용 $$b^2 = c^2+a^2-2ac\\cos{B}$$ 코사인 값 구하기 위 식을 코사인에 대해 정리\n$$\\cos{B} = \\frac{c^2+a^2-b^2}{2ac}$$⚙️EndNote 단위원(Unit Circle) 개념 단위원이란 반지름이 1인 원으로, 원점 (0,0)을 중심으로 하는 원이다.\n단위원의 방정식은 다음과 같이 주어진다.\n$$x^2+y^2=1$$삼각함수를 정의할 때, 단위원을 사용하면 다음과 같이 표현할 수 있다. 한 점 $P(x,y)$가 원 위에 있고, 이 점이 원점과 이루는 각이 $\\theta$일 때,\n$$ \\begin{gathered} x = \\cos{\\theta} \\\\ y = \\sin{\\theta} \\end{gathered} $$단위원의 정의에 따라 점 $P(x,y)$는 항상 원 위에 있어야 하므로, 다음이 성립한다.\n$$\\cos^2{\\theta} + \\sin^2{\\theta} = 1$$즉, 단위원 위의 모든 점이 원의 방정식을 만족하기 때문에 위의 기본 삼각함수 항등식이 성립한다.\n","date":"2025-03-13T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-13-proof-of-the-law-of-cosine/","title":"삼각함수의 코사인 법칙 증명"},{"content":"📌개요 사인 법칙이 모든 삼각형에서 어떻게 성립하는지 외접원을 이용한 증명을 정리\n📌내용 사인 법칙 증명 모든 삼각형에는 외접원이 존재한다. $\\triangle{ABC}$가 있고, 이 삼각형을 둘러싸는 외접원(circumcircle)을 고려하자. 외접원의 반지름을 R이라고 했을 때, 사인 법칙은 다음과 같이 성립한다. $$ \\frac{a}{\\sin{A}} = \\frac{b}{\\sin{B}} = \\frac{c}{\\sin{C}} = 2R $$ 여기서 $A,B,C$는 삼각형의 각(angle) $a,b,c$는 각각 그에 대응하는 변의 길이(length) A \u0026lt; 90 $A$가 $\\overparen{BC}$에 대한 원주각을 이루는 점이다. 외접원의 중심을 포함하며 $\\overparen{BC}$에 대한 원주각이 동일하도록 $A\u0026rsquo;$을 설정하면, $\\triangle{ABC}$의 외접원의 지름($2R$)을 빗변으로 갖는 $\\triangle{A\u0026rsquo;BC}$을 확인할 수 있다.\n$$ \\begin{gathered} \\angle A = \\angle A' \\\\ \\sin A' = \\frac{a}{2R} = \\sin A \\\\ \\therefore 2R = \\frac{a}{\\sin A} \\end{gathered} $$ 같은 방식으로 $B$, $C$에 대해서도 확인할 수 있다.\nA = 90 단위 원(circle of unit radius)\n$\\sin{\\theta}$ 는 각 $\\theta$ 에 대한 y좌표 $\\cos{\\theta}$ 는 각 $\\theta$ 에 대한 x좌표 $\\theta = \\frac{\\pi}{2}$​(즉, $90^\\circ$)일 때 좌표 확인, y축의 가장 위쪽에 해당하는 점 $$(\\cos{\\frac{\\pi}{2}},\\sin{\\frac{\\pi}{2}}) = (0,1)$$ 직각인 $A$를 이용해서 $\\sin A$가 1임을 확인한다. $$\\sin{A} = \\sin{90^\\circ} = 1$$ 그 다음 반원에 대한 지름 $R$, 빗변 $a$를 이용해 확인한다. $$ \\begin{gathered} a = 2R = 2R \\times 1 = 2R \\times \\sin{A} \\\\ a = 2R \\times \\sin{A} \\\\ \\therefore 2R = \\frac{a}{\\sin{A}} \\end{gathered} $$ 같은 방식으로 $B$, $C$에 대해서도 확인할 수 있다.\nA \u0026gt; 90 내접 사각형의 성질을 활용하여 마주보는 두 각의 합이 $\\pi (180^\\circ)$임을 이용한다.\n$\\triangle{ABC}$의 빗변인 $a = \\overline {BC}$ 를 한 변으로 갖는 $\\triangle{A\u0026rsquo;BC}$(직각삼각형)를 설정한다. 외접원에 내접하는 사각형 $ABA\u0026rsquo;C$의 성질을 이용한다. $$\\angle A + \\angle A' = 180 ^\\circ$$ $A$, $A\u0026rsquo;$은 서로 다른 각이지만, 삼각함수의 성질에 의해 $A$, $A\u0026rsquo;$이 보완각 관계를 가지므로 사인 값이 동일하다. $\\sin$ 함수는 원을 기준으로 y축 값을 측정하는 거라서, 비록 $A$와 $A\u0026rsquo;$이 다르게 생겼다고 해도 사인 값 자체는 동일하게 유지된다. $$ \\begin{gathered} \\sin{A'} = \\frac{a}{2R} = \\sin{(180 ^\\circ - A)} = \\sin{A} \\\\ \\sin{A} = \\frac{a}{2R} \\\\ \\therefore 2R = \\frac{a}{\\sin{A}} \\end{gathered} $$ 같은 방식으로 $B$, $C$에 대해서도 확인할 수 있다.\n⚙️EndNote 보완각 성질의 본질 단위원(Unit Circle)을 생각하자. $\\angle A$가 1사분면에 있을 때, 이를 2사분면으로 반사 시키면 $180^\\circ - A$가 된다. 이때 삼각비의 정의에 따라 $\\sin (180 ^\\circ - A) = \\sin A$ 왜냐하면, 단위원에서 y좌표는 그대로 유지되기 때문이다. ","date":"2025-03-12T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-12-proof-of-the-law-of-sine/","title":"삼각함수의 사인 법칙 증명"},{"content":"📌개요 다양한 알고리즘(암호학, 해시 함수, 알고리즘 최적화 등)에 활용할 수 있는 모듈러 연산(Modular Arithmetic)의 동작 원리를 알아 본다.\n📌내용 모듈러 연산(Modular Arithmetic)은 숫자를 특정 값(모듈러, modulus)으로 나눈 나머지를 구하는 연산이다.\n기본 개념 $$A \\mod M = R$$ $A$ : 피연산자(나누려는 수) $M$ : 모듈러 값(나누는 수) $R$ : 나머지(결과값) 예를 들어, $17 \\mod 5$를 계산하면\n$$17÷5=3(몫),나머지=2$$따라서,\n$$17 \\mod 5=2$$동치 관계 $$A \\equiv B \\pmod{M}$$이 식은 A와 B가 같은 나머지를 가지는 경우를 의미한다.\n$$17 \\equiv 2 \\pmod{5}$$이는 17과 2는 5로 나눴을 때 같은 나머지를 가진다는 뜻이다.\n주요 성질 덧셈 두 수의 합을 구한 뒤 모듈러 연산을 수행하는 것과, 각각의 수에 대해 먼저 모듈러 연산을 수행한 후 더하는 것은 결과가 같다.\n$$(A+B) \\mod M = [(A \\mod M)+(B \\mod M)] \\mod M$$뺄셈 두 수의 차이를 구한 뒤 모듈러 연산을 수행하는 것과, 각각 모듈러 연산 후 뺀 값을 모듈러 연산하는 것은 동일하다. 다만, 결과가 음수일 경우 $M$을 더해 양수로 변환한다.\n$$(A−B) \\mod M = [(A \\mod M)−(B \\mod M) + M] \\mod M$$곱셈 두 수의 곱을 직접 모듈러 연산하는 것과, 각각의 수에 대해 먼저 모듈러 연산을 수행한 후 곱한 값을 다시 모듈러 연산하는 것은 동일하다.\n$$(A×B) \\mod M = [(A \\mod M) × (B \\mod M)] \\mod M$$거듭제곱 거듭제곱 후 모듈러 연산을 수행하는 것과, 밑수를 먼저 모듈러 연산한 후 거듭제곱하여 모듈러 연산하는 것은 동일하다.\n빠르게 계산하는 방법: 모듈러 거듭제곱\n$$A^B \\mod M=[(A \\mod M)^B] \\mod M$$나눗셈 모듈러 연산에서 나눗셈은 일반적인 나눗셈이 아니라, $B$의 모듈러 역원(곱셈 역원)을 찾아 곱셈으로 변환하여 계산해야 한다.\n역원 개념 필요, 보통 확장된 유클리드 알고리즘 사용\n$$(A/B) \\mod M=(A×B^{−1}) \\mod M$$Pseudo Code 각 연산에 대해 Pseudo code를 작성해 보자.\n기본 모듈러 연산 시간 복잡도: O(1)\n1 2 FUNCTION modular(A, M): RETURN A - (A // M) * M # (A를 M으로 나눈 나머지) 입력: $A,M$ 출력: $A \\mod M$ 모듈러 덧셈 시간 복잡도: O(1)\n1 2 FUNCTION modular_add(A, B, M): RETURN (A % M + B % M) % M 모듈러 뺄셈 시간 복잡도: O(1)\n1 2 3 4 5 FUNCTION modular_subtract(A, B, M): result = (A % M - B % M) % M IF result \u0026lt; 0: result += M # 음수가 되지 않도록 보정 RETURN result 모듈러 곱셈 시간 복잡도: O(1)\n1 2 FUNCTION modular_multiply(A, B, M): RETURN (A % M * B % M) % M 모듈러 거듭제곱 (빠른 거듭제곱) 거듭제곱을 직접 계산하면 O(B) 이므로, 빠르게 계산하는 방법(O(log B))을 사용해야 한다.\n시간 복잡도: O(log B) (빠른 거듭제곱)\n1 2 3 4 5 6 7 8 9 FUNCTION modular_exponentiation(A, B, M): result = 1 base = A % M WHILE B \u0026gt; 0: IF B % 2 == 1: # B가 홀수라면 result = (result * base) % M base = (base * base) % M B = B // 2 RETURN result 모듈러 나눗셈 (모듈러 역원) 시간 복잡도: O(log M)\n$A/B \\mod M$ 를 계산하려면, $B$의 모듈러 역원 $B^{-1}$을 찾아야 한다.\n페르마의 소정리, M이 소수일 때 $$ B^{−1} \\equiv B^{M−2} \\pmod{M} $$ 1 2 FUNCTION modular_inverse(B, M): RETURN modular_exponentiation(B, M-2, M) # B^(M-2) % M 계산 M이 소수일 때만 사용 가능 🎯결론 모듈러 연산은 시간복잡도 O(1)로 계산할 수 있어 효율적이다. 거듭제곱은 O(log B)로 최적화 가능하다. 나눗셈은 모듈러 역원을 활용해야 한다. 암호학, 해시 함수, 수학적 최적화 등에 널리 사용된다. ⚙️EndNote Markdown 수학식 표현 Markdown에서 수학식을 표현하는 방법은 LaTeX 수식(TeX 수식) 또는 MathJax라고 한다.\nLaTeX(레이텍) 수식: 수학 기호와 공식을 작성하는 데 사용되는 문법 MathJax(매스잭스): 웹에서 LaTeX 스타일의 수식을 렌더링하는 라이브러리 인라인 수식 예시 1 2 # 인라인 수식 모듈러 수학식 $A \\mod M=R$ 결과 인라인 수식: $A \\mod M=R$\n블록 수식 예시 1 2 # 블록 수식 `$$ ... $$` 형태로 작성한다. $$ A \\mod M=R $$ 결과 $$ A \\mod M=R $$","date":"2025-03-09T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-03-09-modular-arithmetic/","title":"모듈러 연산"},{"content":"📌개요 고급 정렬 알고리즘을 이해하고, 더 복잡한 정렬 방식과 응용을 살펴본다.\n📌내용 Counting Sort, Radix Sort, Tim Sort 같은 고급 정렬 알고리즘은 특정한 상황에서 매우 효율적이며, 실무에서도 활용되는 경우가 많다. 계수 정렬(Counting Sort) 시간 복잡도: O(n + k) (k는 최대값)\n데이터의 크기를 기반으로 개수를 세어 정렬하는 알고리즘으로, 숫자의 범위가 제한적일 때 매우 효율적이다.\nPseudo Code 1 2 3 4 5 6 7 8 9 10 11 CountingSort(A, k): C = array of size k+1 initialized to 0 B = array of size length(A) for i from 0 to length(A) - 1: C[A[i]] += 1 for i from 1 to k: C[i] += C[i - 1] for i from length(A) - 1 down to 0: B[C[A[i]] - 1] = A[i] C[A[i]] -= 1 return B JavaScript Code 1 2 3 4 5 6 7 8 9 10 11 12 function countingSort(A, k) { let C = new Array(k + 1).fill(0); let B = new Array(A.length); for (let i = 0; i \u0026lt; A.length; i++) C[A[i]]++; for (let i = 1; i \u0026lt;= k; i++) C[i] += C[i - 1]; for (let i = A.length - 1; i \u0026gt;= 0; i--) { B[C[A[i]] - 1] = A[i]; C[A[i]]--; } return B; } 기수 정렬(Radix Sort) 시간 복잡도: O(nk) (k는 자릿수)\n자릿수를 기준으로 정렬을 반복하여 전체 배열을 정렬하는 알고리즘이다.\nPseudo Code 1 2 3 4 RadixSort(A, d): for i from 0 to d - 1: A = StableCountingSort(A, i) return A JavaScript Code 1 2 3 4 5 6 7 8 9 10 11 function radixSort(A) { let maxNum = Math.max(...A).toString().length; let divisor = 1; for (let i = 0; i \u0026lt; maxNum; i++) { let buckets = [...Array(10)].map(() =\u0026gt; []); for (let num of A) buckets[Math.floor(num / divisor) % 10].push(num); A = [].concat(...buckets); divisor *= 10; } return A; } 팀 정렬(Tim Sort) 시간 복잡도: 최악 O(n log n), 평균 O(n log n)\n합병 정렬과 삽입 정렬을 조합하여 최적의 성능을 보장하는 정렬 알고리즘이다.\nPseudo Code 1 2 3 4 TimSort(A): for each run in A: sort run using InsertionSort merge sorted runs using MergeSort JavaScript Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 function timSort(A) { const RUN = 32; function insertionSort(A, left, right) { for (let i = left + 1; i \u0026lt;= right; i++) { let temp = A[i], j = i - 1; while (j \u0026gt;= left \u0026amp;\u0026amp; A[j] \u0026gt; temp) { A[j + 1] = A[j]; j--; } A[j + 1] = temp; } } for (let i = 0; i \u0026lt; A.length; i += RUN) { insertionSort(A, i, Math.min(i + RUN - 1, A.length - 1)); } let size = RUN; while (size \u0026lt; A.length) { for (let left = 0; left \u0026lt; A.length; left += 2 * size) { let mid = left + size - 1; let right = Math.min(left + 2 * size - 1, A.length - 1); merge(A, left, mid, right); } size *= 2; } return A; } ⚙️EndNote Pseudo Code Pseudo Code는 실제 프로그래밍 언어의 구문과 유사하지만, 특정 프로그래밍 언어의 문법에 얽매이지 않고 알고리즘의 논리적인 흐름을 설명하는 데 사용되는 간단한 서술 형태다.\n언어 독립성: 특정 프로그래밍 언어의 문법을 따르지 않으며, 누구나 쉽게 읽고 이해할 수 있다. 간결함: 복잡한 문법 요소를 생략하고 핵심 알고리즘 로직만을 표현한다. 가독성: 사람이 읽기 쉽게 작성되어, 알고리즘의 흐름과 동작을 쉽게 파악할 수 있다. ","date":"2025-02-22T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-02-22-algorithms-and-data-structure-sorting-advanced/","title":"알고리즘과 자료구조: 고급 정렬"},{"content":"📌개요 나만의 리눅스 터미널을 설정한다.\n📌내용 사전 준비 WSL, Windows Terminal 설치 등은 제외하고 Linux 환경에서 정리 시작 설치에 필요한 wget, curl, git을 설치한다.\n1 2 sudo apt-get update sudo apt install wget curl git zsh를 설치한다.\n1 sudo apt install zsh 현재 유저의 기본 쉘을 변경한다.\n1 chsh -s $(which zsh) 설정을 확인한다.\n1 2 echo $SHELL # /usr/bin/zsh oh-my-zsh 설치 oh-my-zsh 설치\nInstall oh-my-zsh via curl 1 sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; Install oh-my-zsh via wget 1 sh -c \u0026#34;$(wget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\u0026#34; 설정 폰트는 fonts-powerline​등 agnoster 테마를 정상적으로 사용할 수 있는 폰트로 사용한다.\n테마 설정 1 2 3 4 5 6 7 # vim 에디터로 .zshrc 열기 vim ~/.zshrc # .zshrc 중 ZSH_THEME 부분 ZSH_THEME=\u0026#34;agnoster\u0026#34; # 수정 후 저장 # 이후 적용 source ~/.zshrc 플러그인 사용 설정 아래 설치한 플러그인을 사용하기 위해선 .zshrc 파일에 설정해야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # vim 에디터로 .zshrc 열기 vim ~/.zshrc # .zshrc 중 plugins 부분 plugins=( ... zsh-syntax-highlighting zsh-autosuggestions fzf ... ) # 수정 후 저장 # 이후 적용 source ~/.zshrc zsh-syntax-highlighting 명령어 문법에 따른 강조 표시하는 플러그인\n1 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting zsh-autosuggestions 명령어에 대한 자동완성을 돕는 플러그인 이전 사용했던 명령어 또는 사용할 수 있는 명령어 등을 제안해준다.\n1 git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions fzf (Fuzzy Finder ) 파일 찾기, 명령어 히스토리 등 터미널을 강력하게 사용할 수 있다.\nfzf 자세히 보기\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 git clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf ~/.fzf/install​ # Do you want to enable fuzzy auto-completion? ([y]/n) y # Do you want to enable key bindings? ([y]/n) y # Do you want to update your shell configuration files? ([y]/n) y # Update /home/nine/.bashrc: # - [ -f ~/.fzf.bash ] \u0026amp;\u0026amp; source ~/.fzf.bash # + Added # # Update /home/nine/.zshrc: # - [ -f ~/.fzf.zsh ] \u0026amp;\u0026amp; source ~/.fzf.zsh # + Added # # Finished. Restart your shell or reload config file. # source ~/.bashrc # bash # source ~/.zshrc # zsh # # Use uninstall script to remove fzf. # # For more information, see: https://github.com/junegunn/fzf 커스텀 설정 사용자명 + 랜덤 이모지 설정 1 2 3 4 5 6 7 8 9 10 11 12 # vim 에디터로 .zshrc 열기 vim ~/.zshrc # .zshrc 최하단에 추가 prompt_context() { # Custom (Random emoji) emojis=(\u0026#34;🦋\u0026#34; \u0026#34;🌈\u0026#34;) RAND_EMOJI_N=$(( $RANDOM % ${#emojis[@]} + 1)) prompt_segment black default \u0026#34;Name ${emojis[$RAND_EMOJI_N]} \u0026#34; } # 수정 후 저장 # 이후 적용 source ~/.zshrc 새로운 줄에서 명령어 시작 경로가 길거나 명령어가 길거나 뭐, 가끔 테마가 깨지는 경우를 대비해 깔끔하게 새로운 라인에서 명령어를 입력할 수 있게 변경 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 테마의 환경 파일 열기 vim ~/.oh-my-zsh/themes/agnoster.zsh-theme # build_prompt 부분을 찾아서 prompt_hg와 prompt_end 사이에 prompt_newline을 추가 build_prompt() { RETVAL=$? prompt_status prompt_virtualenv prompt_aws prompt_context prompt_dir prompt_git prompt_bzr prompt_hg prompt_newline # 이 위치에 추가한다. prompt_end } # 아래 내용을 추가한다. prompt_newline() { if [[ -n $CURRENT_BG ]]; then echo -n \u0026#34;%{%k%F{$CURRENT_BG}%}$SEGMENT_SEPARATOR %{%k%F{blue}%}$SEGMENT_SEPARATOR\u0026#34; else echo -n \u0026#34;%{%k%}\u0026#34; fi echo -n \u0026#34;%{%f%}\u0026#34; CURRENT_BG=\u0026#39;\u0026#39; } # 수정 후 저장 # 이후 적용 source ~/.zshrc Tmux 터미널 세션을 만들 수 있고 터미널 분할이 가능하다. 세션을 바꿔가며 터미널을 확인할 수 있다.\ntmux 명령으로 실행할 때마다 새로운 세션이 생성된다.\n1 2 3 4 # tmux 설치 sudo apt-get install tmux # 이후 tmux 실행 tmux 이후 tmux의 설정(.tmux.conf)은 세션을 재시작하거나 명령어로 적용할 수 있다.\n1 2 3 # prefix + b, : # 명령 모드에서 source-file ~/.tmux.conf Tmux Command 명령어 설명 tmux 새 tmux 세션 시작 tmux ls 또는 tmux list-sessions 실행 중인 tmux 세션 목록 보기 tmux attach-session -t {세션명} 특정 세션에 접근 (세션명에 따라) tmux attach 또는 tmux a 마지막으로 사용한 tmux 세션에 접근 tmux new -s {세션명} 새로운 tmux 세션을 특정 이름으로 시작 tmux kill-session -t {세션명} 특정 세션 종료 (세션명에 따라) tmux kill-server 모든 tmux 세션 종료 (서버 종료) Ctrl + b, d 현재 tmux 세션에서 detach (세션 분리) Ctrl + b, s 세션 목록 보기 (세션 선택 후 attach 가능) Ctrl + b, $ 현재 세션의 이름 변경 (rename session) Tmux Shortcuts Meta 키 M - 일반적으로 Alt 키로 매핑된다. 즉, M-1은 Alt + 1 과 동일하다. DC는 delete 키를 의미한다. 단축키 설명 C-b C-b Prefix 키 자체를 입력 (즉, C-b를 두 번 입력) Prefix C-o 창을 순환 이동 (Rotate through panes) Prefix C-z 현재 클라이언트 일시 정지 (Suspend the current client) Prefix Space 다음 레이아웃 선택 (Select next layout) Prefix ! 현재 패널을 새로운 창으로 이동 (Break pane to a new window) Prefix \u0026quot; 창을 수직으로 분할 (Split window vertically) Prefix # 모든 붙여넣기 버퍼 목록 표시 (List all paste buffers) Prefix $ 현재 세션 이름 변경 (Rename current session) Prefix % 창을 수평으로 분할 (Split window horizontally) Prefix \u0026amp; 현재 창 종료 (Kill current window) Prefix ' 창 번호 입력 후 이동 (Prompt for window index to select) Prefix ( 이전 클라이언트로 전환 (Switch to previous client) Prefix ) 다음 클라이언트로 전환 (Switch to next client) Prefix , 현재 창 이름 변경 (Rename current window) Prefix - 가장 최근 붙여넣기 버퍼 삭제 (Delete the most recent paste buffer) Prefix . 현재 창 이동 (Move the current window) Prefix / 키 바인딩 설명 표시 (Describe key binding) Prefix 0~9 특정 번호의 창 선택 (Select window 0~9) Prefix : 명령어 입력 프롬프트 (Prompt for a command) Prefix ; 이전 활성 패널로 이동 (Move to the previously active pane) Prefix = 붙여넣기 버퍼 선택 (Choose a paste buffer from a list) Prefix ? 키 바인딩 목록 표시 (List key bindings) Prefix C 옵션 사용자 지정 (Customize options) Prefix D 클라이언트 분리 선택 (Choose and detach a client from a list) Prefix E 패널 크기를 균등하게 조정 (Spread panes out evenly) Prefix L 마지막 클라이언트로 전환 (Switch to the last client) Prefix M 선택된 패널 마크 해제 (Clear the marked pane) Prefix [ 복사 모드 진입 (Enter copy mode) -\u0026gt; spacebar 선택, enter 복사 (키가 동작하지 않는 경우 vi 키맵핑 필요할 수 있음) Prefix ] 최근 붙여넣기 버퍼 붙여넣기 (Paste the most recent paste buffer) Prefix c 새 창 생성 (Create a new window) Prefix d 현재 클라이언트 분리 (Detach the current client) Prefix f 패널 검색 (Search for a pane) Prefix i 창 정보 표시 (Display window information) Prefix l 이전 창으로 이동 (Select the previously current window) Prefix m 현재 패널 마크/마크 해제 (Toggle the marked pane) Prefix n 다음 창 선택 (Select the next window) Prefix o 다음 패널 선택 (Select the next pane) Prefix p 이전 창 선택 (Select the previous window) Prefix q 패널 번호 표시 (Display pane numbers) Prefix r 현재 클라이언트 다시 그리기 (Redraw the current client) Prefix s 세션 목록에서 선택 (Choose a session from a list) Prefix t 시계 표시 (Show a clock) Prefix w 창 목록에서 선택 (Choose a window from a list) Prefix x 활성 패널 종료 (Kill the active pane) Prefix z 패널 확대/축소 (Zoom the active pane) Prefix { 현재 패널을 위쪽 패널과 교환 (Swap the active pane with the pane above) Prefix } 현재 패널을 아래쪽 패널과 교환 (Swap the active pane with the pane below) Prefix ~ 메시지 기록 보기 (Show messages) Prefix DC (DC = delete) 창의 보이는 부분을 커서가 따라가도록 리셋 (Reset visible part of the window follows the cursor) Prefix PPage 복사 모드에서 위로 스크롤 (Enter copy mode and scroll up) Prefix ↑ / ↓ / ← / → 패널 간 이동 (Select pane up/down/left/right) Prefix M-1 ~ M-5 (M = Alt) 레이아웃 설정 (Set even-horizontal, even-vertical, main-horizontal, main-vertical, tiled layout) Prefix M-n / M-p 경고가 있는 창으로 이동 (Select next/previous window with an alert) Prefix M-o 패널을 반대 방향으로 순환 (Rotate through the panes in reverse) Prefix M-↑ / M-↓ / M-← / M-→ 패널 크기 조정 (Resize pane up/down/left/right by 5) Prefix C-↑ / C-↓ / C-← / C-→ 패널 크기 조정 (Resize pane up/down/left/right) Prefix S-↑ / S-↓ / S-← / S-→ 창의 보이는 부분을 이동 (Move the visible part of the window up/down/left/right) Tmux Plugin Manager Tmux Plugin Manager (TPM) 설치\n1 2 3 4 5 6 7 8 9 10 11 12 13 git clone https://github.com/tmux-plugins/tpm ~/.tmux/plugins/tpm # 사용자의 홈 디렉토리에 `.tmux.conf` 파일을 열거나 생성한다. vim ~/.tmux.conf # 아래 내용을 입력 후 저장 # TPM 설정 set -g @plugin \u0026#39;tmux-plugins/tpm\u0026#39; # tmux에서 마우스 스크롤 활성화 set -g mouse on # TPM을 초기화하는 설정 run \u0026#39;~/.tmux/plugins/tpm/tpm\u0026#39; TPM이 설정되었으면 tmux에서 다음 명령을 입력하여 플러그인을 설치할 수 있다.\n1 2 # tmux에서 새로운 세션을 시작하고 (또는 기존 세션에서) prefix + I Tmux Resurrect Tmux Resurrect 자세히 보기\n시스템 리부팅 시 작업하던 Tmux 세션이 날아가는데, 세션을 저장하고 불러올 수 있게 해준다.\n플러그인 설정 및 설치 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 사용자의 홈 디렉토리에 `.tmux.conf` 파일을 생성하거나 수정 vim ~/.tmux.conf # 아래 내용을 run \u0026#39;~/.tmux/plugins/tpm/tpm\u0026#39; 명령 위에 추가 후 저장 # tmux-resurrect 플러그인 추가 set -g @plugin \u0026#39;tmux-plugins/tmux-resurrect\u0026#39; # 만약 TPM 설치도 정상, .tmux.conf 설정도 정상일 때 결과 # [0/0]Installing \u0026#34;tmux-resurrect\u0026#34; # \u0026#34;tmux-resurrect\u0026#34; download success # # TMUX environment reloaded. # # Done, press ESCAPE to continue. Key bindings prefix + Ctrl-s - save prefix + Ctrl-r - restore vi 키 맵핑 tmux는 기본적으로 vi 키로 동작하지만 정상적으로 키 입력이 안 되는 경우가 발생할 수 있다. 그럴 땐 직접 vi 키 맵핑으로 동작할 수 있게 설정에 추가하고 적용한다.\n문서 하단에 추가하고 설정 적용 커맨드 실행한다.\n1 2 3 4 5 6 # 사용자의 홈 디렉토리에 `.tmux.conf` 파일을 생성하거나 수정 vim ~/.tmux.conf # vi 키 바인딩 # 복사모드 Termux에서 실행 중인 tmux 세션에서 스페이스바와 Enter 키를 사용하여 해당 줄을 복사 set-window-option -g mode-keys vi 📌선택사항 Windows Terminal 설치 MicroSoft Store에서 Windows Terminal을 설치한다. 터미널의 설정에서 기본 프로필, 폰트, 컬러 등을 설정한다.\nWSL 설치 명령어로 설치 가능하지만, 더 자세한 자료 확인이 필요하다면 WSL을 사용하여 Windows에 Linux를 설치하는 방법\n1 wsl --install WSL 삭제 1.설치 상태 확인 WSL에서 설치된 Linux 배포판 목록과 해당 배포판의 상태를 확인한다.\n1 2 3 4 wsl -l -v # NAME STATE VERSION # * Ubuntu Stopped 2 2.등록 해제 NAME으로 등록 해제한다.\n1 2 3 4 wsl --unregister Ubuntu # 등록 취소 중입니다. # 작업을 완료했습니다. 3.Ubuntu 제거 Windows의 시작 메뉴에서 ubuntu를 찾아 제거한다.\n4.선택 사항 필요하다면 아래 절차도 진행한다. 버전이 바뀌면서 다를 수 있으니 케이스에 맞지 않다면 넘어간다.\n앱 \u0026gt; 설치된 앱에서 Windows Subsystem for Linux, Linux용 Windows 하위 시스템 삭제 Win + R 눌러 실행을 열고, optionalfeatures를 입력해서 Windows 기능 창을 연다. Linux용 Windows 하위 시스템을 체크 해제하고 재부팅 WSL의 파일은 기본적으로 C:\\Users\\{사용자 이름}\\AppData\\Local\\Packages 폴더에 저장된다. 이 폴더에서 해당 배포판 관련 폴더를 찾아 삭제할 수 있다. Powerlevel10k powerlevel10k repository GitHub-powerlevel10k 심볼 표현을 위한 meslo font\n저장소를 클론 받는다. 폰트도 미리 설치하면 편하다.\n1 git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k .zshrc를 열어서 테마를 변경한다.\n1 2 3 4 5 vim ~/.zshrc # ZSH_THEME 찾아서 변경 ZSH_THEME=\u0026#34;powerlevel10k/powerlevel10k\u0026#34; # 변경된 .zshrc 적용 source ~/.zshrc 변경된 .zshrc 적용 후 아래와 같이 출력 되면 성공\nUTF-8 텍스트 확인 ASCII 문자로 충분했지만 대부분의 오픈 소스들은 거의 최소 UTF-8를 쓰고 있기 때문에 사용하는 폰트가 해당 텍스트를 지원하고 있는지 확인하는 것 PUA(Private Use Area)에 해당하는 글자가 보이는지 확인 PUA는 이름 그대로 사용자 정의 영역이고 UTF-8, UTF-16에서 사용자 개인이 직접 특수 문자를 정의해서 쓸 수 있는 구간.(U+E000 ~ U+F8FF, U+F0000 ~ U+10FFFF) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 This is Powerlevel10k configuration wizard. You are seeing it because you haven\u0026#39;t defined any Powerlevel10k configuration options. It will ask you a few questions and configure your prompt. Does this look like a diamond (rotated square)? reference: https://graphemica.com/%E2%97%86 ---\u0026gt;  \u0026lt;--- (y) Yes. (n) No. (q) Quit and do nothing. Choice [ynq]: 이후 여러가지 설정을 묻는데 취향에 맞게 설정 후 recommended 옵션으로 마무리한다. 기존에 적용했던 랜덤 임티와 이름 표현도 수정이 필요하다.\n테마 재설정 1 p10k configure ~/.p10k.zsh 랜덤 이모지 적용 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # The list of segments shown on the left. Fill it with the most important segments. typeset -g POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=( # =========================[ Line #1 ]========================= # os_icon # os identifier custom_emoji # custom emoji dir # current directory vcs # git status # =========================[ Line #2 ]========================= newline # \\n # prompt_char # prompt symbol ) # custom emoji env POWERLEVEL9K_CUSTOM_EMOJI=\u0026#34;my_random_emoji\u0026#34; POWERLEVEL9K_CUSTOM_EMOJI_BACKGROUND=0 # 배경 # custom emoji function function my_random_emoji() { local emojis=(\u0026#34;🤌\u0026#34;) echo \u0026#34;${emojis[RANDOM % ${#emojis[@]} + 1]}\u0026#34; } 경로 축약 표현 취향에 맞게 적절히 조정 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # If directory is too long, shorten some of its segments to the shortest possible unique # prefix. The shortened directory can be tab-completed to the original. # typeset -g POWERLEVEL9K_SHORTEN_STRATEGY=truncate_to_unique typeset -g POWERLEVEL9K_SHORTEN_STRATEGY=truncate # Replace removed segment suffixes with this symbol. # typeset -g POWERLEVEL9K_SHORTEN_DELIMITER= typeset -g POWERLEVEL9K_SHORTEN_DELIMITER=... # If set to \u0026#34;first\u0026#34; (\u0026#34;last\u0026#34;), remove everything before the first (last) subdirectory that contains # files matching $POWERLEVEL9K_SHORTEN_FOLDER_MARKER. For example, when the current directory is # /foo/bar/git_repo/nested_git_repo/baz, prompt will display git_repo/nested_git_repo/baz (first) # or nested_git_repo/baz (last). This assumes that git_repo and nested_git_repo contain markers # and other directories don\u0026#39;t. # # Optionally, \u0026#34;first\u0026#34; and \u0026#34;last\u0026#34; can be followed by \u0026#34;:\u0026lt;offset\u0026gt;\u0026#34; where \u0026lt;offset\u0026gt; is an integer. # This moves the truncation point to the right (positive offset) or to the left (negative offset) # relative to the marker. Plain \u0026#34;first\u0026#34; and \u0026#34;last\u0026#34; are equivalent to \u0026#34;first:0\u0026#34; and \u0026#34;last:0\u0026#34; # respectively. typeset -g POWERLEVEL9K_DIR_TRUNCATE_BEFORE_MARKER=false # Don\u0026#39;t shorten this many last directory segments. They are anchors. typeset -g POWERLEVEL9K_SHORTEN_DIR_LENGTH=1 # Shorten directory if it\u0026#39;s longer than this even if there is space for it. The value can # be either absolute (e.g., \u0026#39;80\u0026#39;) or a percentage of terminal width (e.g, \u0026#39;50%\u0026#39;). If empty, # directory will be shortened only when prompt doesn\u0026#39;t fit or when other parameters demand it # (see POWERLEVEL9K_DIR_MIN_COMMAND_COLUMNS and POWERLEVEL9K_DIR_MIN_COMMAND_COLUMNS_PCT below). # If set to `0`, directory will always be shortened to its minimum length. typeset -g POWERLEVEL9K_DIR_MAX_LENGTH=80 # When `dir` segment is on the last prompt line, try to shorten it enough to leave at least this # many columns for typing commands. typeset -g POWERLEVEL9K_DIR_MIN_COMMAND_COLUMNS=40 # When `dir` segment is on the last prompt line, try to shorten it enough to leave at least # COLUMNS * POWERLEVEL9K_DIR_MIN_COMMAND_COLUMNS_PCT * 0.01 columns for typing commands. typeset -g POWERLEVEL9K_DIR_MIN_COMMAND_COLUMNS_PCT=50 # If set to true, embed a hyperlink into the directory. Useful for quickly # opening a directory in the file manager simply by clicking the link. # Can also be handy when the directory is shortened, as it allows you to see # the full directory that was used in previous commands. typeset -g POWERLEVEL9K_DIR_HYPERLINK=false ⚙️EndNote WSL Windows Sub-system for Linux, 윈도우의 하위 시스템으로 리눅스 사용\nInfo 개발자는 Windows 컴퓨터에서 동시에 Windows와 Linux의 기능에 액세스할 수 있습니다. WSL(Linux용 Windows 하위 시스템)을 사용하면 개발자가 Linux 배포판(예: Ubuntu, OpenSUSE, Kali, Debian, Arch Linux)을 설치하고 기존 가상 머신 또는 이중 부팅 설정의 오버헤드 없이 Windows에서 직접 Linux 애플리케이션, 유틸리티 및 Bash 명령줄 도구를 사용할 수 있습니다.\nGit 자격증명 관련 WSL을 사용할 때 git 명령어 사용 시 username, password를 요구하는 경우가 있다. 온전한 linux가 아니라서 별도 라이브러리를 사용해야 할 수도 있다. 일단, 간단한 방법으로는 windows의 자격증명을 사용하게끔 설정하는 방법이다. git이 설치되어 있어야 한다.\n1 git config --global credential.helper \u0026#34;/mnt/c/Program\\ Files/Git/mingw64/bin/git-credential-manager.exe\u0026#34; git status WSL을 사용할 때 git status 확인 시 다른 프로필의 터미널로 확인할 때와 다른 경우가 있다.\nCRLF(Line Ending) 차이 Git이 파일 권한(Executable Bit)을 다르게 인식 WSL에서는 파일의 실행 권한(chmod +x)이 적용되지만, Windows에서는 실행 권한 개념이 없기 때문에 파일이 변경된 것처럼 인식될 수 있다. 저장소를 클론 받은 주체에 따라 접근 권한에 대한 오류가 발생할 수도 있다. 따라서 모든 케이스를 해결할 필요는 없고 적절한 조치를 취하면 된다. 해결 가능한 요약 정보만 정리한다.\n1 2 3 4 5 6 7 8 # 줄바꿈 문제 git config --global core.autocrlf false # 파일 권한 변경 감지 방지 git config --global core.fileMode false # 대소문자 문제 방지 git config --global core.ignoreCase false # 심볼릭 링크 문제 해결 git config --global core.symlinks true ","date":"2025-02-15T10:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-02-15-set-up-my-own-linux-terminal/","title":"나만의 리눅스 터미널 설정"},{"content":"📌개요 더 효율적인 정렬 알고리즘을 이해하기 위해 중급 정렬 알고리즘을 살펴본다.\n📌내용 Heap Sort, Merge Sort, Quick Sort 같은 중급 정렬 알고리즘은 실무에서도 많이 사용되며, 고급 정렬 알고리즘을 이해하는 데 중요한 개념을 제공한다. 병합 정렬(Merge Sort) 시간 복잡도: O(n log n)\n분할 정복(Divide and Conquer) 방식을 사용하여 배열을 반씩 나누고 병합하며 정렬하는 알고리즘이다.\n특징 안정적인 정렬 (Stable Sort) → 동일한 값의 상대적 순서 유지 외부 정렬(External Sort)에 적합하다. 디스크나 네트워크에서 데이터를 읽으며 정렬 가능하다. 데이터가 이미 정렬된 경우에도 성능을 유지한다. 연결 리스트(Linked List) 정렬에 유리하다. 연속된 메모리 공간을 필요로 하지 않는다. 실무 사용 예시 안정성이 중요한 경우에 사용한다. 데이터베이스에서 인덱스 정렬 등 대용량 데이터 정렬. 외부 정렬 알고리즘에서 많이 사용한다. 연결 리스트 정렬 배열보다 메모리 재배열이 어려운 경우 Pseudo Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Merge(left, right): result = empty array i = 0, j = 0 while i \u0026lt; length(left) and j \u0026lt; length(right): if left[i] \u0026lt; right[j]: append left[i] to result increment i else: append right[j] to result increment j append remaining elements of left to result (if any) append remaining elements of right to result (if any) return result MergeSort(A, left, right): if left \u0026lt; right: mid = (left + right) / 2 MergeSort(A, left, mid) MergeSort(A, mid + 1, right) Merge(A, left, mid, right) JavaScript Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 function merge(left, right) { let result = []; let i = 0, j = 0; while (i \u0026lt; left.length \u0026amp;\u0026amp; j \u0026lt; right.length) { if (left[i] \u0026lt; right[j]) { result.push(left[i]); i++; } else { result.push(right[j]); j++; } } return result.concat(left.slice(i), right.slice(j)); } function mergeSort(A) { if (A.length \u0026lt; 2) return A; let mid = Math.floor(A.length / 2); let left = mergeSort(A.slice(0, mid)); let right = mergeSort(A.slice(mid)); return merge(left, right); } 퀵 정렬(Quick Sort) 시간 복잡도: 평균 O(n log n), 최악 O(n^2)\n피벗(Pivot)을 기준으로 작은 값과 큰 값으로 나누어 정렬하는 알고리즘이다.\nQuickSort 함수는 배열을 재귀적으로 나누어 정렬한다. Partition 함수는 pivot을 기준으로 배열을 두 두분으로 분할하는 과정이다. pivot보다 작은 값들은 왼쪽, 큰 값들은 오른쪽으로 이동시킨 후 새로운 pivot을 반환한다. 특징 비교 기반 정렬 중 가장 빠른 평균 속도 추가적인 메모리가 거의 필요 없다. 데이터가 랜덤하게 분포되어 있을 때 성능이 뛰어나다 병합 정렬보다 캐시 친화적이다. 실무 사용 예시 대용량 데이터 정렬 일반적인 데이터 정렬 (데이터가 랜덤 분포일 경우) 정렬된 배열을 받는 경우 최악의 성능을 자랑한다. 최악의 케이스를 모두 피할 순 없지만, pivot을 작은 값 또는 큰 값이 아닌 중간 값 또는 랜덤 선택 등으로 지정하는 방법을 사용해볼 수 있다. Pseudo Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Partition(A, low, high): pivot = A[high] i = low - 1 for j = low to high - 1: if A[j] \u0026lt; pivot: i += 1 swap(A[i], A[j]) swap(A[i + 1], A[high]) return i + 1 QuickSort(A, low, high): if low \u0026lt; high: pivotIndex = Partition(A, low, high) QuickSort(A, low, pivotIndex - 1) QuickSort(A, pivotIndex + 1, high) JavaScript Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 function partition(A, low, high) { let pivot = A[high]; let i = low - 1; for (let j = low; j \u0026lt; high; j++) { if (A[j] \u0026lt; pivot) { i++; [A[i], A[j]] = [A[j], A[i]]; } } [A[i + 1], A[high]] = [A[high], A[i + 1]]; return i + 1; } function quickSort(A) { if (A.length \u0026lt;= 1) return A; let pivotIndex = partition(A, 0, A.length - 1); return [ ...quickSort(A.slice(0, pivotIndex)), A[pivotIndex], ...quickSort(A.slice(pivotIndex + 1)), ]; } 힙 정렬(Heap Sort) 시간 복잡도: O(n log n)\n힙(Heap) 자료구조를 활용하여 최댓값 또는 최솟값을 빠르게 찾는 방식으로 정렬하는 알고리즘이다.\n특징 최악의 경우에도 동일한 시간 복잡도를 보장한다. 추가적인 메모리 사용이 거의 없다. 우선순위 큐에서 응용 가능하다. 실무 사용 예시 우선순위 큐 기반의 정렬 최소/최대 힙을 이용한 정렬 실시간 데이터 스트림 정렬 항상 정렬된 상태를 유지해야 하는 경우 메모리 사용을 최소화해야 하는 정렬 Pseudo Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 HEAP_SORT_ASCENDING(A): BUILD_MAX_HEAP(A) FOR i FROM length(A) - 1 DOWNTO 1: SWAP(A[0], A[i]) MAX_HEAPIFY(A, 0, i) RETURN A BUILD_MAX_HEAP(A): FOR i FROM FLOOR(length(A) / 2) - 1 DOWNTO 0: MAX_HEAPIFY(A, i, length(A)) MAX_HEAPIFY(A, i, heapSize): largest ← i left ← 2 * i + 1 right ← 2 * i + 2 IF left \u0026lt; heapSize AND A[left] \u0026gt; A[largest]: largest ← left IF right \u0026lt; heapSize AND A[right] \u0026gt; A[largest]: largest ← right IF largest ≠ i: SWAP(A[i], A[largest]) MAX_HEAPIFY(A, largest, heapSize) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 HEAP_SORT_DESCENDING(A): BUILD_MIN_HEAP(A) FOR i FROM length(A) - 1 DOWNTO 1: SWAP(A[0], A[i]) MIN_HEAPIFY(A, 0, i) RETURN A BUILD_MIN_HEAP(A): FOR i FROM FLOOR(length(A) / 2) - 1 DOWNTO 0: MIN_HEAPIFY(A, i, length(A)) MIN_HEAPIFY(A, i, heapSize): smallest ← i left ← 2 * i + 1 right ← 2 * i + 2 IF left \u0026lt; heapSize AND A[left] \u0026lt; A[smallest]: smallest ← left IF right \u0026lt; heapSize AND A[right] \u0026lt; A[smallest]: smallest ← right IF smallest ≠ i: SWAP(A[i], A[smallest]) MIN_HEAPIFY(A, smallest, heapSize) JavaScript Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 function heapSortAscending(A) { let n = A.length; buildMaxHeap(A); for (let i = n - 1; i \u0026gt; 0; i--) { [A[0], A[i]] = [A[i], A[0]]; maxHeapify(A, 0, i); } return A; } function heapSortDescending(A) { let n = A.length; buildMinHeap(A); for (let i = n - 1; i \u0026gt; 0; i--) { [A[0], A[i]] = [A[i], A[0]]; minHeapify(A, 0, i); } return A; } function buildMaxHeap(A) { let heapSize = A.length; for (let i = Math.floor(heapSize / 2) - 1; i \u0026gt;= 0; i--) { maxHeapify(A, i, heapSize); } } function buildMinHeap(A) { let heapSize = A.length; for (let i = Math.floor(heapSize / 2) - 1; i \u0026gt;= 0; i--) { minHeapify(A, i, heapSize); } } function maxHeapify(A, i, heapSize) { let largest = i; let left = 2 * i + 1; let right = 2 * i + 2; if (left \u0026lt; heapSize \u0026amp;\u0026amp; A[left] \u0026gt; A[largest]) { largest = left; } if (right \u0026lt; heapSize \u0026amp;\u0026amp; A[right] \u0026gt; A[largest]) { largest = right; } if (largest !== i) { [A[i], A[largest]] = [A[largest], A[i]]; maxHeapify(A, largest, heapSize); } } function minHeapify(A, i, heapSize) { let smallest = i; let left = 2 * i + 1; let right = 2 * i + 2; if (left \u0026lt; heapSize \u0026amp;\u0026amp; A[left] \u0026lt; A[smallest]) { smallest = left; } if (right \u0026lt; heapSize \u0026amp;\u0026amp; A[right] \u0026lt; A[smallest]) { smallest = right; } if (smallest !== i) { [A[i], A[smallest]] = [A[smallest], A[i]]; minHeapify(A, smallest, heapSize); } } 📌적절한 사용 전략 빠른 정렬이 필요하면? → 퀵 정렬 (일반적인 경우) 안정적인 정렬이 필요하면? → 병합 정렬 (데이터 정렬 순서를 유지해야 할 때) 메모리를 절약하면서 안정적인 성능을 원하면? → 힙 정렬 정렬 알고리즘 평균 시간 복잡도 최악 시간 복잡도 공간 복잡도 안정성 특징 및 사용 사례 퀵 정렬 (Quick Sort) O(nlogn) O(n2) O(logn) ❌ 가장 빠른 평균 속도, 제자리 정렬, 랜덤 데이터 정렬 병합 정렬 (Merge Sort) O(nlogn) O(nlogn) O(n) ✅ 안정적 정렬, 외부 정렬 및 연결 리스트 정렬 힙 정렬 (Heap Sort) O(nlogn) O(nlogn) O(1) ❌ 우선순위 큐 기반 정렬, 메모리 절약 필요 시 ⚙️EndNote Pseudo Code Pseudo Code는 실제 프로그래밍 언어의 구문과 유사하지만, 특정 프로그래밍 언어의 문법에 얽매이지 않고 알고리즘의 논리적인 흐름을 설명하는 데 사용되는 간단한 서술 형태다.\n언어 독립성: 특정 프로그래밍 언어의 문법을 따르지 않으며, 누구나 쉽게 읽고 이해할 수 있다. 간결함: 복잡한 문법 요소를 생략하고 핵심 알고리즘 로직만을 표현한다. 가독성: 사람이 읽기 쉽게 작성되어, 알고리즘의 흐름과 동작을 쉽게 파악할 수 있다. ","date":"2025-02-10T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-02-10-algorithms-and-data-structure-sorting-intermediate/","title":"알고리즘과 자료구조: 중급 정렬"},{"content":"📌개요 알고리즘과 자료구조의 기본 원리를 이해하기 위한 정렬의 기초를 알아본다.\n📌내용 Bubble Sort, Selection Sort, Insertion Sort 같은 기초적인 정렬 알고리즘은 실제로 실무에서 거의 쓰이지 않지만, 더 효율적인 정렬 알고리즘을 이해하기 위한 기초 개념으로 중요한 역할을 한다. 버블 정렬(Bubble Sort) 시간 복잡도: O(n^2)\n인접한 요소들을 비교하여 정렬하는 알고리즘으로, 반복문을 통해 정렬이 완료될 때까지 계속 비교 및 교환한다.\nPseudo Code 구현에 앞서 Pseudo Code를 작성해본다.\n1 2 3 4 5 BubbleSort(A): for i from 0 to length(A) - 1: for j from 0 to length(A) - i - 1: if A[j] \u0026gt; A[j + 1]: swap(A[j], A[j + 1]) BubbleSort(A) 정렬할 배열 A를 인자로 받는 함수. for i from 0 to length(A) - 1 바깥쪽 반복문은 배열 전체를 순회한다. i는 0부터 배열의 길이-1까지 증가한다. 배열의 마지막 요소는 이미 정렬되었을 가능성이 높기 때문에, 매번 마지막 요소는 비교하지 않는다. for j from 0 to length(A) - i - 1 안쪽 반복문은 배열의 첫 번째 요소부터 배열의 마지막에서 i번째 요소까지 순회한다. i가 증가할수록 비교해야 할 범위가 줄어든다. if A[j] \u0026gt; A[j + 1] 현재 요소 A[j]가 다음 요소 A[j + 1]보다 큰지 확인한다. swap(A[j], A[j + 1]) 만약 현재 요소가 다음 요소보다 크다면, 두 요소의 위치를 교환(swap)한다. 이 과정을 통해 큰 값이 점점 배열의 끝으로 이동하게 된다. JavaScript Code 정렬할 배열: [5, 3, 8, 4, 2]\n1회차: [3, 5, 4, 2, 8] 2회차: [3, 4, 2, 5, 8] 3회차: [3, 2, 4, 5, 8] 4회차: [2, 3, 4, 5, 8] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 function bubbleSort(A) { let n = A.length; for (let i = 0; i \u0026lt; n - 1; i++) { for (let j = 0; j \u0026lt; n - i - 1; j++) { if (A[j] \u0026gt; A[j + 1]) { // A[j]과 A[j + 1]의 위치를 변경한다. let temp = A[j]; A[j] = A[j + 1]; A[j + 1] = temp; } } } return A; } 선택 정렬(Selection Sort) 시간 복잡도: O(n^2)\n매번 최솟값을 찾아서 정렬되지 않은 부분의 첫 번째 요소와 교환하는 알고리즘이다.\nPseudo Code 구현에 앞서 Pseudo Code를 작성해본다.\n1 2 3 4 5 6 7 8 SelectionSort(A): for i from 0 to length(A) - 1: minIndex = i for j from i + 1 to length(A): if A[j] \u0026lt; A[minIndex]: minIndex = j if minIndex != i: swap(A[i], A[minIndex]) SelectionSort(A) 정렬할 배열 A를 인자로 받는 함수. for i from 0 to length(A) - 1 바깥쪽 반복문은 배열 전체를 순회한다. i는 0부터 배열의 길이-1까지 증가한다. minIndex = i 현재 반복의 시작 위치를 최솟값 인덱스로 설정한다. for j from i + 1 to length(A) 안쪽 반복문은 현재 요소의 다음 요소부터 배열의 끝까지 순회한다. if A[j] \u0026lt; A[minIndex] 현재 요소 A[j]가 현재까지의 최솟값 A[minIndex]보다 작은지 확인한다. minIndex = j 만약 현재 요소가 최솟값보다 작다면, 최솟값 인덱스를 현재 요소의 인덱스로 업데이트한다. if minIndex != i 최솟값 인덱스가 현재 반복의 시작 인덱스와 다르다면, 두 요소의 위치를 교환(swap)한다. JavaScript Code 정렬할 배열: [5, 3, 8, 4, 2]\n1회차: [2, 3, 8, 4, 5] 2회차: [2, 3, 8, 4, 5] 3회차: [2, 3, 4, 8, 5] 4회차: [2, 3, 4, 5, 8] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 function selectionSort(A) { let n = A.length; for (let i = 0; i \u0026lt; n - 1; i++) { let minIndex = i; for (let j = i + 1; j \u0026lt; n; j++) { if (A[j] \u0026lt; A[minIndex]) { minIndex = j; } } if (minIndex != i) { // A[i]과 A[minIndex]의 위치를 변경한다. let temp = A[i]; A[i] = A[minIndex]; A[minIndex] = temp; } } return A; } 삽입 정렬(Insertion Sort) 시간 복잡도: O(n^2)\n정렬된 부분과 정렬되지 않은 부분을 나누고, 정렬되지 않은 부분의 요소를 적절한 위치에 삽입하여 정렬한다.\nPseudo Code 구현에 앞서 Pseudo Code를 작성해본다.\n1 2 3 4 5 6 7 8 InsertionSort(A): for i from 1 to length(A) - 1: key = A[i] j = i - 1 while j \u0026gt;= 0 and A[j] \u0026gt; key: A[j + 1] = A[j] j = j - 1 A[j + 1] = key InsertionSort(A) 정렬할 배열 A를 인자로 받는 함수. for i from 1 to length(A) - 1 바깥쪽 반복문은 배열의 두 번째 요소부터 마지막 요소까지 순회한다. key = A[i] 현재 요소를 key 변수에 저장한다. j = i - 1 현재 요소의 이전 요소 인덱스를 j에 저장한다. while j \u0026gt;= 0 and A[j] \u0026gt; key 현재 요소의 이전 요소들이 key보다 큰 동안 반복한다. A[j + 1] = A[j] 현재 요소를 한 칸 뒤로 이동시킨다. j = j - 1 인덱스를 한 칸 앞으로 이동시킨다. A[j + 1] = key key를 올바른 위치에 삽입한다. JavaScript Code 정렬할 배열: [5, 3, 8, 4, 2]\n1회차: [3, 5, 8, 4, 2] 2회차: [3, 5, 8, 4, 2] 3회차: [3, 4, 5, 8, 2] 4회차: [2, 3, 4, 5, 8] 1 2 3 4 5 6 7 8 9 10 11 12 13 function insertionSort(A) { let n = A.length; for (let i = 1; i \u0026lt; n; i++) { let key = A[i]; let j = i - 1; while (j \u0026gt;= 0 \u0026amp;\u0026amp; A[j] \u0026gt; key) { A[j + 1] = A[j]; j = j - 1; } A[j + 1] = key; } return A; } ⚙️EndNote Pseudo Code Pseudo Code는 실제 프로그래밍 언어의 구문과 유사하지만, 특정 프로그래밍 언어의 문법에 얽매이지 않고 알고리즘의 논리적인 흐름을 설명하는 데 사용되는 간단한 서술 형태다.\n언어 독립성: 특정 프로그래밍 언어의 문법을 따르지 않으며, 누구나 쉽게 읽고 이해할 수 있다. 간결함: 복잡한 문법 요소를 생략하고 핵심 알고리즘 로직만을 표현한다. 가독성: 사람이 읽기 쉽게 작성되어, 알고리즘의 흐름과 동작을 쉽게 파악할 수 있다. ","date":"2025-02-09T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-02-09-algorithms-and-data-structure-sorting-basic/","title":"알고리즘과 자료구조: 기초 정렬"},{"content":"📌개요 알고리즘과 자료구조의 기본 원리를 이해하고 간단한 문제를 해결할 수 있다.\n📌자료구조 배열과 문자열 배열(Array) 동일한 타입의 요소들이 연속적으로 배치된 자료구조다. 인덱스를 통해 접근할 수 있으며, 고정된 크기를 갖는다.\n장점: 인덱스를 통해 빠르게 접근 가능 (O(1)) 단점: 크기가 고정되어 있으며, 요소의 삽입 및 삭제가 비효율적일 수 있음 (O(n)) 문자열(String) 문자의 배열로, 문자열의 길이에 따라 크기가 동적으로 변할 수 있다.\n장점: 문자열 연산(비교, 연결 등)이 간편 단점: 문자열의 길이에 따라 연산 시간이 증가 연결 리스트(Linked List) 각 요소가 노드로 구성되며, 각 노드는 데이터와 다음 노드를 가리키는 포인터를 포함한다. 크기가 동적으로 변한다.\n단일 연결 리스트(Singly Linked List): 각 노드가 다음 노드를 가리킨다. 이중 연결 리스트(Doubly Linked List): 각 노드가 이전 노드와 다음 노드를 가리킨다. 장점: 크기가 동적으로 변하며, 요소의 삽입 및 삭제가 용이 (O(1)) 단점: 인덱스를 통한 접근이 비효율적 (O(n)) 스택(Stack)과 큐(Queue) 스택(Stack) LIFO(Last In, First Out) 구조로, 마지막에 삽입된 요소가 가장 먼저 제거된다.\n주요 연산: 삽입(push), 삭제(pop), 조회(peek) 장점: 구현이 간단하고, 함수 호출 스택 등에서 유용 단점: 특정 요소 접근이 비효율적 (O(n)) 큐(Queue) FIFO(First In, First Out) 구조로, 처음에 삽입된 요소가 가장 먼저 제거된다.\n주요 연산: 삽입(enqueue), 삭제(dequeue), 조회(front) 장점: 구현이 간단하고, 작업 대기열 등에서 유용 단점: 특정 요소 접근이 비효율적 (O(n)) 해시 테이블(Hash Table) 키-값 쌍을 저장하는 자료구조로, 해시 함수를 사용하여 키를 인덱스로 변환하여 값을 저장한다.\n장점: 평균적으로 빠른 접근, 삽입, 삭제 가능 (O(1)) 단점: 해시 충돌 가능성, 해시 함수의 성능에 따라 성능 차이 발생 📌알고리즘 빅오 표기법(Big-O Notation) bigocheatsheet\n알고리즘의 시간 복잡도와 공간 복잡도를 나타내는 표기법으로, 입력 크기 n에 대한 연산 횟수나 메모리 사용량을 표현한다.\nO(1): 상수 시간, 입력 크기에 상관없이 일정한 시간 소요 O(n): 선형 시간, 입력 크기에 비례하여 시간 소요 O(log n): 로그 시간, 입력 크기의 로그에 비례하여 시간 소요 O(n^2): 이차 시간, 입력 크기의 제곱에 비례하여 시간 소요 재귀(Recursion) 기본 함수가 자기 자신을 호출하는 기법으로, 문제를 작은 하위 문제로 분할하여 해결한다.\n장점: 코드가 간결해지고, 특정 문제(예: 트리 탐색)에서 유용 단점: 호출 스택의 크기가 커질 수 있으며, 무한 재귀를 방지하기 위해 종료 조건 필요 정렬 알고리즘 버블 정렬(Bubble Sort) 시간 복잡도: O(n^2)\n인접한 요소들을 비교하여 정렬하는 알고리즘으로, 반복문을 통해 정렬이 완료될 때까지 계속 비교 및 교환한다.\n선택 정렬(Selection Sort) 시간 복잡도: O(n^2)\n매번 최솟값을 찾아서 정렬되지 않은 부분의 첫 번째 요소와 교환하는 알고리즘이다.\n삽입 정렬(Insertion Sort) 시간 복잡도: O(n^2)\n정렬된 부분과 정렬되지 않은 부분을 나누고, 정렬되지 않은 부분의 요소를 적절한 위치에 삽입하여 정렬한다.\n","date":"2025-01-31T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-01-31-algorithms-and-data-structure-understanding-basic-concepts/","title":"알고리즘과 자료구조: 기본 개념 이해"},{"content":"📌개요 알고리즘과 자료구조를 단계별로 학습할 수 있는 로드맵\n📌1. 기본 개념 이해 자료구조와 알고리즘의 기본 원리를 이해하고 간단한 문제를 해결할 수 있다.\n자료구조 배열과 문자열 연결 리스트(Linked List) 스택(Stack)과 큐(Queue) 해시 테이블(Hash Table) 알고리즘 빅오 표기법(Big-O Notation) 이해 재귀(Recursion) 기본 정렬 알고리즘: 버블 정렬, 선택 정렬, 삽입 정렬 📌2. 기초 응용 기초 자료구조와 알고리즘을 활용해 조금 더 복잡한 문제를 해결한다.\n자료구조 트리(Tree)와 이진 탐색 트리(BST) 그래프(Graph) 기본 개념 (DFS, BFS 탐색) 우선순위 큐(Priority Queue)와 힙(Heap) 알고리즘 분할 정복(Divide and Conquer) 탐욕 알고리즘(Greedy Algorithm) 이분 탐색(Binary Search) 슬라이딩 윈도우(Sliding Window) 투 포인터(Two Pointer) 📌3. 중급 알고리즘 및 문제 해결 다양한 알고리즘 패턴을 학습하고 효율적인 문제 해결 능력을 기른다.\n자료구조 트라이(Trie) 세그먼트 트리(Segment Tree) 유니온 파인드(Union-Find) 그래프의 심화 (최단 경로 알고리즘: 다익스트라, 플로이드 워셜) 알고리즘 동적 프로그래밍(Dynamic Programming, DP) 백트래킹(Backtracking) 분할 정복의 고급 활용 최적화 문제 해결 📌4. 고급 알고리즘 및 최적화 알고리즘 대회를 준비하거나 실제 프로젝트에서 효율적인 알고리즘을 설계할 수 있다.\n자료구조 펜윅 트리(Fenwick Tree) 고급 그래프 알고리즘 (네트워크 플로우, 최소 스패닝 트리) 알고리즘 고급 동적 프로그래밍 (비트마스크 활용) 문자열 알고리즘 (KMP, 라빈 카프, 접미사 배열) 수학적 알고리즘 (소수 판정, 모듈러 연산, 유클리드 호제법) 비트 연산 활용 📌5. 실전 및 응용 코딩 테스트 준비 및 실제 프로젝트에서 알고리즘 적용\n실전 대비 시간 복잡도와 공간 복잡도 최적화 연습 제한된 시간 내에 문제를 해결하는 연습 응용 실무 프로젝트에서 효율적인 데이터 처리 오픈소스 기여를 통해 알고리즘 설계 경험 쌓기 ⚙️EndNote 매일 꾸준히 문제를 풀고 복습한다. 어려운 문제는 토론하거나 검색을 통해 풀이 과정을 이해한다. 처음부터 모든 걸 완벽히 이해하려 하지 말고, 반복 학습으로 익히는 게 좋다. ","date":"2025-01-28T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-01-28-algorithms-and-data-structure-roadmap/","title":"알고리즘과 자료구조: 학습 로드맵"},{"content":"📌개요 Docker란 무엇이고 왜 사용하는지 정리한다.\n📌Docker란? Docker는 애플리케이션을 컨테이너라는 독립된 환경에서 실행할 수 있게 해주는 플랫폼이다. Docker를 이해하기 위해 필요한 기본 개념들과 지식에 대해 알아본다.\n📌왜 사용하지? 1. 일관된 환경 제공 Docker는 컨테이너를 통해 애플리케이션과 그 종속성들을 포함하는 독립된 환경을 제공한다. 이를 통해 개발, 테스트, 배포 환경에서 일관된 실행 환경을 보장할 수 있다. 이는 \u0026ldquo;내 컴에선 잘 되네요. (Works on my machine)\u0026rdquo; 문제를 해결하는 데 도움이 된다.\n2. 효율적인 리소스 사용 Docker 컨테이너는 가상 머신보다 가볍고 빠르다. 컨테이너는 호스트 운영 체제의 커널을 공유하면서 실행되기 때문에, 리소스 오버헤드가 적고 빠른 성능을 제공한다.\n3. 빠른 배포와 스케일링 Docker를 사용하면 애플리케이션의 배포와 스케일링이 용이하다. 이미지를 통해 컨테이너를 빠르게 생성하고 실행할 수 있으며, 필요한 경우 컨테이너를 쉽게 확장할 수 있다.\n4. 이식성 Docker 이미지는 어디서나 일관된 환경을 제공하기 때문에, 애플리케이션을 다양한 플랫폼에서 쉽게 실행할 수 있다. 이는 로컬 개발 환경, 테스트 서버, 클라우드 환경 등에서 동일하게 동작한다.\n5. 버전 관리와 롤백 Docker 이미지는 버전 관리가 가능하며, 특정 버전으로 롤백할 수 있다. 이는 애플리케이션의 안정성과 일관성을 유지하는 데 도움이 된다.\n6. 분리된 환경 제공 Docker는 각 애플리케이션을 분리된 컨테이너에서 실행하기 때문에, 서로 간섭 없이 독립적으로 실행할 수 있다. 이는 보안성과 안정성을 높이는 데 기여한다.\n7. DevOps와 CI/CD 통합 Docker는 DevOps와 CI/CD 파이프라인과 쉽게 통합할 수 있다. 이를 통해 자동화된 빌드, 테스트, 배포 프로세스를 구현할 수 있다.\n📌Docker를 이해하기 위한 기본 지식 Docker를 이해하고 사용하기 위해 다음과 같은 기본 지식이 필요하다.\n리눅스 명령어와 기초 운영 체제 지식: Docker는 주로 리눅스 환경에서 사용되므로 리눅스 명령어와 파일 시스템 구조에 대한 기본적인 이해가 필요하다. 가상화 개념: Docker는 가상화 기술의 한 형태이므로, 전통적인 가상 머신과 비교하여 컨테이너의 장점과 차이점을 이해하는 것이 도움이 된다. 네트워킹 기초: 컨테이너 간의 통신과 네트워킹 설정을 이해하기 위해 기본적인 네트워킹 개념이 필요하다. 버전 관리 시스템: Dockerfile과 이미지를 버전 관리하는 데 도움이 되므로 Git과 같은 버전 관리 시스템에 대한 기본적인 이해가 필요하다. 📌Docker의 핵심 개념 1. Containers (컨테이너) 컨테이너는 애플리케이션과 그 종속성들을 포함하는 경량의 독립 실행 환경이다.\n컨테이너는 동일한 호스트 운영 체제 커널을 공유하면서도 서로 격리되어 실행된다. 이를 통해 애플리케이션이 어디서나 일관되게 실행될 수 있다.\n2. Images (이미지) 이미지는 Docker 컨테이너를 생성하기 위한 읽기 전용 템플릿이다.\n이미지는 여러 개의 레이어로 구성되며, 각 레이어는 기존 이미지에서 변경된 내용을 포함한다. 이미지는 재사용 가능하며, Docker Hub와 같은 레지스트리에 저장하고 배포할 수 있다.\n3. Volumes (볼륨) 볼륨은 컨테이너와 호스트 간의 데이터를 지속적으로 저장하고 공유하기 위한 메커니즘이다.\n컨테이너가 삭제되더라도 볼륨에 저장된 데이터는 유지된다. 볼륨은 컨테이너 간에 데이터를 공유할 때도 유용하하다.\n4. Builds (빌드) 빌드는 Dockerfile을 사용하여 이미지를 생성하는 과정이다.\nDockerfile은 애플리케이션 환경을 설정하는 명령어들의 스크립트다. docker build 명령어를 사용하여 Dockerfile을 기반으로 이미지를 생성할 수 있다.\n","date":"2025-01-27T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-01-27-getting-started-with-docker/","title":"Docker 시작하기: 기초 개념과 필수 지식 정리"},{"content":"📌개요 폼 제출을 필요로하는 부모 페이지에서 useForm을 생하고 각 값을 자식 컴포넌트와 연결한다. 특정 조건에 따라 동적으로 자식 컴포넌트의 disabled, readonly 등을 반영하되 리렌더링을 최소화한다. 📌내용 진행 중인 React + typescript 프로젝트에서 form 전략을 고민했고, useForm을 생성해서 FormProvider로 감싼 내부에 입력 컴포넌트들을 배치한다.\n직접 참조와 함수 내부에서 할당하여 반환하는 방식의 차이 useWatch를 직접 참조하는 것과 참조한 값을 반환하는 것은 다르게 동작한다. 직접 참조 useForm에서 해당 필드의 값이 변경되면 리렌더링이 발생한다. useWatch를 직접 참조하는 컴포넌트는 값이 변경될 때마다 리렌더링되며, 자식 컴포넌트들도 모두 리렌더링된다.\n이는 useState와 useEffect를 사용하여 값이 변경될 때마다 상태가 업데이트되고, 컴포넌트가 리렌더링되기 때문이다.\n1 const test = useWatch({name: someName, control: someControl}); 함수 내부에서 할당하여 반환 test 함수는 useWatch를 직접 참조하지 않고, 내부의 nameValue가 useWatch를 통해 최신 값을 가지게 된다. 따라서 test 함수는 리렌더링을 발생 시키지 않고 useForm의 특정 필드에 대한 최신 값을 반환하는 함수가 된다.\nuseWatch 훅을 함수 내부에서 사용하여 값을 반환하는 경우, 해당 함수는 리액트 컴포넌트의 리렌더링을 트리거하지 않는다. 이는 함수가 호출될 때마다 최신 값을 반환하지만, 해당 함수가 리렌더링을 직접적으로 유발하지 않기 때문이다.\n1 2 3 4 5 6 7 8 9 const test = () =\u0026gt; { const nameValue = useWatch({name: someName, control: someControl}); return nameValue; }; function test() { return useWatch({name: someName, control: someControl}); }; watch VS useWatch 직접 참조를 기준으로 테스트한 결과를 정리해본다.\nuseForm.watch useForm 훅에서 제공하는 watch 메서드를 사용하면 값이 변경될 때 useForm의 값을 사용하는 컴포넌트들을 모두 리렌더링한다.\nuseWatch useWatch 훅은 useForm의 특정 값을 참조하는 컴포넌트만 리렌더링한다.\n🎯결론 일단, 각 방식의 장단점을 고려하여 적절히 사용하면 된다.\nuseForm의 값을 사용하는 폼이 몇 개 안 될 땐 useForm.watch를 사용해서 간편하게 리렌더링을 발생 시키며 UI를 업데이트할 수 있다. 폼이 꽤 많아지거나 리렌더링으로 인해 데이터 핸들링이 복잡한 경우 useWatch를 사용해 내부적으로 변경된 최신 값을 적용하며 불필요한 리렌더링을 방지할 수 있다. useWatch를 직접 참조하는 방식은 간결하고 이해하기 쉽지만, 리렌더링이 발생할 수 있다. 함수 내부에서 할당하여 반환하는 방식은 리렌더링을 최소화할 수 있지만, 사용 방식이 다소 복잡할 수 있다. React 훅은 기본적으로 상태나 컨텍스트의 변화를 감지하고 리렌더링을 트리거한다. 훅을 함수 내부에서 사용하고 값을 반환하는 방식은 참조 불변성을 이용한 접근법이라고 볼 수 있고특정 상황에 유리할 수 있다.\nuseWatch뿐만 아니라 다른 React 훅들도 직접 참조하지 않고 함수 내부에서 사용한다면 리렌더링을 트리거하지 않을 수도 있다.\n결국, 커스텀 훅을 정의하는 것과 같은 행위라고 볼 수 있다.\n⚙️EndNote useForm useForm은 react-hook-form 라이브러리에서 제공하는 훅으로 폼 상태 및 유효성 검사를 관리하는 데 사용된다. useForm 훅을 사용하면 폼을 쉽게 설정하고 관리할 수 있다. 이를 통해 폼의 각 입력 필드를 등록하고, 폼 상태를 추적하며, 유효성 검사를 수행할 수 있다. 또한, 폼 제출 시 처리할 함수를 정의할 수 있다.\n장점 간편한 설정: useForm은 사용하기 매우 간단하며, 폼 상태 관리와 유효성 검사를 쉽게 구현할 수 있다. 성능 최적화: react-hook-form은 리렌더링을 최소화하도록 설계되어 있어 성능이 뛰어나다. 유연성: 다양한 유효성 검사 규칙을 쉽게 적용할 수 있으며, 커스텀 유효성 검사도 지원한다. 타입스크립트 지원: 타입스크립트를 완벽히 지원하여 타입 안전성을 보장한다. 작은 번들 크기: 라이브러리의 번들 크기가 작아 애플리케이션의 전체 크기에 거의 영향을 주지 않는다. 단점 러닝 커브: 초기 설정과 사용법을 익히는 데 약간의 러닝 커브가 있을 수 있다. 특히 초보자에게는 복잡하게 느껴질 수 있다. 제한된 내장 구성 요소: 다른 폼 라이브러리와 달리 react-hook-form은 내장된 스타일링이나 UI 구성 요소를 제공하지 않으므로, 직접 스타일링을 해야 한다. 외부 라이브러리와의 통합: 일부 외부 컴포넌트 라이브러리와의 통합이 까다로울 수 있다. 이 경우, 별도의 어댑터나 커스텀 훅을 작성해야 할 수 있다. useWatch useWatch는 react-hook-form 라이브러리에서 제공하는 훅으로 useForm의 특정 필드를 실시간으로 관찰(watch)한다. 관련 필드의 값이 변경될 때마다 업데이트된 값을 반환하는 훅이다. 이를 통해 사용자는 특정 필드의 값이 변경될 때마다 적절한 동작을 수행할 수 있다.\n장점 실시간 값 추적: useWatch를 사용하면 특정 필드의 값을 실시간으로 추적할 수 있다. 사용자가 입력한 값에 따라 즉각적으로 UI를 업데이트할 수 있다. 간편한 사용: useWatch는 간단한 API를 제공한다. 복잡한 상태 관리나 값 추적 로직을 간단하게 구현할 수 있다. 성능 최적화: 필요한 필드만 관찰할 수 있다. 불필요한 렌더링을 줄이고 성능을 최적화할 수 있다. 동적 폼 구성: 특정 필드의 값에 따라 다른 필드나 UI 요소를 동적으로 변경하거나 조건부 렌더링을 쉽게 구현할 수 있다. 단점 복잡한 폼에서는 코드 관리가 어려울 수 있음: 여러 필드를 관찰해야 하는 복잡한 폼의 경우, useWatch를 사용한 코드가 복잡해질 수 있다. 이 경우 코드의 가독성과 유지보수성이 떨어질 수 있다. 의존성 관리 필요: useWatch 훅을 사용할 때, 관찰할 필드가 변경되면 의존성을 적절히 관리해야 한다. 그렇지 않으면 예상치 못한 동작이 발생할 수 있다. 초기 값 설정: 폼 필드의 초기 값을 설정할 때 주의가 필요하다. useWatch는 폼 필드의 초기 값을 제대로 반영하기 위해 초기 값을 명확히 설정해야 한다. 제한된 문서화: useWatch는 비교적 새로운 훅이기 때문에 관련 문서나 예제가 부족할 수 있다. (작성 시점) 따라서 복잡한 사용 사례에 대한 정보를 찾기가 어려울 수 있다. 코드 react-hook-form useWatch에서 useEffect, useState를 어떻게 사용하고 있는지 간단하게 확인해본다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import { useEffect, useState } from \u0026#39;react\u0026#39;; import { useFormContext } from \u0026#39;./useFormContext\u0026#39;; function useWatch({ control, name }) { const methods = useFormContext(); const actualControl = control || methods.control; const [value, setValue] = useState(actualControl.getValues(name)); useEffect(() =\u0026gt; { const subscription = actualControl.watch((values) =\u0026gt; { setValue(values[name]); }); return () =\u0026gt; subscription.unsubscribe(); }, [name, actualControl]); return value; } ","date":"2025-01-25T21:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-01-25-optimizing-rerendering-with-usewatch/","title":"useWatch 사용한 리렌더링 최적화"},{"content":"📌개요 Java의 public static void main(String[] args)\n📌내용 Java 프로젝트를 생성하고 IDE에서 제공하는 메인 템플릿을 생성하게 되면 아래와 같은 코드를 확인할 수 있다.\n백준 같은 알고리즘 풀이 사이트에선 채점 시 제출한 코드를 Main.java로 가정하여 받는다.\n1 2 3 4 5 public class SampleProject { public static void main(String[] args) { ... } } Java 는 애플리케이션이 실행되면 제일 먼저 메인(main)메소드를 실행한다. Java에서 public static void main(String[] args) 메서드의 형태가 항상 동일한 이유는 Java 언어 명세에 명시되어 있기 때문이다. 이는 Java 프로그램의 시작 지점을 정의하기 위한 약속(규약, convention)이다.\n📌왜 main 메서드의 형태가 고정되어 있을까? JVM(Java Virtual Machine)의 요구 사항 Java 프로그램은 JVM에서 실행되며, JVM은 프로그램의 시작 지점을 찾기 위해 특정 형태의 메서드를 탐색한다. 이때 main 메서드는 반드시 아래와 같은 조건을 만족해야 한다.\npublic: JVM이 프로그램 외부에서 호출할 수 있어야 함. static: JVM이 클래스의 인스턴스를 생성하지 않고도 호출할 수 있어야 함. void: 반환값이 필요하지 않음. 프로그램의 시작점으로 동작하기 때문. String[] args: 명령줄 인수를 받을 수 있어야 함. Java 언어 명세 (Java Language Specification, JLS) JLS의 12.1.4 \u0026ldquo;Invoke the main Method\u0026rdquo; 섹션에 이 규칙이 명시되어 있다. JVM은 실행 시 지정된 클래스에서 다음과 같은 메서드를 찾아 호출한다.\n1 public static void main(String[] args) 일관성 및 예측 가능성 프로그램의 진입점을 표준화함으로써 개발자들이 Java 프로그램의 구조를 쉽게 이해할 수 있다. 다른 사람이 작성한 코드를 보더라도 어디에서 실행이 시작되는지 즉시 알 수 있다.\n📌변경하면 안 될까? 접근 제한자 변경: private 또는 protected로 바꾸면 JVM이 메서드를 찾지 못하고 NoSuchMethodError가 발생한다. static 제거: JVM은 클래스 인스턴스를 생성하지 않으므로 static이 없으면 호출할 수 없다. 매개변수 변경: String[] args 대신 다른 매개변수를 사용하면 JVM이 인식하지 못한다. 📌예외 사항 Overloading main 메서드는 오버로딩할 수 있다.\n그러나 JVM은 여전히 public static void main(String[] args)를 실행한다. 1 2 3 4 5 6 7 8 public class SampleProject { public static void main(String[] args) { System.out.println(\u0026#34;Hello, World!\u0026#34;); } public static void main() { System.out.println(\u0026#34;This won\u0026#39;t be called by JVM!\u0026#34;); } } Entry Point 변경 Java 11 이상에서는 public static void main(String[] args) 없이도 java.util.function 인터페이스를 활용하여 프로그램을 실행할 수 있다. 예를 들어, java -cp . MyProgram 형태로 실행 가능하다.\n📌결론 public static void main(String[] args)는 Java의 규약이며, Java 언어 명세에 명시된 내용이다. 이 형태를 따르지 않으면 JVM이 프로그램의 진입점을 찾을 수 없으므로 반드시 이 구조를 따라야 한다.\n⚙️EndNote main 메서드의 구성 요소 public (공개 접근 제어자) JVM이 프로그램을 실행할 때 main 메서드를 호출해야 하므로, 외부에서 접근 가능하도록 public이어야 한다. static (정적 메서드) Java에서 메서드를 호출하려면 일반적으로 객체를 생성해야 하지만, main은 프로그램 시작점이므로 객체 없이 호출 가능해야 한다. 따라서 static으로 선언하여 JVM이 클래스 로딩 후 바로 실행할 수 있도록 한다. void (반환 값 없음) main 메서드는 실행이 목적이므로 별도의 반환 값이 필요하지 않다. 필요한 경우 상황에 맞게 변경하여 사용한다. 프로그램 종료 상태는 System.exit(status)를 통해 반환할 수 있다. String[] args (명령행 인자) 프로그램 실행 시 전달되는 명령행 인자(Arguments)를 배열로 받는다. 예를 들어, java Main hello world 명령어를 실행하면 args[0] = \u0026quot;hello\u0026quot;, args[1] = \u0026quot;world\u0026quot;가 된다. ","date":"2025-01-05T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2025-01-05-main-class-of-java/","title":"Java의 main 클래스"},{"content":"📌개요 깃 서버를 구축해서 같은 네트워크 망의 다른 PC에서 프로젝트 클론해보자.\n📌Gitea란? Gitea는 Git 저장소를 관리하기 위한 오픈 소스 분산 버전 관리 시스템이다. GitHub와 유사한 기능을 제공하며, 자체 호스팅이 가능하여 사용자가 자신의 서버에 설치하고 운영할 수 있다. Gitea는 경량화된 애플리케이션으로, 개인 프로젝트나 소규모 팀에서 사용하기에 적합하다. 주요 기능으로는 코드 리뷰, 이슈 트래킹, 지속적 통합(CI) 등을 제공하며 그 외에도 유용한 기능을 많이 제공한다.\n📌설치 및 구동 아래 깃헙 저장소에 접속하여 윈도우 설치 파일을 다운로드 받는다.\nhttps://github.com/go-gitea/gitea/releases gitea-1.21.2-gogit-windows-4.0-amd64.exe 필요한 버전으로 받는다. 다운로드 받은 파일을 관리자 권한으로 실행하면 아래와 같은 명령창이 확인된다. 기본 포트 3000으로 접속할 수 있다.\n접속하면 Github UI와 비슷한 화면에서 형상 관리가 가능하다.\n데이터베이스 설정 Gitea requires MySQL, PostgreSQL, MSSQL, SQLite3 or TiDB (MySQL protocol). 사용할 DB를 연결할 수도 있고, 기본적으로 별도의 DB와 연결하지 않아도 자체 SQLite와 함께 사용할 수 있다. ID: giteaadmin, PW: giteaadmin, e-mail: jgjo@kors.co.kr 각종 설정 관리자 생성, 사용자 권한 등 설정 These configuration options will be written into: D:\\gitea_config\\custom\\conf\\app.ini Uninstall 삭제는 bat, sh 등 삭제 관련 소스를 찾을 수 있다. 일단 gitea 설치 시 설정했던 경로들의 gitea 관련 폴더를 삭제하면 다시 설치할 수 있다. 또한 gitea-1.21.2-gogit-windows-4.0-amd64.exe를 실행한 위치에 data, custom 폴더가 생성된 걸 확인할 수 있다. 📌보안 설정 무엇보다 앞서 DNS가 필요하다. 외부 도메인 없이 내부 IP만으로 사용할 수가 없다. 유효하지 않은 인증서라고\u0026hellip; gitea 문서: https://docs.gitea.com/next/administration/https-setup https 보안 설정을 위해 필요한 인증 파일을 발급해야 한다. cert.pem key.pem 1 2 3 4 5 6 [server] ... ROOT_URL = https://000.000.000.000:3000/ #http \u0026gt;\u0026gt;\u0026gt; https 수정 PROTOCOL = https #추가 CERT_FILE = custom/https/cert.pem #추가, 경로는 자유 KEY_FILE = custom/https/key.pem #추가, 경로는 자유 OpenSSL 설치 다운로드: OpenSSL 공식 웹사이트에서 Windows용 설치 파일을 다운로드한다. https://slproweb.com/products/Win32OpenSSL.html\n설치: 다운로드한 설치 파일을 실행하여 OpenSSL을 설치한다.\nOpenSSL 명령어 사용 명령 프롬프트 열기: OpenSSL을 설치한 후, 명령 프롬프트(Windows의 cmd)를 연다.\n버전 확인: OpenSSL이 정상적으로 설치되었는지 버전을 확인한다.\n1 openssl version RSA 키 생성: 개인 키를 생성한다.\n1 openssl genrsa -out key.pem 2048 이 명령은 2048 비트 길이의 RSA 개인 키를 생성하고 key.pem 파일에 저장한다.\n자체 서명된 인증서 생성: 개인 키를 사용하여 자체 서명된 인증서를 생성한다.\n1 openssl req -new -x509 -key key.pem -out cert.pem -days 3650 이 명령은 10년(3650일) 동안 유효한 자체 서명된 인증서를 생성하고 cert.pem 파일에 저장한다. 입력하라는 대로 입력해도 멈추는 경우가 있는데 입력 후에 .을 입력해서 넘기거나 구글링하거나 빈 칸으로 넘어가보자.\n인증서 정보 입력: 명령을 실행하면 추가 정보를 입력해야 할 수 있다. 실제 운영에서는 이 정보가 중요하며 신뢰할 수 있는 인증서를 위해 정확하고 유효한 정보를 제공해야 한다.\n파일 확인: 생성된 key.pem과 cert.pem 파일을 확인하여 사용한다.\n이제 생성된 key.pem과 cert.pem 파일을 필요한 곳에서 사용할 수 있다.\n자체 서명된 인증서를 사용하는 경우, 클라이언트에서는 브라우저에 예외로 등록해야만 경고 없이 접속할 수 있다. 그러나 실제 제품에서는 신뢰할 수 있는 인증 기관으로부터 유효한 인증서를 구입하는 것이 좋다.\n🎯결론 외부에 소스를 업로드 하길 원치 않지만, 자체적으로 버전 관리가 필요한 상황이었다. 내부에서 저장소를 공유하고 버전 관리를 할 수 있게 됐다.\n⚙️EndNote 사용자 관리, 저장소 인증 관리 등은 포럼에서 확인하는 것이 좋을 것 같다. Gitea Forum: https://forum.gitea.com/ 네트워크 연결 상태 때문인지 한 번씩 push에서 인증 오류 뜰 때가 있는데 다시 시도하면 정상적으로 된다. ","date":"2023-12-21T00:00:00+09:00","permalink":"https://blog.b9f1.com/p/2023-12-21-try-building-a-gitea-server/","title":"Gitea 서버 구축해보기"},{"content":"📌개요 이번 근무지에선 SVN을 사용한다. SVN은 다양한 버전이 있고 사용하는 OS에 맞는 것을 선택할 수 있다.\n📌설치 우선 사용 중인 VISUALSVN SERVER로 작성하겠다. 사이트에서 설치 파일을 받아 설치한다.\n설치 과정 라이선스 동의 서버와 관리자 또는 관리자만, command line tools 환경 변수 등록 등 옵션 선택 설치 경로, 저장소 경로, 서버 포트, 백업 경로 등을 설정 색인 설정 Subversion 또는 Windows 인증 방식 설정 📌저장소 생성 1.Repository Type FSFS (Fast Secure File System) 표준 Subversion Repository로 기본적으로 사용하는 저장소\nVDFS (VisutalSVN Distributed File System) 분산 파일 시스템과 유사한 형태를 지니며 특징은 다음과 같다.\nMaster / Slave 형태의 아키텍처로 구성 Commit할 경우 Master Server로 적용된 후 Slave Server로 자동 복제 됨 Slave Server로도 Commit 가능하며, 이 경우에 동이에 Master Server로도 자동 Commit 됨 Distributed VDFS는 FSFS repository와 기능적으론 동일하다. 그렇기 때문에 서버 구성을 어떻게 할 것인가에 따라 FSFS / VDFS를 선택하면 된다. 2.Repository Structure Repository Type을 선택한 후에는 Repository Structure를 선택해야 한다.\n이 두가지의 차이는 간단하다. 하나의 Repository에 하나의 프로젝트를 관리하는지 아니면 여러 개의 프로젝트를 관리하는지에 따라 선택하면 된다.\nEmpty repository : Standard Project로, 한 개의 Repository에 여러 Project를 관리할 수 있는 구조로 Repository를 생성한다. Single-Project Repository : 한 개의 Repository에 하나의 Proeject를 관리할 수 있는 구조로 Repository를 생성한다. 3.Respository Access Permissions 마지막으로 권한 설정이다.\nNobody has access : 아무나 접근 가능 All Subversion users have Read/Write access : SVN에 등록된 User들 접근 가능 Customize Permissions : 커스터마이징에 따라 접근 가능 📌Checkout SVN은 Git과 동작 방식이 다르다. 그래서 checkout 이후에 생기는 outgoing이 찝찝해서 알아봄.\nCheckout만 했는데 Outgoing이요? SVN에서는 Checkout 작업 자체가 로컬에 원격 저장소의 파일과 디렉토리를 가져오는 작업이다. 따라서 Checkout 이후에는 로컬 작업 디렉토리에 원격 저장소의 상태가 복제되어 추적된다. 이로 인해 변경된 사항이나 새로 생성된 파일 등이 발생할 수 있다. 따라서 새로운 Checkout은 변경된 내용이 있는 것처럼 Outgoing으로 표시될 수 있다. 예를 들어 TEST라는 원격 저장소를 받았는데 자동으로 메이븐 업데이트며 유효성 검사며 빌드며.. 진행하면서 생기는 변경 사항들을 svn ignore 처리했는데도 TEST라는 폴더에 변경사항이 있다고 outgoing이 표시될 수 있다. 이를 checkout 했다는 의미로 커밋을 하던, 무시하고 작업한 이후에 작업 내용과 함께 커밋을 하던 본인이나 팀의 방식대로 사용하는 것이 찝찝함을 없앨 수 있는 하나의 방법 같다. SVN과 Git은 다르다. SVN에서는 Checkout 후에 변경 사항이 있는 것처럼 Outgoing이 표시될 수 있다. 이는 SVN이 원격 저장소와 로컬 사이의 상태를 비교하기 때문이다. 하지만 이렇게 표시되더라도 변경 사항이 없는 것이라면 이를 무시하거나 커밋하지 않고 무시할 수 있다.\nGit의 경우에는 Clone 작업을 통해 저장소를 가져오더라도 파일의 추적이나 변경 사항이 바로 일어나지 않는다. 이는 Git의 동작 방식과 SVN의 동작 방식이 다르기 때문이다. Git에서는 로컬에 있는 작업 디렉토리에서 명시적으로 git add 명령어를 사용하여 추적하거나 변경 사항을 스테이징해야 한다.\n간단히 말하자면, SVN의 Checkout은 원격 저장소의 상태를 로컬에 복제하는 작업이다. 이로 인해 변경 사항이 있는 것처럼 Outgoing으로 표시될 수 있다. 그러나 변경 사항이 없다면 이를 무시하거나 커밋하지 않고 무시할 수 있다. Git과 SVN는 다르게 동작하므로, Git의 Clone과 SVN의 Checkout은 이러한 점에서 차이가 있다.\n🎯결론 필요한 SVN을 설치해서 저장소와 사용자 인증을 생성하고 URL을 공유할 수 있는 형태로 만든다.\n공유 받은 URL을 사용해 형상 관리를 한다. 끝.\n","date":"2022-04-12T10:00:00+09:00","permalink":"https://blog.b9f1.com/p/2022-04-12-try-using-svn/","title":"SVN을 사용해보자."},{"content":"📌DP란 최적화 이론의 한 기술이며, 특정 범위까지의 값을 구하기 위해서 그것과 다른 범위까지의 값을 이용하여 효율적으로 값을 구하는 알고리즘 설계 기법이다.\n앞에서 이미 구한 답을 다른 곳에서 재활용하는 것. 동적 계획법은 구체적인 알고리즘이라기보다 문제 해결 패러다임에 가깝다.\n어떤 문제를 풀기 위해 그 문제를 더 작은 문제의 연장선으로 생각하고 과거에 구한 해를 활용하는 방식의 알고리즘을 총칭한다.\n📌접근 방식 Top-Down 방식 (메모이제이션, Memoization) Top-Down 방식은 주어진 문제를 해결하기 위해 재귀 호출을 사용하고, 각 하위 문제의 결과를 메모리에 저장하여 중복 계산을 방지한다. 이 방식은 다음과 같은 단계를 따른다.\n재귀 함수 정의 문제를 해결하기 위한 재귀 함수를 정의한다. 이 함수는 현재 문제의 해결을 위해 하위 문제를 호출한다. 메모리 저장소 설정 하위 문제의 결과를 저장할 데이터 구조를 설정한다. 보통 배열이나 해시맵을 사용한다. 기저 사례(Base Case) 정의 재귀 호출의 종료 조건을 설정한다. 결과 계산 및 저장 재귀 호출을 통해 하위 문제를 해결하고, 그 결과를 메모리에 저장한다. 문제 해결 최종적으로 원래 문제의 결과를 메모리에서 조회하거나 계산하여 반환한다. 장점:\n코드가 직관적이고 이해하기 쉬울 수 있다. 메모이제이션을 통해 중복 계산을 방지할 수 있다. 단점:\n재귀 호출로 인해 스택 오버플로우가 발생할 수 있다. 메모리 사용이 상대적으로 많을 수 있다. Bottom-Up 방식 (타뷸레이션, Tabulation) Bottom-Up 방식은 하위 문제부터 해결하여 점차 원래 문제를 해결해 나가는 접근 방식이다. 이 방식은 일반적으로 반복문을 사용하여 DP 테이블을 채우는 방식이다.\nDP 테이블 설정 문제를 해결하기 위한 DP 배열 또는 테이블을 설정한다. 이 테이블은 하위 문제의 결과를 저장한다. 초기 조건 설정 DP 테이블의 초기값을 설정한다. 보통 가장 간단한 하위 문제의 결과를 초기화한다. 점화식 적용 점화식을 사용하여 DP 테이블을 채워 나간다. 반복문을 통해 테이블의 값을 갱신한다. 문제 해결 DP 테이블에서 원래 문제의 결과를 찾는다. 장점:\n재귀 호출이 없으므로 스택 오버플로우의 위험이 없다. 메모리와 계산 효율이 좋을 수 있다. 단점:\n코드가 복잡해질 수 있다. DP 테이블을 설정하는 데 시간이 필요할 수 있다. 요약 문제의 성격에 따라 두 방식 중 하나를 선택하여 문제를 해결할 수 있다.\nTop-Down 방식은 문제를 자연스럽게 나누어 해결할 수 있는 경우에 적합하다.\nTop-Down 방식은 재귀 호출과 메모이제이션을 사용하여 하위 문제의 결과를 저장하고 중복 계산을 방지한다. Bottom-Up 방식은 문제를 명확히 정의하고 순서대로 해결할 수 있는 경우에 유용하다.\nBottom-Up 방식은 반복문을 사용하여 하위 문제부터 차근차근 해결하며 DP 테이블을 채워나간다. 📌그래서 언제 사용하는데? 문제 정의 및 분석 해결하고자 하는 문제를 명확히 이해한다. 문제의 입력, 출력, 제약 조건 등을 파악 문제를 작은 하위 문제로 나누어 생각해본다. 하위 문제 정의 문제를 더 작은 하위 문제로 나눌 수 있는 방법을 고민한다. 하위 문제의 결과가 원래 문제를 해결하는 데 어떻게 사용될 수 있는지 정의한다. 상태 정의 각 하위 문제의 상태를 정의한다. DP 배열이나 테이블의 인덱스가 될 수 있다. 예를 들어, 최적화 문제에서는 일반적으로 DP 배열의 인덱스가 문제의 현재 상태를 나타낸다. 점화식 수립 하위 문제의 해를 원래 문제의 해로 결합하는 방법을 정의하는 점화식을 설정한다. 점화식은 문제를 재귀적으로 정의하는 방식으로, 문제의 최적해를 작은 문제들의 최적해로 표현한다. 초기 조건 설정 DP 배열의 초기값을 설정한다. 이는 문제의 가장 작은 하위 문제의 해결책을 정의하는 것으로, 일반적으로 *기저 사례(Base Case)이다. 문제 해결 및 결과 도출 점화식을 사용하여 DP 배열을 채워나간다. 최종적으로 DP 배열에서 원하는 결과를 찾는다. 이 결과는 문제의 최적해를 제공해야 한다. 시간 복잡도 및 공간 복잡도 분석 알고리즘의 시간 복잡도와 공간 복잡도를 분석하여 효율성을 검토한다. 📌예제: 피보나치 수열 DP를 사용하여 피보나치 수열을 구하는 과정을 보자.\n문제 정의 및 분석 피보나치 수열의 n번째 항을 구하는 문제 입력: n(정수) 출력: n번째 피보나치 수 하위 문제 정의 피보나치 수열의 n번째 항을 구하기 위해, (n-1)번째와 (n-2)번째 항이 필요하다. 상태 정의 dp[i]를 i번째 피보나치 수로 정의한다. 점화식 수립 점화식: dp[i] = dp[i-1] + dp[i-2] 초기 조건: dp[0] = 0, dp[1] = 1 초기 조건 설정 dp[0] = 0, dp[1] = 1로 설정한다. 문제 해결 및 결과 도출 DP 배열을 채우면서 dp[n]을 계산한다. 시간 복잡도 및 공간 복잡도 분석 시간 복잡도: O(n) 공간 복잡도: O(n) ⚙️EndNote 기저 사례 - Base Case 쪼개지지 않는 가장 작은 작업들을 가리켜 재귀 호출의 기저 사례(base case)라고 한다. 점화식 수열의 항 사이에 성립하는 관계식. ","date":"2022-01-15T10:00:00+09:00","permalink":"https://blog.b9f1.com/p/2022-01-15-dynamic-programming/","title":"Dynamic Programming"},{"content":"📌개요 형상 관리는 소스 코드 변경에 대한 모든 관리를 의미하며, 이를 통해 프로젝트의 일관성과 추적 가능성을 유지한다. 형상 관리 전략은 이러한 변경 사항을 체계적으로 관리하기 위한 방법들을 포함한다.\n프로젝트와 형상 관리 툴에 따라 달라질 수 있지만, 이번엔 Git을 기준으로 형상 관리 전략을 정리한다.\n📌형상 관리 브랜치 전략 브랜치 명명 규칙이 다를 수 있지만, 보편적이라고 생각되는 정보로 정리한다. 주요 브랜치 master 브랜치: 제품으로 출시될 수 있는 안정된 상태의 코드가 포함된다. 배포 가능한 버전만이 병합된다. develop 브랜치: 다음 출시 버전을 개발하는 브랜치다. 기능 개발과 버그 수정이 이루어진다. 보조 브랜치 feature 브랜치: 새로운 기능을 개발할 때 사용된다. develop 브랜치에서 파생하며, 작업 완료 후 develop 브랜치로 병합된다. release 브랜치: 출시 준비를 위해 사용된다. develop 브랜치에서 파생하며, QA 단계에서 발생한 버그를 수정한다. 준비가 완료되면 master와 develop 브랜치로 병합된다. hotfix 브랜치: 배포된 버전에서 긴급하게 수정이 필요한 버그를 처리할 때 사용된다. master 브랜치에서 파생하며, 수정 완료 후 master와 develop 브랜치로 병합된다. 📌단계별 작업 흐름 초기 브랜치 구성: 프로젝트 시작 시 master와 develop 브랜치를 생성한다. 기능 개발: 새로운 기능 개발 시 develop 브랜치에서 feature 브랜치를 생성한다. 기능 개발이 완료되면 feature 브랜치를 develop 브랜치로 병합한다. 출시 준비: 다음 출시를 준비하기 위해 develop 브랜치에서 release 브랜치를 생성한다. QA 과정을 거치며 발생한 버그들을 release 브랜치에서 수정한다. 최종 배포: QA가 완료되면 release 브랜치를 master와 develop 브랜치로 병합한다. master 브랜치에 새로운 버전 태그를 추가한다. 긴급 수정: 배포된 버전에서 긴급한 버그가 발생하면 master 브랜치에서 hotfix 브랜치를 생성한다. 버그 수정이 완료되면 hotfix 브랜치를 master와 develop 브랜치로 병합한다. 📌Convention 규칙은 일관성을 유지하고, 목적과 내용을 쉽게 파악할 수 있게 도와준다. 브랜치 명명 규칙 master 브랜치: master라는 이름을 사용한다. develop 브랜치: develop라는 이름을 사용한다. feature 브랜치: feature/{기능명} 형식을 사용한다. ex) feature/login-page, feature/add-user-profile release 브랜치: release/{버전번호} 형식을 사용한다. ex) release/1.0.0, release/2.1.0 hotfix 브랜치: hotfix/{수정사항} 형식을 사용한다. ex) hotfix/urgent-bug, hotfix/security-patch 커밋 메시지 규칙 타입: feat: 새로운 기능 추가 fix: 버그 수정 docs: 문서 수정 style: 코드 포맷팅, 세미콜론 누락 등 (비즈니스 로직에 변경 없음) refactor: 코드 리팩토링 test: 테스트 추가, 수정 chore: 기타 변경사항 형식: {타입}: {변경사항 설명} ex) feat: add login functionality, fix: resolve user profile bug ⚙️EndNote 형상 관리 전략은 프로젝트의 변경 사항을 체계적으로 관리하고 추적할 수 있게 하는 중요한 역할을 한다.\n이를 통해 개발 팀은 코드의 일관성을 유지하고, 배포 과정에서 발생할 수 있는 문제를 최소화할 수 있다.\nGit을 활용한 형상 관리 전략은 다양한 프로젝트에 적용할 수 있으며, 각 브랜치의 역할을 명확히 정의함으로써 효율적인 개발 프로세스를 지원한다.\n","date":"2022-01-14T10:00:00+09:00","permalink":"https://blog.b9f1.com/p/2022-01-14-version-control-strategy-with-git/","title":"Git을 사용한 형상 관리 전략"}]